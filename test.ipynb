{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21713ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1880853/1880853 [02:38<00:00, 11858.57 examples/s]\n",
      "Generating test split: 100%|██████████| 100529/100529 [00:08<00:00, 12447.23 examples/s]\n",
      "Generating validation split: 100%|██████████| 89154/89154 [00:06<00:00, 13012.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "token = \"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\"\n",
    "dataset = load_dataset(\"code_search_net\", token=token,trust_remote_code= True)  # or a coding Q&A dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8666ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93842e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 1880853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 100529\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "        num_rows: 89154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e196a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4416f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 16948.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset prepared and saved as stackoverflow_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_dict[\"train\"].select(range(10000))\n",
    "\n",
    "# Formatting function\n",
    "def format_data(example):\n",
    "    instruction = example['func_documentation_string'].strip()\n",
    "    response = example['func_code_string'].strip()\n",
    "\n",
    "    # Handle empty documentation\n",
    "    if not instruction:\n",
    "        instruction = \"Describe the following Python function.\"\n",
    "\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```python\n",
    "{response}\n",
    "```\"\"\"\n",
    "    }\n",
    "\n",
    "# Apply formatting, remove all old columns\n",
    "dataset = dataset.map(format_data, remove_columns=dataset.column_names)\n",
    "\n",
    "# Save formatted data to JSONL for fine-tuning\n",
    "with open(\"stackoverflow_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in dataset:\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Dataset prepared and saved as stackoverflow_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ee8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2051002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f6c5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [07:39<00:00, 114.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.71s/it]\n",
      "Generating train split: 10000 examples [00:00, 144917.51 examples/s]\n",
      "c:\\Users\\sasss\\anaconda3\\envs\\RS\\lib\\site-packages\\trl\\trainer\\utils.py:118: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 67\u001b[0m\n\u001b[0;32m     60\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForCompletionOnlyLM(\n\u001b[0;32m     61\u001b[0m     instruction_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Instruction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m     response_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     63\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Trainer setup\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     76\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Start Fine-tuning\u001b[39;00m\n\u001b[0;32m     79\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Update to LLaMA-3.2 when available\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=False,  # recommended to avoid issues with llama models\n",
    "    token=token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load prepared data\n",
    "train_dataset = load_dataset(\"json\", data_files=\"stackoverflow_data.jsonl\", split=\"train\")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama3-stackoverflow\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Data Collator (optional but recommended)\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"### Instruction:\\n\",\n",
    "    response_template=\"\\n\\n### Response:\\n\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    data_collator=collator,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\"\n",
    ")\n",
    "\n",
    "# Start Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned LoRA model\n",
    "trainer.save_model(\"llama3-stackoverflow-lora\")\n",
    "print(\"✅ Model fine-tuned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
