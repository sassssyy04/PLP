{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1555593,"sourceType":"datasetVersion","datasetId":850380},{"sourceId":11415419,"sourceType":"datasetVersion","datasetId":7138623}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This project builds an intelligent assistant capable of classifying code-related questions and generating accurate responses based on the type of question. It is designed to help students and developers interact more effectively with programming problems and receive targeted help.","metadata":{}},{"cell_type":"markdown","source":"#### Note: This notebook should be run on Kaggle","metadata":{}},{"cell_type":"markdown","source":"# Question Categorization\n## Masked Languge Modeling\n### Load Dataset","metadata":{}},{"cell_type":"markdown","source":"Add this dataset to input https://www.kaggle.com/datasets/imoore/60k-stack-overflow-questions-with-quality-rate","metadata":{}},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/train.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv\")\n\nprint(\"First 5 records:\", train_df.head())\n\ntrain_texts = train_df['Title'].fillna(\"\") + \" \" + train_df['Body'].fillna(\"\")\nvalid_texts = valid_df['Title'].fillna(\"\") + \" \" + valid_df['Body'].fillna(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:05:01.741775Z","iopub.execute_input":"2025-05-13T02:05:01.742022Z","iopub.status.idle":"2025-05-13T02:05:05.590908Z","shell.execute_reply.started":"2025-05-13T02:05:01.741996Z","shell.execute_reply":"2025-05-13T02:05:05.590143Z"}},"outputs":[{"name":"stdout","text":"First 5 records:          Id                                              Title  \\\n0  34552656             Java: Repeat Task Every Random Seconds   \n1  34553034                  Why are Java Optionals immutable?   \n2  34553174  Text Overlay Image with Darkened Opacity React...   \n3  34553318         Why ternary operator in swift is so picky?   \n4  34553755                 hide/show fab with scale animation   \n\n                                                Body  \\\n0  <p>I'm already familiar with repeating tasks e...   \n1  <p>I'd like to understand why Java 8 Optionals...   \n2  <p>I am attempting to overlay a title over an ...   \n3  <p>The question is very simple, but I just cou...   \n4  <p>I'm using custom floatingactionmenu. I need...   \n\n                                                Tags         CreationDate  \\\n0                                     <java><repeat>  2016-01-01 00:21:59   \n1                                   <java><optional>  2016-01-01 02:03:20   \n2  <javascript><image><overlay><react-native><opa...  2016-01-01 02:48:24   \n3  <swift><operators><whitespace><ternary-operato...  2016-01-01 03:30:17   \n4  <android><material-design><floating-action-but...  2016-01-01 05:21:48   \n\n          Y  \n0  LQ_CLOSE  \n1        HQ  \n2        HQ  \n3        HQ  \n4        HQ  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Disable wandb to get a faster training speed","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:05:05.591764Z","iopub.execute_input":"2025-05-13T02:05:05.591994Z","iopub.status.idle":"2025-05-13T02:05:05.596077Z","shell.execute_reply.started":"2025-05-13T02:05:05.591976Z","shell.execute_reply":"2025-05-13T02:05:05.595299Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:09:23.276691Z","iopub.execute_input":"2025-05-13T02:09:23.277172Z","iopub.status.idle":"2025-05-13T02:09:23.280937Z","shell.execute_reply.started":"2025-05-13T02:09:23.277150Z","shell.execute_reply":"2025-05-13T02:09:23.280234Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\nfrom datasets import Dataset\n\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n#tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/codebert-base\")\n\n# Change to HuggingFace datasets format\ntrain_dataset = Dataset.from_dict({\"text\": train_texts})\nvalid_dataset = Dataset.from_dict({\"text\": valid_texts})\n\n# Encode with tokenizer\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\n\ntokenized_train = train_dataset.map(tokenize_function, batched=True)\ntokenized_valid = valid_dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:05:05.597511Z","iopub.execute_input":"2025-05-13T02:05:05.597755Z","iopub.status.idle":"2025-05-13T02:05:42.190946Z","shell.execute_reply.started":"2025-05-13T02:05:05.597740Z","shell.execute_reply":"2025-05-13T02:05:42.190210Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abf15eb461e4db491d2d357ad62943e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba41d2a1cfa945898a0104df6d45bb30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb72d18ea93f44b09413cac81af859ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bcabe70ba34e7cbdc5167b6bdf66b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5268e70c96d340f392fde67e4efa2f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7d7fbd3fc1640eb9a6ca06b7c564c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcaa0d3c3f74f91aacbcc45f73c45fc"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Create Mask","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:09:29.909965Z","iopub.execute_input":"2025-05-13T02:09:29.910703Z","iopub.status.idle":"2025-05-13T02:09:29.914573Z","shell.execute_reply.started":"2025-05-13T02:09:29.910673Z","shell.execute_reply":"2025-05-13T02:09:29.913759Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Initialize the model and trainer","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM, TrainingArguments, Trainer\n\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n#model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base\")\n#To load trained weights use the following line, replace the path with your model path\n#model = RobertaForMaskedLM.from_pretrained(\"/kaggle/input/codeqa-datasets-and-models/mlm-roberta\")\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./mlm-roberta-stackoverflow\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    save_total_limit=2,\n    prediction_loss_only=True,\n    fp16=True,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_valid,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:09:34.427580Z","iopub.execute_input":"2025-05-13T02:09:34.428284Z","iopub.status.idle":"2025-05-13T02:09:34.876320Z","shell.execute_reply.started":"2025-05-13T02:09:34.428260Z","shell.execute_reply":"2025-05-13T02:09:34.875579Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(\"./mlm-roberta-stackoverflow\")\ntokenizer.save_pretrained(\"./mlm-roberta-stackoverflow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:09:37.161664Z","iopub.execute_input":"2025-05-13T02:09:37.162435Z","iopub.status.idle":"2025-05-13T03:00:37.104738Z","shell.execute_reply.started":"2025-05-13T02:09:37.162400Z","shell.execute_reply":"2025-05-13T03:00:37.104058Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4221' max='4221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4221/4221 50:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.784000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.608200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.543300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.483800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.458200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.416500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.391000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.371600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('./mlm-roberta-stackoverflow/tokenizer_config.json',\n './mlm-roberta-stackoverflow/special_tokens_map.json',\n './mlm-roberta-stackoverflow/vocab.json',\n './mlm-roberta-stackoverflow/merges.txt',\n './mlm-roberta-stackoverflow/added_tokens.json',\n './mlm-roberta-stackoverflow/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Download the trained model","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\nshutil.make_archive('/kaggle/working/mlm-roberta', 'zip', '/kaggle/working/mlm-roberta-stackoverflow')\n\nfrom IPython.display import FileLink, display\nFileLink(\"mlm-roberta.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:00:37.106099Z","iopub.execute_input":"2025-05-13T03:00:37.106434Z","iopub.status.idle":"2025-05-13T03:03:31.655364Z","shell.execute_reply.started":"2025-05-13T03:00:37.106407Z","shell.execute_reply":"2025-05-13T03:03:31.654789Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/mlm-roberta.zip","text/html":"<a href='mlm-roberta.zip' target='_blank'>mlm-roberta.zip</a><br>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### Load and evaluate the model","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM, RobertaTokenizerFast\n\nmodel = RobertaForMaskedLM.from_pretrained(\"/kaggle/input/codeqa-datasets-and-models/mlm-roberta\")\ntokenizer = RobertaTokenizerFast.from_pretrained(\"/kaggle/input/codeqa-datasets-and-models/mlm-roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:03:31.656066Z","iopub.execute_input":"2025-05-13T03:03:31.656306Z","iopub.status.idle":"2025-05-13T03:03:32.337030Z","shell.execute_reply.started":"2025-05-13T03:03:31.656289Z","shell.execute_reply":"2025-05-13T03:03:32.336246Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:03:32.338563Z","iopub.execute_input":"2025-05-13T03:03:32.338862Z","iopub.status.idle":"2025-05-13T03:03:32.834206Z","shell.execute_reply.started":"2025-05-13T03:03:32.338836Z","shell.execute_reply":"2025-05-13T03:03:32.833685Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# load valid data\ndf_valid = pd.read_csv(\"/kaggle/input/60k-stack-overflow-questions-with-quality-rate/valid.csv\")\ndf_valid = df_valid.dropna(subset=[\"Body\"])  # 防止空值报错\n\nvalid_dataset = Dataset.from_pandas(df_valid[[\"Body\"]])\n\n# Tokenize for MLM\ndef tokenize_mlm(example):\n    return tokenizer(example[\"Body\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_valid = valid_dataset.map(tokenize_mlm, batched=True)\ntokenized_valid.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:03:32.857305Z","iopub.execute_input":"2025-05-13T03:03:32.857563Z","iopub.status.idle":"2025-05-13T03:04:07.340859Z","shell.execute_reply.started":"2025-05-13T03:03:32.857543Z","shell.execute_reply":"2025-05-13T03:04:07.340288Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b11bf02befc4909ad27c33d7b5a549c"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:04:07.341562Z","iopub.execute_input":"2025-05-13T03:04:07.341840Z","iopub.status.idle":"2025-05-13T03:04:07.345787Z","shell.execute_reply.started":"2025-05-13T03:04:07.341816Z","shell.execute_reply":"2025-05-13T03:04:07.345191Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\neval_results = trainer.evaluate(eval_dataset=tokenized_valid)\nprint(eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:04:07.346348Z","iopub.execute_input":"2025-05-13T03:04:07.346543Z","iopub.status.idle":"2025-05-13T03:06:19.734918Z","shell.execute_reply.started":"2025-05-13T03:04:07.346529Z","shell.execute_reply":"2025-05-13T03:06:19.733919Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [938/938 02:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 3.4965949058532715, 'eval_runtime': 131.8841, 'eval_samples_per_second': 113.736, 'eval_steps_per_second': 7.112}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Classification\n### Load Dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\n# Supposed to be 11 classes, but in the cleaned train dataset, actually\n# there are only 9 classes available. After merging similar meaning classes, \n# we got 7 classes for this question categorization module\nlabel_mapping = {\n    \"code_understanding\": \"code_explain\",\n    \"code_explain\": \"code_explain\",\n    \"logical\": \"logical_reasoning\",\n    \"reasoning\": \"logical_reasoning\",\n    \"error\": \"error\",\n    \"usage\": \"usage\",\n    \"algorithm\": \"algorithm\",\n    \"task\": \"task\",\n    #\"comparison\": \"comparison\",\n    \"variable\": \"variable\",\n    #\"guiding\": \"guiding\"\n}\n\ndef jsonl_to_json(input_file, output_file):\n    # Make sure the output path exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Extract data from the file\n    questions = []\n    labels = []\n    \n    with open(input_file, \"r\") as f:\n        for line in f:\n            data = json.loads(line)\n            question = data[\"question\"]  # Extract question\n            original_label = data[\"questionType\"]  # Extract label\n            \n            # Remap the questions to the labels\n            if original_label in label_mapping:\n                new_label = label_mapping[original_label]\n            else:\n                new_label = original_label  # Keep the label if the label is not in the dict\n                print(\"New label found\", new_label)\n            \n            questions.append(question)\n            labels.append(new_label)\n    \n    # Change the format for Hugging Face models\n    train_data = [{\"text\": q, \"label\": l} for q, l in zip(questions, labels)]\n    \n    # Write the data into the .json files\n    with open(output_file, \"w\") as f:\n        json.dump(train_data, f, ensure_ascii=False, indent=4)\n    \n    print(f\"Converted data has been saved to {output_file}\")\n\ninput_file = \"/kaggle/input/codeqa-datasets-and-models/cs1qa/cs1qa/augmented_train_cleaned.jsonl\"\noutput_dir = \"/kaggle/working/cs1qa\"\noutput_file = os.path.join(output_dir, \"train.json\")\njsonl_to_json(input_file, output_file)\n\ninput_file = \"/kaggle/input/codeqa-datasets-and-models/cs1qa/cs1qa/test_cleaned.jsonl\"\noutput_dir = \"/kaggle/working/cs1qa\"\noutput_file = os.path.join(output_dir, \"test.json\")\njsonl_to_json(input_file, output_file)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:19.737535Z","iopub.execute_input":"2025-05-13T03:06:19.737748Z","iopub.status.idle":"2025-05-13T03:06:20.089793Z","shell.execute_reply.started":"2025-05-13T03:06:19.737732Z","shell.execute_reply":"2025-05-13T03:06:20.089006Z"}},"outputs":[{"name":"stdout","text":"Converted data has been saved to /kaggle/working/cs1qa/train.json\nConverted data has been saved to /kaggle/working/cs1qa/test.json\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from datasets import load_dataset\nimport numpy as np\nimport torch\n\ndataset = load_dataset(\"json\", data_files={\"train\": \"/kaggle/working/cs1qa/train.json\", \"test\": \"/kaggle/working/cs1qa/test.json\"})\n\nlabels = list(set(example[\"label\"] for example in dataset[\"train\"]))\nlabel2id = {label: i for i, label in enumerate(labels)}\nid2label = {i: label for label, i in label2id.items()}\n\ndef encode_labels(example):\n    example[\"label\"] = label2id[example[\"label\"]]\n    return example\n\ndataset = dataset.map(encode_labels)\nprint(id2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:20.090649Z","iopub.execute_input":"2025-05-13T03:06:20.091155Z","iopub.status.idle":"2025-05-13T03:06:20.781975Z","shell.execute_reply.started":"2025-05-13T03:06:20.091129Z","shell.execute_reply":"2025-05-13T03:06:20.781162Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8909527e8e3d4f429f174568df135ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c43ce51888456b87935d0de0fd2d85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5907 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f976bf20117d4a61bda2b369f5516aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1bbe17cea944594876070dcfe334784"}},"metadata":{}},{"name":"stdout","text":"{0: 'error', 1: 'usage', 2: 'algorithm', 3: 'code_explain', 4: 'variable', 5: 'logical_reasoning', 6: 'task'}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Describe the dataset\ntrain_labels = dataset[\"train\"][\"label\"]\ntest_labels = dataset[\"test\"][\"label\"] if \"test\" in dataset else []\n\ntrain_unique, train_counts = np.unique(train_labels, return_counts=True)\nprint(\"Train set label distribution:\")\nfor label_id, count in zip(train_unique, train_counts):\n    print(f\"{id2label[label_id]}: {count}\")\n\nif len(test_labels) > 0:\n    test_unique, test_counts = np.unique(test_labels, return_counts=True)\n    print(\"\\nTest set label distribution:\")\n    for label_id, count in zip(test_unique, test_counts):\n        print(f\"{id2label[label_id]}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:20.782685Z","iopub.execute_input":"2025-05-13T03:06:20.782910Z","iopub.status.idle":"2025-05-13T03:06:20.793830Z","shell.execute_reply.started":"2025-05-13T03:06:20.782893Z","shell.execute_reply":"2025-05-13T03:06:20.793185Z"}},"outputs":[{"name":"stdout","text":"Train set label distribution:\nerror: 575\nusage: 486\nalgorithm: 717\ncode_explain: 1054\nvariable: 1165\nlogical_reasoning: 1116\ntask: 794\n\nTest set label distribution:\nerror: 193\nusage: 162\nalgorithm: 239\ncode_explain: 229\nvariable: 389\nlogical_reasoning: 371\ntask: 264\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaForSequenceClassification\n\ntokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:20.794537Z","iopub.execute_input":"2025-05-13T03:06:20.794874Z","iopub.status.idle":"2025-05-13T03:06:22.015031Z","shell.execute_reply.started":"2025-05-13T03:06:20.794856Z","shell.execute_reply":"2025-05-13T03:06:22.014230Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859647bfc3af4c04ae7d6356a4a49e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfacedef85a04fe8bef83329807d0911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76f47e4dbc81439885d217a572642c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b531d3b974c43fca7026acde453f0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13685746856a40c2894c39cd6b9ae6e2"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\n\n#tokenizer = RobertaTokenizerFast.from_pretrained(\"/kaggle/input/codeqa-datasets-and-models/mlm-roberta\")\n\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ndataset = dataset.map(tokenize, batched=True)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:22.015886Z","iopub.execute_input":"2025-05-13T03:06:22.016153Z","iopub.status.idle":"2025-05-13T03:06:25.445807Z","shell.execute_reply.started":"2025-05-13T03:06:22.016131Z","shell.execute_reply":"2025-05-13T03:06:25.445206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5907 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2dc0bd71bac4547a192e988cfd55050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0265d2162e4e4089b99d3fe8a9c3f425"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification\n\nmodel = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\",\n                                                         num_labels=len(label2id),\n                                                         id2label=id2label,\n                                                         label2id=label2id,\n                                                         hidden_dropout_prob=0.3,  # 加大 dropout\n                                                         attention_probs_dropout_prob=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:25.446493Z","iopub.execute_input":"2025-05-13T03:06:25.446736Z","iopub.status.idle":"2025-05-13T03:06:28.864742Z","shell.execute_reply.started":"2025-05-13T03:06:25.446721Z","shell.execute_reply":"2025-05-13T03:06:28.864118Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30bb9424a7c44e7a8cda1ad90e1754dd"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:28.865404Z","iopub.execute_input":"2025-05-13T03:06:28.865639Z","iopub.status.idle":"2025-05-13T03:06:28.870088Z","shell.execute_reply.started":"2025-05-13T03:06:28.865622Z","shell.execute_reply":"2025-05-13T03:06:28.869359Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom transformers import EarlyStoppingCallback \n\nlr = 2e-5\ntrain_epochs = 10\n\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta_cs1qa_results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=lr,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    num_train_epochs=train_epochs,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    save_total_limit=2,\n    greater_is_better=True,\n    metric_for_best_model='f1',\n    warmup_steps=200,\n    lr_scheduler_type='cosine_with_restarts',\n)\n    \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:06:28.870931Z","iopub.execute_input":"2025-05-13T03:06:28.871141Z","iopub.status.idle":"2025-05-13T03:23:47.700231Z","shell.execute_reply.started":"2025-05-13T03:06:28.871117Z","shell.execute_reply":"2025-05-13T03:23:47.699688Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dcc674798d4411e9c3b09605530e0f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2960' max='3700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2960/3700 17:17 < 04:19, 2.85 it/s, Epoch 8/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.456014</td>\n      <td>0.411478</td>\n      <td>0.335022</td>\n      <td>0.411478</td>\n      <td>0.295570</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.720900</td>\n      <td>1.062162</td>\n      <td>0.596643</td>\n      <td>0.598852</td>\n      <td>0.596643</td>\n      <td>0.575119</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.040100</td>\n      <td>0.942639</td>\n      <td>0.681104</td>\n      <td>0.667902</td>\n      <td>0.681104</td>\n      <td>0.664441</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.040100</td>\n      <td>0.952123</td>\n      <td>0.672442</td>\n      <td>0.665440</td>\n      <td>0.672442</td>\n      <td>0.661127</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.799000</td>\n      <td>0.890245</td>\n      <td>0.726042</td>\n      <td>0.721601</td>\n      <td>0.726042</td>\n      <td>0.722303</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.661800</td>\n      <td>0.857468</td>\n      <td>0.732539</td>\n      <td>0.731833</td>\n      <td>0.732539</td>\n      <td>0.730459</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.584400</td>\n      <td>0.876068</td>\n      <td>0.720628</td>\n      <td>0.723326</td>\n      <td>0.720628</td>\n      <td>0.718123</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.584400</td>\n      <td>0.898640</td>\n      <td>0.725501</td>\n      <td>0.728169</td>\n      <td>0.725501</td>\n      <td>0.723967</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2960, training_loss=0.8948978888021933, metrics={'train_runtime': 1038.061, 'train_samples_per_second': 56.904, 'train_steps_per_second': 3.564, 'total_flos': 3108533553100800.0, 'train_loss': 0.8948978888021933, 'epoch': 8.0})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Evaluate the model\n#metrics = trainer.evaluate()\n#print(metrics)\n\n# Save the model and tokenizer\ntrainer.save_model(\"./roberta_cs1qa_model_10epoch\")\ntokenizer.save_pretrained(\"./roberta_cs1qa_model_10epoch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:23:47.700892Z","iopub.execute_input":"2025-05-13T03:23:47.701104Z","iopub.status.idle":"2025-05-13T03:23:49.093461Z","shell.execute_reply.started":"2025-05-13T03:23:47.701088Z","shell.execute_reply":"2025-05-13T03:23:49.092822Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('./roberta_cs1qa_model_10epoch/tokenizer_config.json',\n './roberta_cs1qa_model_10epoch/special_tokens_map.json',\n './roberta_cs1qa_model_10epoch/vocab.json',\n './roberta_cs1qa_model_10epoch/merges.txt',\n './roberta_cs1qa_model_10epoch/added_tokens.json')"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import shutil\nshutil.make_archive('/kaggle/working/mlm-roberta-cs1qa', 'zip', '/kaggle/working/roberta_cs1qa_model_10epoch')\n\nfrom IPython.display import FileLink\nFileLink(\"mlm-roberta-cs1qa.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:23:49.094155Z","iopub.execute_input":"2025-05-13T03:23:49.094447Z","iopub.status.idle":"2025-05-13T03:24:14.578711Z","shell.execute_reply.started":"2025-05-13T03:23:49.094424Z","shell.execute_reply":"2025-05-13T03:24:14.578087Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/mlm-roberta-cs1qa.zip","text/html":"<a href='mlm-roberta-cs1qa.zip' target='_blank'>mlm-roberta-cs1qa.zip</a><br>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:24:14.579283Z","iopub.execute_input":"2025-05-13T03:24:14.579498Z","iopub.status.idle":"2025-05-13T03:24:23.611155Z","shell.execute_reply.started":"2025-05-13T03:24:14.579482Z","shell.execute_reply":"2025-05-13T03:24:23.610206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [58/58 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.857467532157898, 'eval_accuracy': 0.7325392528424473, 'eval_precision': 0.7318332635306875, 'eval_recall': 0.7325392528424473, 'eval_f1': 0.7304591718765676, 'eval_runtime': 9.0229, 'eval_samples_per_second': 204.702, 'eval_steps_per_second': 6.428, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Streamlit App","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    AutoModelForCausalLM\n)\nimport torch\n\nst.title(\"CodeQA\")\n\nwith st.sidebar:\n    st.header(\"Settings\")\n    max_length = st.slider(\"Max Answer Length\", 100, 500, 300)\n    temperature = st.slider(\"Temperature\", 0.1, 1.0, 0.7)\n\n@st.cache_resource\ndef load_models():\n    cls_model_path = \"/kaggle/working/roberta_cs1qa_model_10epoch\"\n    cls_tokenizer = AutoTokenizer.from_pretrained(cls_model_path)\n    cls_model = AutoModelForSequenceClassification.from_pretrained(cls_model_path)\n    \n    gen_model_path = \"/kaggle/input/codeqa-datasets-and-models/codellama/codellama\"\n    gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_path)\n    gen_model = AutoModelForCausalLM.from_pretrained(\n        gen_model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    \n    return cls_model, cls_tokenizer, gen_model, gen_tokenizer\n\ncls_model, cls_tokenizer, gen_model, gen_tokenizer = load_models()\n\nid2label = {0: 'usage', \n            1: 'code_explain', \n            2: 'logical_reasoning', \n            3: 'error', \n            4: 'task', \n            5: 'variable', \n            6: 'algorithm'}\n\nquestion = st.text_area(\"Enter your question\", height=150, placeholder=\"Example: why does this loop only execute 5 times?\")\ncode = st.text_area(\"Enter your code(if applicable)\", height=150, placeholder=\"Optional: Paste your code here...\") \n\n\nif st.button(\"Generate\"):\n    if not question.strip():\n        st.warning(\"Please enter your question.\")\n    else:\n        with st.spinner(\"Analysing...\"):\n            cls_inputs = cls_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True)\n            with torch.no_grad():\n                cls_outputs = cls_model(**cls_inputs)\n            pred = cls_outputs.logits.argmax(dim=-1).item()\n            category = id2label.get(pred, \"unknown\")\n\n            code_explain_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a code behavior specialist. Explain ONLY:\n            1. What the code literally does (no assumptions)\n            2. Key operations line-by-line\n            3. Output format if applicable\n            Avoid suggesting improvements or external context.\n            <</SYS>>\n            \n            [Task: code_explain]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            logical_reasoing_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a logic analyzer.:\n            1. Trace the control flow step-by-step\n            2. Highlight conditionals/branches\n            3. Show final conclusion with evidence\n            <</SYS>>\n            \n            [Task: logical_reasoning]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            error_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a debugger. Respond with:\n            1. Exact error type + trigger line\n            2. Immediate fix (code snippet)\n            3. Root cause\n            Format fixes as: ```python\\n[code]\\n```\n            <</SYS>>\n            \n            [Task: error]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            usage_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a documentation expert. Provide:\n            1. Standard library/API usage syntax\n            2. Required parameters + return type\n            3. Minimal working example\n            <</SYS>>\n            \n            [Task: usage]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            algorithm_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are an algorithms professor. Explain:\n            1. Algorithm name/pattern\n            2. Time/space complexity (Big-O)\n            3. Optimization potential\n            <</SYS>>\n            \n            [Task: algorithm]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            task_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a pair programmer.:\n            1. Break down requirements into steps\n            2. Provide complete implementation\n            3. Explain key decisions\n            Format code as: ```python\\n[code]\\n```\n            <</SYS>>\n            \n            [Task: task]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            variable_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a runtime inspector.:\n            1. Track variable value changes\n            2. Show scope/lifetime\n            3. Highlight type conversions\n            <</SYS>>\n            \n            [Task: variable]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n            \n            concept_prompt = \"\"\"\n            [INST] <<SYS>>\n            You are a CS lecturer. Explain:\n            1. Core concept definition\n            2. Real-world analogy\n            3. Simple code demonstration\n            <</SYS>>\n            \n            [Task: concept]\n            Question: {question}\n            Code:{code}\n            [/INST]\n            \"\"\"\n \n            PROMPT_TEMPLATES = {\n                \"code_explain\" : code_explain_prompt,\n                \"logical_reasoing\" : logical_reasoing_prompt,\n                \"error\" : error_prompt,\n                \"usage\" : usage_prompt,\n                \"algorithm\" : algorithm_prompt,\n                \"task\" : task_prompt,\n                \"variable\" : variable_prompt,\n                \"concept\" : concept_prompt\n            }\n            \n            def build_prompt(category, question, code=None):\n                template = PROMPT_TEMPLATES[category]\n                prompt = template.replace(\"{question}\", question)\n                if \"{code}\" in prompt and code:\n                    prompt = prompt.replace(\"{code}\", code)\n                return prompt\n\n            prompt = build_prompt(category, question, code=code)\n\n            st.write(prompt)\n            \n            gen_inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n            outputs = gen_model.generate(\n                **gen_inputs,\n                max_new_tokens=max_length,\n                temperature=temperature,\n                do_sample=True\n            )\n            answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            answer = answer.split(\"[/INST]\")[-1].strip()\n\n        st.write(answer)\n\nst.divider()\nst.subheader(\"Example Questions\")\nexamples = [\n    \"How to reverst arrays in Python\",\n    \"Why I got 'IndexError: list index out of range'?\"\n]\nfor ex in examples:\n    if st.button(ex, key=ex):\n        question = ex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:53:09.352217Z","iopub.execute_input":"2025-05-13T03:53:09.352569Z","iopub.status.idle":"2025-05-13T03:53:09.361325Z","shell.execute_reply.started":"2025-05-13T03:53:09.352546Z","shell.execute_reply":"2025-05-13T03:53:09.360665Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!pip install streamlit -q\n!pip install pyngrok -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:24:23.612063Z","iopub.execute_input":"2025-05-13T03:24:23.612746Z","iopub.status.idle":"2025-05-13T03:24:32.608313Z","shell.execute_reply.started":"2025-05-13T03:24:23.612716Z","shell.execute_reply":"2025-05-13T03:24:32.607277Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import subprocess\nsubprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n\n# set ngrok\nfrom pyngrok import ngrok\nngrok.set_auth_token(\"2vPblhhFOkqjvlAOgSDa8qisMXa_546jNEKPFTVZyrBfdGZMm\")  # ← 替换这里\n\npublic_url = ngrok.connect(addr=\"8501\", proto=\"http\") \nprint(\"Access URL\", public_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T03:24:32.609625Z","iopub.execute_input":"2025-05-13T03:24:32.609887Z","iopub.status.idle":"2025-05-13T03:24:34.039270Z","shell.execute_reply.started":"2025-05-13T03:24:32.609864Z","shell.execute_reply":"2025-05-13T03:24:34.038642Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"Usage: streamlit run [OPTIONS] TARGET [ARGS]...\nTry 'streamlit run --help' for help.\n\nError: Invalid value: File does not exist: app.py\n","output_type":"stream"},{"name":"stdout","text":"Access URL NgrokTunnel: \"https://99a1-34-42-216-145.ngrok-free.app\" -> \"http://localhost:8501\"\n","output_type":"stream"}],"execution_count":31}]}