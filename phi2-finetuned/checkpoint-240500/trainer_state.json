{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.749555097753828,
  "eval_steps": 500,
  "global_step": 240500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0001558326606556815,
      "grad_norm": 0.8496657013893127,
      "learning_rate": 4.9997506677429514e-05,
      "loss": 1.8663,
      "step": 50
    },
    {
      "epoch": 0.000311665321311363,
      "grad_norm": 0.6794496178627014,
      "learning_rate": 4.999490946641858e-05,
      "loss": 1.5096,
      "step": 100
    },
    {
      "epoch": 0.0004674979819670445,
      "grad_norm": 0.9423673152923584,
      "learning_rate": 4.999231225540765e-05,
      "loss": 1.4533,
      "step": 150
    },
    {
      "epoch": 0.000623330642622726,
      "grad_norm": 0.6413052082061768,
      "learning_rate": 4.998971504439673e-05,
      "loss": 1.3963,
      "step": 200
    },
    {
      "epoch": 0.0007791633032784075,
      "grad_norm": 0.7025055885314941,
      "learning_rate": 4.99871178333858e-05,
      "loss": 1.3862,
      "step": 250
    },
    {
      "epoch": 0.000934995963934089,
      "grad_norm": 0.7646145224571228,
      "learning_rate": 4.998452062237487e-05,
      "loss": 1.386,
      "step": 300
    },
    {
      "epoch": 0.0010908286245897706,
      "grad_norm": 0.6723560094833374,
      "learning_rate": 4.998192341136394e-05,
      "loss": 1.3786,
      "step": 350
    },
    {
      "epoch": 0.001246661285245452,
      "grad_norm": 0.7501801252365112,
      "learning_rate": 4.9979326200353015e-05,
      "loss": 1.4067,
      "step": 400
    },
    {
      "epoch": 0.0014024939459011336,
      "grad_norm": 0.7366626262664795,
      "learning_rate": 4.997672898934209e-05,
      "loss": 1.3414,
      "step": 450
    },
    {
      "epoch": 0.001558326606556815,
      "grad_norm": 0.5664870738983154,
      "learning_rate": 4.997413177833116e-05,
      "loss": 1.3672,
      "step": 500
    },
    {
      "epoch": 0.0017141592672124965,
      "grad_norm": 0.707743763923645,
      "learning_rate": 4.997153456732023e-05,
      "loss": 1.3794,
      "step": 550
    },
    {
      "epoch": 0.001869991927868178,
      "grad_norm": 0.6412230730056763,
      "learning_rate": 4.9968937356309305e-05,
      "loss": 1.4027,
      "step": 600
    },
    {
      "epoch": 0.0020258245885238597,
      "grad_norm": 0.60760498046875,
      "learning_rate": 4.996634014529837e-05,
      "loss": 1.3773,
      "step": 650
    },
    {
      "epoch": 0.002181657249179541,
      "grad_norm": 0.734705924987793,
      "learning_rate": 4.996374293428745e-05,
      "loss": 1.3641,
      "step": 700
    },
    {
      "epoch": 0.0023374899098352227,
      "grad_norm": 0.882408618927002,
      "learning_rate": 4.996114572327652e-05,
      "loss": 1.4109,
      "step": 750
    },
    {
      "epoch": 0.002493322570490904,
      "grad_norm": 0.7095734477043152,
      "learning_rate": 4.995854851226559e-05,
      "loss": 1.3751,
      "step": 800
    },
    {
      "epoch": 0.0026491552311465856,
      "grad_norm": 0.572027325630188,
      "learning_rate": 4.995595130125466e-05,
      "loss": 1.3353,
      "step": 850
    },
    {
      "epoch": 0.002804987891802267,
      "grad_norm": 0.8691175580024719,
      "learning_rate": 4.995335409024374e-05,
      "loss": 1.3831,
      "step": 900
    },
    {
      "epoch": 0.0029608205524579486,
      "grad_norm": 0.7337713241577148,
      "learning_rate": 4.9950756879232806e-05,
      "loss": 1.3527,
      "step": 950
    },
    {
      "epoch": 0.00311665321311363,
      "grad_norm": 0.6690821647644043,
      "learning_rate": 4.994815966822188e-05,
      "loss": 1.4145,
      "step": 1000
    },
    {
      "epoch": 0.0032724858737693116,
      "grad_norm": 0.7521985173225403,
      "learning_rate": 4.994556245721095e-05,
      "loss": 1.3447,
      "step": 1050
    },
    {
      "epoch": 0.003428318534424993,
      "grad_norm": 0.584939181804657,
      "learning_rate": 4.994296524620002e-05,
      "loss": 1.3789,
      "step": 1100
    },
    {
      "epoch": 0.0035841511950806745,
      "grad_norm": 0.7578305602073669,
      "learning_rate": 4.9940368035189096e-05,
      "loss": 1.3915,
      "step": 1150
    },
    {
      "epoch": 0.003739983855736356,
      "grad_norm": 0.7836129069328308,
      "learning_rate": 4.993777082417817e-05,
      "loss": 1.3492,
      "step": 1200
    },
    {
      "epoch": 0.0038958165163920375,
      "grad_norm": 0.6350499391555786,
      "learning_rate": 4.993517361316724e-05,
      "loss": 1.3929,
      "step": 1250
    },
    {
      "epoch": 0.004051649177047719,
      "grad_norm": 0.6463618874549866,
      "learning_rate": 4.993257640215631e-05,
      "loss": 1.3481,
      "step": 1300
    },
    {
      "epoch": 0.0042074818377034005,
      "grad_norm": 0.7150506377220154,
      "learning_rate": 4.992997919114538e-05,
      "loss": 1.3549,
      "step": 1350
    },
    {
      "epoch": 0.004363314498359082,
      "grad_norm": 0.6580155491828918,
      "learning_rate": 4.992738198013445e-05,
      "loss": 1.2908,
      "step": 1400
    },
    {
      "epoch": 0.004519147159014763,
      "grad_norm": 0.73553466796875,
      "learning_rate": 4.992478476912353e-05,
      "loss": 1.3591,
      "step": 1450
    },
    {
      "epoch": 0.004674979819670445,
      "grad_norm": 0.7916200757026672,
      "learning_rate": 4.9922187558112596e-05,
      "loss": 1.3626,
      "step": 1500
    },
    {
      "epoch": 0.004830812480326126,
      "grad_norm": 0.6015838980674744,
      "learning_rate": 4.991959034710167e-05,
      "loss": 1.341,
      "step": 1550
    },
    {
      "epoch": 0.004986645140981808,
      "grad_norm": 0.763563871383667,
      "learning_rate": 4.991699313609075e-05,
      "loss": 1.3246,
      "step": 1600
    },
    {
      "epoch": 0.005142477801637489,
      "grad_norm": 0.5979167222976685,
      "learning_rate": 4.9914395925079814e-05,
      "loss": 1.3547,
      "step": 1650
    },
    {
      "epoch": 0.005298310462293171,
      "grad_norm": 0.6430054903030396,
      "learning_rate": 4.9911798714068886e-05,
      "loss": 1.3338,
      "step": 1700
    },
    {
      "epoch": 0.005454143122948852,
      "grad_norm": 0.6330245733261108,
      "learning_rate": 4.990920150305796e-05,
      "loss": 1.3078,
      "step": 1750
    },
    {
      "epoch": 0.005609975783604534,
      "grad_norm": 0.5595652461051941,
      "learning_rate": 4.990660429204703e-05,
      "loss": 1.2861,
      "step": 1800
    },
    {
      "epoch": 0.005765808444260215,
      "grad_norm": 0.7001324892044067,
      "learning_rate": 4.9904007081036104e-05,
      "loss": 1.3455,
      "step": 1850
    },
    {
      "epoch": 0.005921641104915897,
      "grad_norm": 0.5372718572616577,
      "learning_rate": 4.9901409870025176e-05,
      "loss": 1.3749,
      "step": 1900
    },
    {
      "epoch": 0.006077473765571578,
      "grad_norm": 0.6712771654129028,
      "learning_rate": 4.989881265901425e-05,
      "loss": 1.3327,
      "step": 1950
    },
    {
      "epoch": 0.00623330642622726,
      "grad_norm": 0.5737619400024414,
      "learning_rate": 4.989621544800332e-05,
      "loss": 1.3743,
      "step": 2000
    },
    {
      "epoch": 0.006389139086882941,
      "grad_norm": 0.628946840763092,
      "learning_rate": 4.989361823699239e-05,
      "loss": 1.324,
      "step": 2050
    },
    {
      "epoch": 0.006544971747538623,
      "grad_norm": 0.8023446202278137,
      "learning_rate": 4.989102102598146e-05,
      "loss": 1.3743,
      "step": 2100
    },
    {
      "epoch": 0.006700804408194305,
      "grad_norm": 0.7081485986709595,
      "learning_rate": 4.988842381497054e-05,
      "loss": 1.347,
      "step": 2150
    },
    {
      "epoch": 0.006856637068849986,
      "grad_norm": 0.656901478767395,
      "learning_rate": 4.9885826603959605e-05,
      "loss": 1.3158,
      "step": 2200
    },
    {
      "epoch": 0.007012469729505668,
      "grad_norm": 0.5949526429176331,
      "learning_rate": 4.988322939294868e-05,
      "loss": 1.372,
      "step": 2250
    },
    {
      "epoch": 0.007168302390161349,
      "grad_norm": 0.6408527493476868,
      "learning_rate": 4.9880632181937756e-05,
      "loss": 1.3549,
      "step": 2300
    },
    {
      "epoch": 0.007324135050817031,
      "grad_norm": 0.7067235708236694,
      "learning_rate": 4.987803497092682e-05,
      "loss": 1.4014,
      "step": 2350
    },
    {
      "epoch": 0.007479967711472712,
      "grad_norm": 0.5071877837181091,
      "learning_rate": 4.9875437759915895e-05,
      "loss": 1.374,
      "step": 2400
    },
    {
      "epoch": 0.007635800372128394,
      "grad_norm": 0.630078911781311,
      "learning_rate": 4.987284054890497e-05,
      "loss": 1.3333,
      "step": 2450
    },
    {
      "epoch": 0.007791633032784075,
      "grad_norm": 0.6559736132621765,
      "learning_rate": 4.987024333789404e-05,
      "loss": 1.4038,
      "step": 2500
    },
    {
      "epoch": 0.007947465693439756,
      "grad_norm": 0.5242152810096741,
      "learning_rate": 4.986764612688311e-05,
      "loss": 1.3425,
      "step": 2550
    },
    {
      "epoch": 0.008103298354095439,
      "grad_norm": 0.624191164970398,
      "learning_rate": 4.986504891587218e-05,
      "loss": 1.3939,
      "step": 2600
    },
    {
      "epoch": 0.00825913101475112,
      "grad_norm": 0.6670030951499939,
      "learning_rate": 4.986245170486125e-05,
      "loss": 1.4125,
      "step": 2650
    },
    {
      "epoch": 0.008414963675406801,
      "grad_norm": 0.6846696138381958,
      "learning_rate": 4.985985449385033e-05,
      "loss": 1.3312,
      "step": 2700
    },
    {
      "epoch": 0.008570796336062482,
      "grad_norm": 0.6871005296707153,
      "learning_rate": 4.9857257282839395e-05,
      "loss": 1.2977,
      "step": 2750
    },
    {
      "epoch": 0.008726628996718165,
      "grad_norm": 0.5817345976829529,
      "learning_rate": 4.985466007182847e-05,
      "loss": 1.3353,
      "step": 2800
    },
    {
      "epoch": 0.008882461657373846,
      "grad_norm": 0.6821984648704529,
      "learning_rate": 4.985206286081755e-05,
      "loss": 1.3334,
      "step": 2850
    },
    {
      "epoch": 0.009038294318029527,
      "grad_norm": 0.6328069567680359,
      "learning_rate": 4.984946564980661e-05,
      "loss": 1.3138,
      "step": 2900
    },
    {
      "epoch": 0.009194126978685208,
      "grad_norm": 0.48368072509765625,
      "learning_rate": 4.9846868438795685e-05,
      "loss": 1.3829,
      "step": 2950
    },
    {
      "epoch": 0.00934995963934089,
      "grad_norm": 0.6541323065757751,
      "learning_rate": 4.984427122778476e-05,
      "loss": 1.3269,
      "step": 3000
    },
    {
      "epoch": 0.009505792299996572,
      "grad_norm": 0.5829547047615051,
      "learning_rate": 4.984167401677383e-05,
      "loss": 1.3431,
      "step": 3050
    },
    {
      "epoch": 0.009661624960652253,
      "grad_norm": 0.6509102582931519,
      "learning_rate": 4.98390768057629e-05,
      "loss": 1.3279,
      "step": 3100
    },
    {
      "epoch": 0.009817457621307934,
      "grad_norm": 0.5398967862129211,
      "learning_rate": 4.9836479594751975e-05,
      "loss": 1.3355,
      "step": 3150
    },
    {
      "epoch": 0.009973290281963617,
      "grad_norm": 0.544200599193573,
      "learning_rate": 4.983388238374105e-05,
      "loss": 1.3666,
      "step": 3200
    },
    {
      "epoch": 0.010129122942619298,
      "grad_norm": 0.6693305969238281,
      "learning_rate": 4.983128517273012e-05,
      "loss": 1.3321,
      "step": 3250
    },
    {
      "epoch": 0.010284955603274979,
      "grad_norm": 0.7142634391784668,
      "learning_rate": 4.9828687961719186e-05,
      "loss": 1.3815,
      "step": 3300
    },
    {
      "epoch": 0.010440788263930662,
      "grad_norm": 0.5172840356826782,
      "learning_rate": 4.982609075070826e-05,
      "loss": 1.4016,
      "step": 3350
    },
    {
      "epoch": 0.010596620924586343,
      "grad_norm": 0.6491748094558716,
      "learning_rate": 4.982349353969734e-05,
      "loss": 1.3099,
      "step": 3400
    },
    {
      "epoch": 0.010752453585242024,
      "grad_norm": 0.6466702222824097,
      "learning_rate": 4.9820896328686404e-05,
      "loss": 1.3245,
      "step": 3450
    },
    {
      "epoch": 0.010908286245897705,
      "grad_norm": 0.5479609370231628,
      "learning_rate": 4.9818299117675476e-05,
      "loss": 1.3265,
      "step": 3500
    },
    {
      "epoch": 0.011064118906553387,
      "grad_norm": 0.38946375250816345,
      "learning_rate": 4.9815701906664555e-05,
      "loss": 1.4065,
      "step": 3550
    },
    {
      "epoch": 0.011219951567209068,
      "grad_norm": 0.5250698328018188,
      "learning_rate": 4.981310469565362e-05,
      "loss": 1.3797,
      "step": 3600
    },
    {
      "epoch": 0.01137578422786475,
      "grad_norm": 0.6882994771003723,
      "learning_rate": 4.9810507484642694e-05,
      "loss": 1.283,
      "step": 3650
    },
    {
      "epoch": 0.01153161688852043,
      "grad_norm": 0.6640287041664124,
      "learning_rate": 4.9807910273631766e-05,
      "loss": 1.3442,
      "step": 3700
    },
    {
      "epoch": 0.011687449549176113,
      "grad_norm": 0.5191344618797302,
      "learning_rate": 4.980531306262084e-05,
      "loss": 1.3216,
      "step": 3750
    },
    {
      "epoch": 0.011843282209831794,
      "grad_norm": 0.6437234878540039,
      "learning_rate": 4.980271585160991e-05,
      "loss": 1.3656,
      "step": 3800
    },
    {
      "epoch": 0.011999114870487475,
      "grad_norm": 0.5990815162658691,
      "learning_rate": 4.9800118640598984e-05,
      "loss": 1.3592,
      "step": 3850
    },
    {
      "epoch": 0.012154947531143157,
      "grad_norm": 0.6728096604347229,
      "learning_rate": 4.979757337380827e-05,
      "loss": 1.3533,
      "step": 3900
    },
    {
      "epoch": 0.01231078019179884,
      "grad_norm": 0.5436310768127441,
      "learning_rate": 4.979497616279735e-05,
      "loss": 1.3088,
      "step": 3950
    },
    {
      "epoch": 0.01246661285245452,
      "grad_norm": 0.5493251085281372,
      "learning_rate": 4.9792378951786414e-05,
      "loss": 1.275,
      "step": 4000
    },
    {
      "epoch": 0.012622445513110201,
      "grad_norm": 0.7027632594108582,
      "learning_rate": 4.9789781740775487e-05,
      "loss": 1.3843,
      "step": 4050
    },
    {
      "epoch": 0.012778278173765882,
      "grad_norm": 0.506352961063385,
      "learning_rate": 4.978718452976456e-05,
      "loss": 1.3024,
      "step": 4100
    },
    {
      "epoch": 0.012934110834421565,
      "grad_norm": 0.6514437794685364,
      "learning_rate": 4.978458731875363e-05,
      "loss": 1.3206,
      "step": 4150
    },
    {
      "epoch": 0.013089943495077246,
      "grad_norm": 0.47791898250579834,
      "learning_rate": 4.9781990107742704e-05,
      "loss": 1.3433,
      "step": 4200
    },
    {
      "epoch": 0.013245776155732927,
      "grad_norm": 0.5080920457839966,
      "learning_rate": 4.977939289673178e-05,
      "loss": 1.2898,
      "step": 4250
    },
    {
      "epoch": 0.01340160881638861,
      "grad_norm": 0.5998125076293945,
      "learning_rate": 4.977679568572085e-05,
      "loss": 1.3015,
      "step": 4300
    },
    {
      "epoch": 0.013557441477044291,
      "grad_norm": 0.5779537558555603,
      "learning_rate": 4.977419847470992e-05,
      "loss": 1.3263,
      "step": 4350
    },
    {
      "epoch": 0.013713274137699972,
      "grad_norm": 0.4653300642967224,
      "learning_rate": 4.9771601263698994e-05,
      "loss": 1.3954,
      "step": 4400
    },
    {
      "epoch": 0.013869106798355653,
      "grad_norm": 0.5466722846031189,
      "learning_rate": 4.976900405268806e-05,
      "loss": 1.3449,
      "step": 4450
    },
    {
      "epoch": 0.014024939459011336,
      "grad_norm": 0.5696160197257996,
      "learning_rate": 4.976640684167714e-05,
      "loss": 1.3637,
      "step": 4500
    },
    {
      "epoch": 0.014180772119667017,
      "grad_norm": 0.448036789894104,
      "learning_rate": 4.976380963066621e-05,
      "loss": 1.4011,
      "step": 4550
    },
    {
      "epoch": 0.014336604780322698,
      "grad_norm": 0.522909939289093,
      "learning_rate": 4.976121241965528e-05,
      "loss": 1.3485,
      "step": 4600
    },
    {
      "epoch": 0.01449243744097838,
      "grad_norm": 0.5425151586532593,
      "learning_rate": 4.975861520864435e-05,
      "loss": 1.2897,
      "step": 4650
    },
    {
      "epoch": 0.014648270101634062,
      "grad_norm": 0.5634627342224121,
      "learning_rate": 4.975601799763342e-05,
      "loss": 1.325,
      "step": 4700
    },
    {
      "epoch": 0.014804102762289743,
      "grad_norm": 0.6236270070075989,
      "learning_rate": 4.9753420786622495e-05,
      "loss": 1.3726,
      "step": 4750
    },
    {
      "epoch": 0.014959935422945424,
      "grad_norm": 0.5920132994651794,
      "learning_rate": 4.975082357561157e-05,
      "loss": 1.3234,
      "step": 4800
    },
    {
      "epoch": 0.015115768083601105,
      "grad_norm": 0.6990236639976501,
      "learning_rate": 4.974822636460064e-05,
      "loss": 1.3209,
      "step": 4850
    },
    {
      "epoch": 0.015271600744256788,
      "grad_norm": 0.7027589082717896,
      "learning_rate": 4.974562915358971e-05,
      "loss": 1.384,
      "step": 4900
    },
    {
      "epoch": 0.015427433404912469,
      "grad_norm": 0.6034034490585327,
      "learning_rate": 4.9743031942578785e-05,
      "loss": 1.4062,
      "step": 4950
    },
    {
      "epoch": 0.01558326606556815,
      "grad_norm": 0.695512056350708,
      "learning_rate": 4.974043473156785e-05,
      "loss": 1.3634,
      "step": 5000
    },
    {
      "epoch": 0.01573909872622383,
      "grad_norm": 0.5534629821777344,
      "learning_rate": 4.973783752055693e-05,
      "loss": 1.3631,
      "step": 5050
    },
    {
      "epoch": 0.015894931386879512,
      "grad_norm": 0.6742116212844849,
      "learning_rate": 4.9735240309546e-05,
      "loss": 1.3132,
      "step": 5100
    },
    {
      "epoch": 0.016050764047535193,
      "grad_norm": 0.47502270340919495,
      "learning_rate": 4.973264309853507e-05,
      "loss": 1.3259,
      "step": 5150
    },
    {
      "epoch": 0.016206596708190878,
      "grad_norm": 0.6603335738182068,
      "learning_rate": 4.973004588752415e-05,
      "loss": 1.3334,
      "step": 5200
    },
    {
      "epoch": 0.01636242936884656,
      "grad_norm": 0.6556662917137146,
      "learning_rate": 4.972744867651321e-05,
      "loss": 1.3637,
      "step": 5250
    },
    {
      "epoch": 0.01651826202950224,
      "grad_norm": 0.5195320844650269,
      "learning_rate": 4.9724851465502286e-05,
      "loss": 1.3566,
      "step": 5300
    },
    {
      "epoch": 0.01667409469015792,
      "grad_norm": 0.7432105541229248,
      "learning_rate": 4.972225425449136e-05,
      "loss": 1.3371,
      "step": 5350
    },
    {
      "epoch": 0.016829927350813602,
      "grad_norm": 0.661855936050415,
      "learning_rate": 4.971965704348043e-05,
      "loss": 1.3612,
      "step": 5400
    },
    {
      "epoch": 0.016985760011469283,
      "grad_norm": 0.651515543460846,
      "learning_rate": 4.97170598324695e-05,
      "loss": 1.368,
      "step": 5450
    },
    {
      "epoch": 0.017141592672124964,
      "grad_norm": 0.6595757007598877,
      "learning_rate": 4.9714462621458576e-05,
      "loss": 1.3603,
      "step": 5500
    },
    {
      "epoch": 0.01729742533278065,
      "grad_norm": 0.6546897292137146,
      "learning_rate": 4.971186541044765e-05,
      "loss": 1.3513,
      "step": 5550
    },
    {
      "epoch": 0.01745325799343633,
      "grad_norm": 0.4904356300830841,
      "learning_rate": 4.970926819943672e-05,
      "loss": 1.3867,
      "step": 5600
    },
    {
      "epoch": 0.01760909065409201,
      "grad_norm": 0.5138305425643921,
      "learning_rate": 4.970667098842579e-05,
      "loss": 1.3129,
      "step": 5650
    },
    {
      "epoch": 0.01776492331474769,
      "grad_norm": 0.5181068181991577,
      "learning_rate": 4.970407377741486e-05,
      "loss": 1.3069,
      "step": 5700
    },
    {
      "epoch": 0.017920755975403373,
      "grad_norm": 0.680880606174469,
      "learning_rate": 4.970147656640394e-05,
      "loss": 1.4085,
      "step": 5750
    },
    {
      "epoch": 0.018076588636059054,
      "grad_norm": 0.7555378079414368,
      "learning_rate": 4.969887935539301e-05,
      "loss": 1.3523,
      "step": 5800
    },
    {
      "epoch": 0.018232421296714735,
      "grad_norm": 0.6456076502799988,
      "learning_rate": 4.9696282144382076e-05,
      "loss": 1.3383,
      "step": 5850
    },
    {
      "epoch": 0.018388253957370416,
      "grad_norm": 0.6247308254241943,
      "learning_rate": 4.969368493337115e-05,
      "loss": 1.3328,
      "step": 5900
    },
    {
      "epoch": 0.0185440866180261,
      "grad_norm": 0.5434100031852722,
      "learning_rate": 4.969108772236022e-05,
      "loss": 1.358,
      "step": 5950
    },
    {
      "epoch": 0.01869991927868178,
      "grad_norm": 0.5679662823677063,
      "learning_rate": 4.9688490511349294e-05,
      "loss": 1.3978,
      "step": 6000
    },
    {
      "epoch": 0.018855751939337462,
      "grad_norm": 0.7174541354179382,
      "learning_rate": 4.9685893300338366e-05,
      "loss": 1.3512,
      "step": 6050
    },
    {
      "epoch": 0.019011584599993143,
      "grad_norm": 0.585736095905304,
      "learning_rate": 4.968329608932744e-05,
      "loss": 1.3125,
      "step": 6100
    },
    {
      "epoch": 0.019167417260648825,
      "grad_norm": 0.6771566271781921,
      "learning_rate": 4.968069887831651e-05,
      "loss": 1.3142,
      "step": 6150
    },
    {
      "epoch": 0.019323249921304506,
      "grad_norm": 0.612395703792572,
      "learning_rate": 4.9678101667305584e-05,
      "loss": 1.3732,
      "step": 6200
    },
    {
      "epoch": 0.019479082581960187,
      "grad_norm": 0.6444770097732544,
      "learning_rate": 4.967550445629465e-05,
      "loss": 1.3294,
      "step": 6250
    },
    {
      "epoch": 0.019634915242615868,
      "grad_norm": 0.6375851631164551,
      "learning_rate": 4.967290724528373e-05,
      "loss": 1.3827,
      "step": 6300
    },
    {
      "epoch": 0.019790747903271552,
      "grad_norm": 0.5187817215919495,
      "learning_rate": 4.96703100342728e-05,
      "loss": 1.3439,
      "step": 6350
    },
    {
      "epoch": 0.019946580563927233,
      "grad_norm": 0.6051862239837646,
      "learning_rate": 4.966771282326187e-05,
      "loss": 1.3019,
      "step": 6400
    },
    {
      "epoch": 0.020102413224582914,
      "grad_norm": 0.7398232221603394,
      "learning_rate": 4.9665115612250946e-05,
      "loss": 1.2952,
      "step": 6450
    },
    {
      "epoch": 0.020258245885238595,
      "grad_norm": 0.4449402689933777,
      "learning_rate": 4.966251840124002e-05,
      "loss": 1.3546,
      "step": 6500
    },
    {
      "epoch": 0.020414078545894276,
      "grad_norm": 0.6513887047767639,
      "learning_rate": 4.9659921190229085e-05,
      "loss": 1.2999,
      "step": 6550
    },
    {
      "epoch": 0.020569911206549957,
      "grad_norm": 0.6030101776123047,
      "learning_rate": 4.965732397921816e-05,
      "loss": 1.2881,
      "step": 6600
    },
    {
      "epoch": 0.02072574386720564,
      "grad_norm": 0.8167698383331299,
      "learning_rate": 4.965472676820723e-05,
      "loss": 1.371,
      "step": 6650
    },
    {
      "epoch": 0.020881576527861323,
      "grad_norm": 0.6563253998756409,
      "learning_rate": 4.96521295571963e-05,
      "loss": 1.3727,
      "step": 6700
    },
    {
      "epoch": 0.021037409188517004,
      "grad_norm": 0.6176069974899292,
      "learning_rate": 4.9649532346185375e-05,
      "loss": 1.3038,
      "step": 6750
    },
    {
      "epoch": 0.021193241849172685,
      "grad_norm": 0.7273112535476685,
      "learning_rate": 4.964693513517445e-05,
      "loss": 1.316,
      "step": 6800
    },
    {
      "epoch": 0.021349074509828366,
      "grad_norm": 0.5471296310424805,
      "learning_rate": 4.964433792416352e-05,
      "loss": 1.3615,
      "step": 6850
    },
    {
      "epoch": 0.021504907170484047,
      "grad_norm": 0.5459910035133362,
      "learning_rate": 4.964174071315259e-05,
      "loss": 1.3627,
      "step": 6900
    },
    {
      "epoch": 0.021660739831139728,
      "grad_norm": 0.5495458245277405,
      "learning_rate": 4.963914350214166e-05,
      "loss": 1.3691,
      "step": 6950
    },
    {
      "epoch": 0.02181657249179541,
      "grad_norm": 0.4113849103450775,
      "learning_rate": 4.963654629113074e-05,
      "loss": 1.2805,
      "step": 7000
    },
    {
      "epoch": 0.02197240515245109,
      "grad_norm": 0.5577514171600342,
      "learning_rate": 4.963394908011981e-05,
      "loss": 1.3433,
      "step": 7050
    },
    {
      "epoch": 0.022128237813106775,
      "grad_norm": 0.7519777417182922,
      "learning_rate": 4.9631351869108875e-05,
      "loss": 1.3349,
      "step": 7100
    },
    {
      "epoch": 0.022284070473762456,
      "grad_norm": 0.6392936706542969,
      "learning_rate": 4.962875465809795e-05,
      "loss": 1.3329,
      "step": 7150
    },
    {
      "epoch": 0.022439903134418137,
      "grad_norm": 0.5335951447486877,
      "learning_rate": 4.962615744708703e-05,
      "loss": 1.3683,
      "step": 7200
    },
    {
      "epoch": 0.022595735795073818,
      "grad_norm": 0.7154409885406494,
      "learning_rate": 4.962356023607609e-05,
      "loss": 1.2872,
      "step": 7250
    },
    {
      "epoch": 0.0227515684557295,
      "grad_norm": 0.5882086157798767,
      "learning_rate": 4.9620963025065165e-05,
      "loss": 1.3291,
      "step": 7300
    },
    {
      "epoch": 0.02290740111638518,
      "grad_norm": 0.659472644329071,
      "learning_rate": 4.961836581405424e-05,
      "loss": 1.3084,
      "step": 7350
    },
    {
      "epoch": 0.02306323377704086,
      "grad_norm": 0.6148248314857483,
      "learning_rate": 4.961576860304331e-05,
      "loss": 1.3104,
      "step": 7400
    },
    {
      "epoch": 0.023219066437696542,
      "grad_norm": 0.6387063264846802,
      "learning_rate": 4.961317139203238e-05,
      "loss": 1.3229,
      "step": 7450
    },
    {
      "epoch": 0.023374899098352227,
      "grad_norm": 0.8223085999488831,
      "learning_rate": 4.961057418102145e-05,
      "loss": 1.3595,
      "step": 7500
    },
    {
      "epoch": 0.023530731759007908,
      "grad_norm": 0.5619892477989197,
      "learning_rate": 4.960797697001053e-05,
      "loss": 1.3739,
      "step": 7550
    },
    {
      "epoch": 0.02368656441966359,
      "grad_norm": 0.7208254933357239,
      "learning_rate": 4.96053797589996e-05,
      "loss": 1.3162,
      "step": 7600
    },
    {
      "epoch": 0.02384239708031927,
      "grad_norm": 0.7374066114425659,
      "learning_rate": 4.9602782547988666e-05,
      "loss": 1.3505,
      "step": 7650
    },
    {
      "epoch": 0.02399822974097495,
      "grad_norm": 0.6434023380279541,
      "learning_rate": 4.9600185336977745e-05,
      "loss": 1.36,
      "step": 7700
    },
    {
      "epoch": 0.024154062401630632,
      "grad_norm": 0.48161375522613525,
      "learning_rate": 4.959758812596682e-05,
      "loss": 1.3598,
      "step": 7750
    },
    {
      "epoch": 0.024309895062286313,
      "grad_norm": 0.5243804454803467,
      "learning_rate": 4.9594990914955884e-05,
      "loss": 1.3052,
      "step": 7800
    },
    {
      "epoch": 0.024465727722941998,
      "grad_norm": 0.4795176684856415,
      "learning_rate": 4.9592393703944956e-05,
      "loss": 1.3715,
      "step": 7850
    },
    {
      "epoch": 0.02462156038359768,
      "grad_norm": 0.5532738566398621,
      "learning_rate": 4.958979649293403e-05,
      "loss": 1.3222,
      "step": 7900
    },
    {
      "epoch": 0.02477739304425336,
      "grad_norm": 0.5473823547363281,
      "learning_rate": 4.95871992819231e-05,
      "loss": 1.3154,
      "step": 7950
    },
    {
      "epoch": 0.02493322570490904,
      "grad_norm": 0.7284204959869385,
      "learning_rate": 4.9584602070912174e-05,
      "loss": 1.3519,
      "step": 8000
    },
    {
      "epoch": 0.025089058365564722,
      "grad_norm": 0.6039177179336548,
      "learning_rate": 4.9582004859901246e-05,
      "loss": 1.3519,
      "step": 8050
    },
    {
      "epoch": 0.025244891026220403,
      "grad_norm": 0.5117738246917725,
      "learning_rate": 4.957940764889032e-05,
      "loss": 1.3051,
      "step": 8100
    },
    {
      "epoch": 0.025400723686876084,
      "grad_norm": 0.4897916913032532,
      "learning_rate": 4.957681043787939e-05,
      "loss": 1.316,
      "step": 8150
    },
    {
      "epoch": 0.025556556347531765,
      "grad_norm": 0.5397919416427612,
      "learning_rate": 4.957421322686846e-05,
      "loss": 1.3182,
      "step": 8200
    },
    {
      "epoch": 0.02571238900818745,
      "grad_norm": 0.47112131118774414,
      "learning_rate": 4.9571616015857536e-05,
      "loss": 1.3098,
      "step": 8250
    },
    {
      "epoch": 0.02586822166884313,
      "grad_norm": 0.542188823223114,
      "learning_rate": 4.956901880484661e-05,
      "loss": 1.4004,
      "step": 8300
    },
    {
      "epoch": 0.02602405432949881,
      "grad_norm": 0.5434668660163879,
      "learning_rate": 4.9566421593835674e-05,
      "loss": 1.3534,
      "step": 8350
    },
    {
      "epoch": 0.026179886990154493,
      "grad_norm": 0.5650601983070374,
      "learning_rate": 4.956382438282475e-05,
      "loss": 1.3127,
      "step": 8400
    },
    {
      "epoch": 0.026335719650810174,
      "grad_norm": 0.6570718884468079,
      "learning_rate": 4.9561279116034046e-05,
      "loss": 1.3106,
      "step": 8450
    },
    {
      "epoch": 0.026491552311465855,
      "grad_norm": 0.6367811560630798,
      "learning_rate": 4.955868190502311e-05,
      "loss": 1.3857,
      "step": 8500
    },
    {
      "epoch": 0.026647384972121536,
      "grad_norm": 0.605301022529602,
      "learning_rate": 4.9556084694012184e-05,
      "loss": 1.357,
      "step": 8550
    },
    {
      "epoch": 0.02680321763277722,
      "grad_norm": 0.6133922934532166,
      "learning_rate": 4.9553487483001257e-05,
      "loss": 1.228,
      "step": 8600
    },
    {
      "epoch": 0.0269590502934329,
      "grad_norm": 0.642794668674469,
      "learning_rate": 4.955089027199033e-05,
      "loss": 1.347,
      "step": 8650
    },
    {
      "epoch": 0.027114882954088582,
      "grad_norm": 0.5705519914627075,
      "learning_rate": 4.95482930609794e-05,
      "loss": 1.3545,
      "step": 8700
    },
    {
      "epoch": 0.027270715614744263,
      "grad_norm": 0.553007185459137,
      "learning_rate": 4.9545695849968474e-05,
      "loss": 1.2859,
      "step": 8750
    },
    {
      "epoch": 0.027426548275399944,
      "grad_norm": 0.6529485583305359,
      "learning_rate": 4.9543098638957547e-05,
      "loss": 1.1841,
      "step": 8800
    },
    {
      "epoch": 0.027582380936055625,
      "grad_norm": 0.46758124232292175,
      "learning_rate": 4.954050142794662e-05,
      "loss": 1.309,
      "step": 8850
    },
    {
      "epoch": 0.027738213596711307,
      "grad_norm": 0.8001310229301453,
      "learning_rate": 4.9537904216935685e-05,
      "loss": 1.3346,
      "step": 8900
    },
    {
      "epoch": 0.027894046257366988,
      "grad_norm": 0.7069540023803711,
      "learning_rate": 4.953530700592476e-05,
      "loss": 1.3777,
      "step": 8950
    },
    {
      "epoch": 0.028049878918022672,
      "grad_norm": 0.5484519600868225,
      "learning_rate": 4.9532709794913837e-05,
      "loss": 1.3825,
      "step": 9000
    },
    {
      "epoch": 0.028205711578678353,
      "grad_norm": 0.6357167363166809,
      "learning_rate": 4.95301125839029e-05,
      "loss": 1.3238,
      "step": 9050
    },
    {
      "epoch": 0.028361544239334034,
      "grad_norm": 0.6089522242546082,
      "learning_rate": 4.9527515372891975e-05,
      "loss": 1.3157,
      "step": 9100
    },
    {
      "epoch": 0.028517376899989715,
      "grad_norm": 0.5416909456253052,
      "learning_rate": 4.952491816188105e-05,
      "loss": 1.3556,
      "step": 9150
    },
    {
      "epoch": 0.028673209560645396,
      "grad_norm": 0.5011885762214661,
      "learning_rate": 4.952232095087012e-05,
      "loss": 1.3482,
      "step": 9200
    },
    {
      "epoch": 0.028829042221301077,
      "grad_norm": 0.6984428763389587,
      "learning_rate": 4.951972373985919e-05,
      "loss": 1.3692,
      "step": 9250
    },
    {
      "epoch": 0.02898487488195676,
      "grad_norm": 0.55859375,
      "learning_rate": 4.9517126528848265e-05,
      "loss": 1.3304,
      "step": 9300
    },
    {
      "epoch": 0.02914070754261244,
      "grad_norm": 0.5255328416824341,
      "learning_rate": 4.951452931783734e-05,
      "loss": 1.3821,
      "step": 9350
    },
    {
      "epoch": 0.029296540203268124,
      "grad_norm": 0.7860026359558105,
      "learning_rate": 4.951193210682641e-05,
      "loss": 1.3003,
      "step": 9400
    },
    {
      "epoch": 0.029452372863923805,
      "grad_norm": 0.4465259313583374,
      "learning_rate": 4.950933489581548e-05,
      "loss": 1.3554,
      "step": 9450
    },
    {
      "epoch": 0.029608205524579486,
      "grad_norm": 0.6106611490249634,
      "learning_rate": 4.950673768480455e-05,
      "loss": 1.3289,
      "step": 9500
    },
    {
      "epoch": 0.029764038185235167,
      "grad_norm": 0.621932327747345,
      "learning_rate": 4.950414047379363e-05,
      "loss": 1.2975,
      "step": 9550
    },
    {
      "epoch": 0.029919870845890848,
      "grad_norm": 0.5347567200660706,
      "learning_rate": 4.950154326278269e-05,
      "loss": 1.3218,
      "step": 9600
    },
    {
      "epoch": 0.03007570350654653,
      "grad_norm": 0.5084118843078613,
      "learning_rate": 4.9498946051771766e-05,
      "loss": 1.3533,
      "step": 9650
    },
    {
      "epoch": 0.03023153616720221,
      "grad_norm": 0.56060791015625,
      "learning_rate": 4.9496348840760845e-05,
      "loss": 1.3555,
      "step": 9700
    },
    {
      "epoch": 0.030387368827857895,
      "grad_norm": 0.5590512156486511,
      "learning_rate": 4.949375162974991e-05,
      "loss": 1.3356,
      "step": 9750
    },
    {
      "epoch": 0.030543201488513576,
      "grad_norm": 0.6450424790382385,
      "learning_rate": 4.949115441873898e-05,
      "loss": 1.4093,
      "step": 9800
    },
    {
      "epoch": 0.030699034149169257,
      "grad_norm": 0.661279022693634,
      "learning_rate": 4.9488557207728056e-05,
      "loss": 1.4056,
      "step": 9850
    },
    {
      "epoch": 0.030854866809824938,
      "grad_norm": 0.518473744392395,
      "learning_rate": 4.948595999671713e-05,
      "loss": 1.3452,
      "step": 9900
    },
    {
      "epoch": 0.03101069947048062,
      "grad_norm": 0.5992773771286011,
      "learning_rate": 4.94833627857062e-05,
      "loss": 1.3307,
      "step": 9950
    },
    {
      "epoch": 0.0311665321311363,
      "grad_norm": 0.6271765232086182,
      "learning_rate": 4.948076557469527e-05,
      "loss": 1.3878,
      "step": 10000
    },
    {
      "epoch": 0.031322364791791985,
      "grad_norm": 0.5613080859184265,
      "learning_rate": 4.9478168363684346e-05,
      "loss": 1.3166,
      "step": 10050
    },
    {
      "epoch": 0.03147819745244766,
      "grad_norm": 0.6463966369628906,
      "learning_rate": 4.947557115267342e-05,
      "loss": 1.3602,
      "step": 10100
    },
    {
      "epoch": 0.03163403011310335,
      "grad_norm": 0.538008451461792,
      "learning_rate": 4.9472973941662484e-05,
      "loss": 1.2859,
      "step": 10150
    },
    {
      "epoch": 0.031789862773759024,
      "grad_norm": 0.5980814099311829,
      "learning_rate": 4.9470376730651556e-05,
      "loss": 1.3513,
      "step": 10200
    },
    {
      "epoch": 0.03194569543441471,
      "grad_norm": 0.5692278146743774,
      "learning_rate": 4.9467779519640636e-05,
      "loss": 1.3503,
      "step": 10250
    },
    {
      "epoch": 0.032101528095070386,
      "grad_norm": 0.6212124228477478,
      "learning_rate": 4.94651823086297e-05,
      "loss": 1.3609,
      "step": 10300
    },
    {
      "epoch": 0.03225736075572607,
      "grad_norm": 0.6525622606277466,
      "learning_rate": 4.9462585097618774e-05,
      "loss": 1.3097,
      "step": 10350
    },
    {
      "epoch": 0.032413193416381755,
      "grad_norm": 0.6026220321655273,
      "learning_rate": 4.9459987886607846e-05,
      "loss": 1.3698,
      "step": 10400
    },
    {
      "epoch": 0.03256902607703743,
      "grad_norm": 0.6604406237602234,
      "learning_rate": 4.945739067559692e-05,
      "loss": 1.3371,
      "step": 10450
    },
    {
      "epoch": 0.03272485873769312,
      "grad_norm": 0.5572109222412109,
      "learning_rate": 4.945479346458599e-05,
      "loss": 1.3117,
      "step": 10500
    },
    {
      "epoch": 0.032880691398348795,
      "grad_norm": 0.6909775137901306,
      "learning_rate": 4.9452196253575064e-05,
      "loss": 1.3815,
      "step": 10550
    },
    {
      "epoch": 0.03303652405900448,
      "grad_norm": 0.6230343580245972,
      "learning_rate": 4.9449599042564136e-05,
      "loss": 1.36,
      "step": 10600
    },
    {
      "epoch": 0.03319235671966016,
      "grad_norm": 0.5915206074714661,
      "learning_rate": 4.944700183155321e-05,
      "loss": 1.3198,
      "step": 10650
    },
    {
      "epoch": 0.03334818938031584,
      "grad_norm": 0.6122881770133972,
      "learning_rate": 4.944440462054228e-05,
      "loss": 1.2859,
      "step": 10700
    },
    {
      "epoch": 0.033504022040971526,
      "grad_norm": 0.43910908699035645,
      "learning_rate": 4.944180740953135e-05,
      "loss": 1.3356,
      "step": 10750
    },
    {
      "epoch": 0.033659854701627204,
      "grad_norm": 0.4497618079185486,
      "learning_rate": 4.9439210198520426e-05,
      "loss": 1.3222,
      "step": 10800
    },
    {
      "epoch": 0.03381568736228289,
      "grad_norm": 0.5217254161834717,
      "learning_rate": 4.943661298750949e-05,
      "loss": 1.3407,
      "step": 10850
    },
    {
      "epoch": 0.033971520022938566,
      "grad_norm": 0.4973808228969574,
      "learning_rate": 4.9434015776498565e-05,
      "loss": 1.2809,
      "step": 10900
    },
    {
      "epoch": 0.03412735268359425,
      "grad_norm": 0.6376396417617798,
      "learning_rate": 4.9431418565487644e-05,
      "loss": 1.3312,
      "step": 10950
    },
    {
      "epoch": 0.03428318534424993,
      "grad_norm": 0.5133669972419739,
      "learning_rate": 4.942882135447671e-05,
      "loss": 1.2954,
      "step": 11000
    },
    {
      "epoch": 0.03443901800490561,
      "grad_norm": 0.552364706993103,
      "learning_rate": 4.9426276087686e-05,
      "loss": 1.376,
      "step": 11050
    },
    {
      "epoch": 0.0345948506655613,
      "grad_norm": 0.5814685225486755,
      "learning_rate": 4.9423678876675074e-05,
      "loss": 1.3327,
      "step": 11100
    },
    {
      "epoch": 0.034750683326216975,
      "grad_norm": 0.6514005661010742,
      "learning_rate": 4.942108166566414e-05,
      "loss": 1.3459,
      "step": 11150
    },
    {
      "epoch": 0.03490651598687266,
      "grad_norm": 0.521371066570282,
      "learning_rate": 4.941848445465322e-05,
      "loss": 1.3374,
      "step": 11200
    },
    {
      "epoch": 0.03506234864752834,
      "grad_norm": 0.5665985941886902,
      "learning_rate": 4.941588724364229e-05,
      "loss": 1.3564,
      "step": 11250
    },
    {
      "epoch": 0.03521818130818402,
      "grad_norm": 0.45458582043647766,
      "learning_rate": 4.941329003263136e-05,
      "loss": 1.2821,
      "step": 11300
    },
    {
      "epoch": 0.0353740139688397,
      "grad_norm": 0.585964024066925,
      "learning_rate": 4.941069282162044e-05,
      "loss": 1.3341,
      "step": 11350
    },
    {
      "epoch": 0.03552984662949538,
      "grad_norm": 0.6169666647911072,
      "learning_rate": 4.940809561060951e-05,
      "loss": 1.3536,
      "step": 11400
    },
    {
      "epoch": 0.03568567929015106,
      "grad_norm": 0.6826348900794983,
      "learning_rate": 4.9405498399598575e-05,
      "loss": 1.3513,
      "step": 11450
    },
    {
      "epoch": 0.035841511950806745,
      "grad_norm": 0.6145089864730835,
      "learning_rate": 4.940290118858765e-05,
      "loss": 1.3231,
      "step": 11500
    },
    {
      "epoch": 0.03599734461146243,
      "grad_norm": 0.6278401017189026,
      "learning_rate": 4.940030397757672e-05,
      "loss": 1.3846,
      "step": 11550
    },
    {
      "epoch": 0.03615317727211811,
      "grad_norm": 0.7355685830116272,
      "learning_rate": 4.939770676656579e-05,
      "loss": 1.32,
      "step": 11600
    },
    {
      "epoch": 0.03630900993277379,
      "grad_norm": 0.6501304507255554,
      "learning_rate": 4.9395109555554865e-05,
      "loss": 1.3614,
      "step": 11650
    },
    {
      "epoch": 0.03646484259342947,
      "grad_norm": 0.6337055563926697,
      "learning_rate": 4.939251234454394e-05,
      "loss": 1.301,
      "step": 11700
    },
    {
      "epoch": 0.036620675254085154,
      "grad_norm": 0.41295114159584045,
      "learning_rate": 4.938991513353301e-05,
      "loss": 1.3173,
      "step": 11750
    },
    {
      "epoch": 0.03677650791474083,
      "grad_norm": 0.5521849989891052,
      "learning_rate": 4.938731792252208e-05,
      "loss": 1.3033,
      "step": 11800
    },
    {
      "epoch": 0.036932340575396516,
      "grad_norm": 0.6295973658561707,
      "learning_rate": 4.938472071151115e-05,
      "loss": 1.3176,
      "step": 11850
    },
    {
      "epoch": 0.0370881732360522,
      "grad_norm": 0.6295578479766846,
      "learning_rate": 4.938212350050023e-05,
      "loss": 1.3094,
      "step": 11900
    },
    {
      "epoch": 0.03724400589670788,
      "grad_norm": 0.5667539834976196,
      "learning_rate": 4.93795262894893e-05,
      "loss": 1.2738,
      "step": 11950
    },
    {
      "epoch": 0.03739983855736356,
      "grad_norm": 0.4580700397491455,
      "learning_rate": 4.9376929078478366e-05,
      "loss": 1.3188,
      "step": 12000
    },
    {
      "epoch": 0.03755567121801924,
      "grad_norm": 0.5771214365959167,
      "learning_rate": 4.9374331867467445e-05,
      "loss": 1.4024,
      "step": 12050
    },
    {
      "epoch": 0.037711503878674925,
      "grad_norm": 0.5450723767280579,
      "learning_rate": 4.937173465645652e-05,
      "loss": 1.3214,
      "step": 12100
    },
    {
      "epoch": 0.0378673365393306,
      "grad_norm": 0.5245606303215027,
      "learning_rate": 4.936913744544558e-05,
      "loss": 1.3502,
      "step": 12150
    },
    {
      "epoch": 0.03802316919998629,
      "grad_norm": 0.5243322849273682,
      "learning_rate": 4.9366540234434656e-05,
      "loss": 1.2793,
      "step": 12200
    },
    {
      "epoch": 0.03817900186064197,
      "grad_norm": 0.6634674668312073,
      "learning_rate": 4.936394302342373e-05,
      "loss": 1.2964,
      "step": 12250
    },
    {
      "epoch": 0.03833483452129765,
      "grad_norm": 0.6987941861152649,
      "learning_rate": 4.93613458124128e-05,
      "loss": 1.3196,
      "step": 12300
    },
    {
      "epoch": 0.038490667181953334,
      "grad_norm": 0.6231584548950195,
      "learning_rate": 4.935874860140187e-05,
      "loss": 1.3387,
      "step": 12350
    },
    {
      "epoch": 0.03864649984260901,
      "grad_norm": 0.5934983491897583,
      "learning_rate": 4.935615139039094e-05,
      "loss": 1.3363,
      "step": 12400
    },
    {
      "epoch": 0.038802332503264696,
      "grad_norm": 0.6165801882743835,
      "learning_rate": 4.935355417938002e-05,
      "loss": 1.2793,
      "step": 12450
    },
    {
      "epoch": 0.03895816516392037,
      "grad_norm": 0.5420882701873779,
      "learning_rate": 4.935095696836909e-05,
      "loss": 1.3474,
      "step": 12500
    },
    {
      "epoch": 0.03911399782457606,
      "grad_norm": 0.73361736536026,
      "learning_rate": 4.9348359757358157e-05,
      "loss": 1.2829,
      "step": 12550
    },
    {
      "epoch": 0.039269830485231735,
      "grad_norm": 0.6302063465118408,
      "learning_rate": 4.9345762546347236e-05,
      "loss": 1.3381,
      "step": 12600
    },
    {
      "epoch": 0.03942566314588742,
      "grad_norm": 0.5337669253349304,
      "learning_rate": 4.934316533533631e-05,
      "loss": 1.3625,
      "step": 12650
    },
    {
      "epoch": 0.039581495806543104,
      "grad_norm": 0.6561464071273804,
      "learning_rate": 4.9340568124325374e-05,
      "loss": 1.2956,
      "step": 12700
    },
    {
      "epoch": 0.03973732846719878,
      "grad_norm": 0.6201326251029968,
      "learning_rate": 4.9337970913314447e-05,
      "loss": 1.3199,
      "step": 12750
    },
    {
      "epoch": 0.039893161127854466,
      "grad_norm": 0.5381652116775513,
      "learning_rate": 4.933537370230352e-05,
      "loss": 1.2778,
      "step": 12800
    },
    {
      "epoch": 0.040048993788510144,
      "grad_norm": 0.4777800440788269,
      "learning_rate": 4.933277649129259e-05,
      "loss": 1.314,
      "step": 12850
    },
    {
      "epoch": 0.04020482644916583,
      "grad_norm": 0.6574421525001526,
      "learning_rate": 4.9330179280281664e-05,
      "loss": 1.3168,
      "step": 12900
    },
    {
      "epoch": 0.040360659109821506,
      "grad_norm": 0.6006549596786499,
      "learning_rate": 4.9327582069270737e-05,
      "loss": 1.3104,
      "step": 12950
    },
    {
      "epoch": 0.04051649177047719,
      "grad_norm": 0.47469907999038696,
      "learning_rate": 4.932498485825981e-05,
      "loss": 1.3135,
      "step": 13000
    },
    {
      "epoch": 0.040672324431132875,
      "grad_norm": 0.6917524933815002,
      "learning_rate": 4.932238764724888e-05,
      "loss": 1.3433,
      "step": 13050
    },
    {
      "epoch": 0.04082815709178855,
      "grad_norm": 0.5787776708602905,
      "learning_rate": 4.931979043623795e-05,
      "loss": 1.2815,
      "step": 13100
    },
    {
      "epoch": 0.04098398975244424,
      "grad_norm": 0.47281724214553833,
      "learning_rate": 4.9317193225227027e-05,
      "loss": 1.3527,
      "step": 13150
    },
    {
      "epoch": 0.041139822413099915,
      "grad_norm": 0.47664567828178406,
      "learning_rate": 4.93145960142161e-05,
      "loss": 1.3975,
      "step": 13200
    },
    {
      "epoch": 0.0412956550737556,
      "grad_norm": 0.4908343553543091,
      "learning_rate": 4.9311998803205165e-05,
      "loss": 1.3468,
      "step": 13250
    },
    {
      "epoch": 0.04145148773441128,
      "grad_norm": 0.721848726272583,
      "learning_rate": 4.9309401592194244e-05,
      "loss": 1.3437,
      "step": 13300
    },
    {
      "epoch": 0.04160732039506696,
      "grad_norm": 0.5941927433013916,
      "learning_rate": 4.9306804381183317e-05,
      "loss": 1.2906,
      "step": 13350
    },
    {
      "epoch": 0.041763153055722646,
      "grad_norm": 0.6098179817199707,
      "learning_rate": 4.930420717017238e-05,
      "loss": 1.3365,
      "step": 13400
    },
    {
      "epoch": 0.041918985716378324,
      "grad_norm": 0.4796716570854187,
      "learning_rate": 4.9301609959161455e-05,
      "loss": 1.3181,
      "step": 13450
    },
    {
      "epoch": 0.04207481837703401,
      "grad_norm": 0.6073000431060791,
      "learning_rate": 4.929901274815053e-05,
      "loss": 1.355,
      "step": 13500
    },
    {
      "epoch": 0.042230651037689686,
      "grad_norm": 0.6920226812362671,
      "learning_rate": 4.92964155371396e-05,
      "loss": 1.3006,
      "step": 13550
    },
    {
      "epoch": 0.04238648369834537,
      "grad_norm": 0.6108245849609375,
      "learning_rate": 4.929381832612867e-05,
      "loss": 1.3726,
      "step": 13600
    },
    {
      "epoch": 0.04254231635900105,
      "grad_norm": 0.6233474016189575,
      "learning_rate": 4.9291221115117745e-05,
      "loss": 1.2834,
      "step": 13650
    },
    {
      "epoch": 0.04269814901965673,
      "grad_norm": 0.7085373997688293,
      "learning_rate": 4.928862390410682e-05,
      "loss": 1.2549,
      "step": 13700
    },
    {
      "epoch": 0.04285398168031241,
      "grad_norm": 0.5703209042549133,
      "learning_rate": 4.928602669309589e-05,
      "loss": 1.3308,
      "step": 13750
    },
    {
      "epoch": 0.043009814340968094,
      "grad_norm": 0.4790760278701782,
      "learning_rate": 4.9283429482084956e-05,
      "loss": 1.3281,
      "step": 13800
    },
    {
      "epoch": 0.04316564700162378,
      "grad_norm": 0.6729546189308167,
      "learning_rate": 4.9280832271074035e-05,
      "loss": 1.3309,
      "step": 13850
    },
    {
      "epoch": 0.043321479662279457,
      "grad_norm": 0.6412696242332458,
      "learning_rate": 4.927823506006311e-05,
      "loss": 1.3094,
      "step": 13900
    },
    {
      "epoch": 0.04347731232293514,
      "grad_norm": 0.6497218608856201,
      "learning_rate": 4.927563784905217e-05,
      "loss": 1.3233,
      "step": 13950
    },
    {
      "epoch": 0.04363314498359082,
      "grad_norm": 0.5260319709777832,
      "learning_rate": 4.9273040638041246e-05,
      "loss": 1.3158,
      "step": 14000
    },
    {
      "epoch": 0.0437889776442465,
      "grad_norm": 0.5534887909889221,
      "learning_rate": 4.9270443427030325e-05,
      "loss": 1.3589,
      "step": 14050
    },
    {
      "epoch": 0.04394481030490218,
      "grad_norm": 0.6038696765899658,
      "learning_rate": 4.926784621601939e-05,
      "loss": 1.3196,
      "step": 14100
    },
    {
      "epoch": 0.044100642965557865,
      "grad_norm": 0.7177510857582092,
      "learning_rate": 4.926524900500846e-05,
      "loss": 1.3298,
      "step": 14150
    },
    {
      "epoch": 0.04425647562621355,
      "grad_norm": 0.7040425539016724,
      "learning_rate": 4.9262651793997536e-05,
      "loss": 1.3082,
      "step": 14200
    },
    {
      "epoch": 0.04441230828686923,
      "grad_norm": 0.5971282720565796,
      "learning_rate": 4.926005458298661e-05,
      "loss": 1.3539,
      "step": 14250
    },
    {
      "epoch": 0.04456814094752491,
      "grad_norm": 0.5138790011405945,
      "learning_rate": 4.925745737197568e-05,
      "loss": 1.3127,
      "step": 14300
    },
    {
      "epoch": 0.04472397360818059,
      "grad_norm": 0.5469639301300049,
      "learning_rate": 4.925486016096475e-05,
      "loss": 1.3652,
      "step": 14350
    },
    {
      "epoch": 0.044879806268836274,
      "grad_norm": 0.5001862049102783,
      "learning_rate": 4.9252262949953826e-05,
      "loss": 1.3055,
      "step": 14400
    },
    {
      "epoch": 0.04503563892949195,
      "grad_norm": 0.7014634013175964,
      "learning_rate": 4.92496657389429e-05,
      "loss": 1.3256,
      "step": 14450
    },
    {
      "epoch": 0.045191471590147636,
      "grad_norm": 0.473521888256073,
      "learning_rate": 4.9247068527931964e-05,
      "loss": 1.2734,
      "step": 14500
    },
    {
      "epoch": 0.04534730425080332,
      "grad_norm": 0.5744761824607849,
      "learning_rate": 4.924447131692104e-05,
      "loss": 1.3287,
      "step": 14550
    },
    {
      "epoch": 0.045503136911459,
      "grad_norm": 0.5863804221153259,
      "learning_rate": 4.9241874105910116e-05,
      "loss": 1.3037,
      "step": 14600
    },
    {
      "epoch": 0.04565896957211468,
      "grad_norm": 0.5033121109008789,
      "learning_rate": 4.923927689489918e-05,
      "loss": 1.3028,
      "step": 14650
    },
    {
      "epoch": 0.04581480223277036,
      "grad_norm": 0.5229918360710144,
      "learning_rate": 4.9236679683888254e-05,
      "loss": 1.2957,
      "step": 14700
    },
    {
      "epoch": 0.045970634893426045,
      "grad_norm": 0.6024394631385803,
      "learning_rate": 4.923408247287733e-05,
      "loss": 1.3955,
      "step": 14750
    },
    {
      "epoch": 0.04612646755408172,
      "grad_norm": 0.5664291381835938,
      "learning_rate": 4.92314852618664e-05,
      "loss": 1.3453,
      "step": 14800
    },
    {
      "epoch": 0.04628230021473741,
      "grad_norm": 0.707773745059967,
      "learning_rate": 4.922893999507569e-05,
      "loss": 1.3015,
      "step": 14850
    },
    {
      "epoch": 0.046438132875393084,
      "grad_norm": 0.6748160123825073,
      "learning_rate": 4.9226342784064763e-05,
      "loss": 1.3255,
      "step": 14900
    },
    {
      "epoch": 0.04659396553604877,
      "grad_norm": 0.5577136278152466,
      "learning_rate": 4.9223745573053836e-05,
      "loss": 1.3657,
      "step": 14950
    },
    {
      "epoch": 0.04674979819670445,
      "grad_norm": 0.6500369310379028,
      "learning_rate": 4.922114836204291e-05,
      "loss": 1.3015,
      "step": 15000
    },
    {
      "epoch": 0.04690563085736013,
      "grad_norm": 0.6255702972412109,
      "learning_rate": 4.9218551151031974e-05,
      "loss": 1.2758,
      "step": 15050
    },
    {
      "epoch": 0.047061463518015816,
      "grad_norm": 0.6593366861343384,
      "learning_rate": 4.921595394002105e-05,
      "loss": 1.2773,
      "step": 15100
    },
    {
      "epoch": 0.04721729617867149,
      "grad_norm": 0.5971003174781799,
      "learning_rate": 4.9213356729010126e-05,
      "loss": 1.2887,
      "step": 15150
    },
    {
      "epoch": 0.04737312883932718,
      "grad_norm": 0.5243743658065796,
      "learning_rate": 4.921075951799919e-05,
      "loss": 1.3119,
      "step": 15200
    },
    {
      "epoch": 0.047528961499982855,
      "grad_norm": 0.5865691900253296,
      "learning_rate": 4.9208162306988264e-05,
      "loss": 1.2678,
      "step": 15250
    },
    {
      "epoch": 0.04768479416063854,
      "grad_norm": 0.5299168229103088,
      "learning_rate": 4.9205565095977343e-05,
      "loss": 1.3204,
      "step": 15300
    },
    {
      "epoch": 0.047840626821294224,
      "grad_norm": 0.6551268696784973,
      "learning_rate": 4.920296788496641e-05,
      "loss": 1.3388,
      "step": 15350
    },
    {
      "epoch": 0.0479964594819499,
      "grad_norm": 0.5958969593048096,
      "learning_rate": 4.920037067395548e-05,
      "loss": 1.2684,
      "step": 15400
    },
    {
      "epoch": 0.048152292142605586,
      "grad_norm": 0.6927958726882935,
      "learning_rate": 4.9197773462944554e-05,
      "loss": 1.3014,
      "step": 15450
    },
    {
      "epoch": 0.048308124803261264,
      "grad_norm": 0.636989951133728,
      "learning_rate": 4.919517625193363e-05,
      "loss": 1.3207,
      "step": 15500
    },
    {
      "epoch": 0.04846395746391695,
      "grad_norm": 0.5205816030502319,
      "learning_rate": 4.91925790409227e-05,
      "loss": 1.2947,
      "step": 15550
    },
    {
      "epoch": 0.048619790124572626,
      "grad_norm": 0.6730199456214905,
      "learning_rate": 4.918998182991177e-05,
      "loss": 1.3285,
      "step": 15600
    },
    {
      "epoch": 0.04877562278522831,
      "grad_norm": 0.6243571639060974,
      "learning_rate": 4.918738461890084e-05,
      "loss": 1.3069,
      "step": 15650
    },
    {
      "epoch": 0.048931455445883995,
      "grad_norm": 0.7028855085372925,
      "learning_rate": 4.918478740788992e-05,
      "loss": 1.3349,
      "step": 15700
    },
    {
      "epoch": 0.04908728810653967,
      "grad_norm": 0.4967229664325714,
      "learning_rate": 4.918219019687898e-05,
      "loss": 1.312,
      "step": 15750
    },
    {
      "epoch": 0.04924312076719536,
      "grad_norm": 0.6167621612548828,
      "learning_rate": 4.9179592985868055e-05,
      "loss": 1.3329,
      "step": 15800
    },
    {
      "epoch": 0.049398953427851035,
      "grad_norm": 0.6000052690505981,
      "learning_rate": 4.9176995774857134e-05,
      "loss": 1.3614,
      "step": 15850
    },
    {
      "epoch": 0.04955478608850672,
      "grad_norm": 0.5442448258399963,
      "learning_rate": 4.91743985638462e-05,
      "loss": 1.2987,
      "step": 15900
    },
    {
      "epoch": 0.0497106187491624,
      "grad_norm": 0.6132346391677856,
      "learning_rate": 4.917180135283527e-05,
      "loss": 1.3538,
      "step": 15950
    },
    {
      "epoch": 0.04986645140981808,
      "grad_norm": 0.5367105007171631,
      "learning_rate": 4.9169204141824345e-05,
      "loss": 1.3374,
      "step": 16000
    },
    {
      "epoch": 0.050022284070473766,
      "grad_norm": 0.6514263153076172,
      "learning_rate": 4.916660693081342e-05,
      "loss": 1.2956,
      "step": 16050
    },
    {
      "epoch": 0.050178116731129443,
      "grad_norm": 0.4962891936302185,
      "learning_rate": 4.916400971980249e-05,
      "loss": 1.2984,
      "step": 16100
    },
    {
      "epoch": 0.05033394939178513,
      "grad_norm": 0.546183705329895,
      "learning_rate": 4.916141250879156e-05,
      "loss": 1.2724,
      "step": 16150
    },
    {
      "epoch": 0.050489782052440806,
      "grad_norm": 0.6512373089790344,
      "learning_rate": 4.9158815297780635e-05,
      "loss": 1.2927,
      "step": 16200
    },
    {
      "epoch": 0.05064561471309649,
      "grad_norm": 0.5714925527572632,
      "learning_rate": 4.915621808676971e-05,
      "loss": 1.2953,
      "step": 16250
    },
    {
      "epoch": 0.05080144737375217,
      "grad_norm": 0.569648802280426,
      "learning_rate": 4.915362087575878e-05,
      "loss": 1.3459,
      "step": 16300
    },
    {
      "epoch": 0.05095728003440785,
      "grad_norm": 0.571098804473877,
      "learning_rate": 4.9151023664747846e-05,
      "loss": 1.3769,
      "step": 16350
    },
    {
      "epoch": 0.05111311269506353,
      "grad_norm": 0.5413248538970947,
      "learning_rate": 4.9148426453736925e-05,
      "loss": 1.3174,
      "step": 16400
    },
    {
      "epoch": 0.051268945355719214,
      "grad_norm": 0.41414687037467957,
      "learning_rate": 4.914582924272599e-05,
      "loss": 1.3615,
      "step": 16450
    },
    {
      "epoch": 0.0514247780163749,
      "grad_norm": 0.5618326663970947,
      "learning_rate": 4.914323203171506e-05,
      "loss": 1.2907,
      "step": 16500
    },
    {
      "epoch": 0.051580610677030576,
      "grad_norm": 0.6484780311584473,
      "learning_rate": 4.914063482070414e-05,
      "loss": 1.3312,
      "step": 16550
    },
    {
      "epoch": 0.05173644333768626,
      "grad_norm": 0.7405387759208679,
      "learning_rate": 4.913803760969321e-05,
      "loss": 1.393,
      "step": 16600
    },
    {
      "epoch": 0.05189227599834194,
      "grad_norm": 0.5075403451919556,
      "learning_rate": 4.913544039868228e-05,
      "loss": 1.3467,
      "step": 16650
    },
    {
      "epoch": 0.05204810865899762,
      "grad_norm": 0.604575514793396,
      "learning_rate": 4.913284318767135e-05,
      "loss": 1.2855,
      "step": 16700
    },
    {
      "epoch": 0.0522039413196533,
      "grad_norm": 0.6261927485466003,
      "learning_rate": 4.9130245976660426e-05,
      "loss": 1.3134,
      "step": 16750
    },
    {
      "epoch": 0.052359773980308985,
      "grad_norm": 0.5492614507675171,
      "learning_rate": 4.91276487656495e-05,
      "loss": 1.2947,
      "step": 16800
    },
    {
      "epoch": 0.05251560664096467,
      "grad_norm": 0.6490501165390015,
      "learning_rate": 4.912505155463857e-05,
      "loss": 1.3131,
      "step": 16850
    },
    {
      "epoch": 0.05267143930162035,
      "grad_norm": 0.7340015769004822,
      "learning_rate": 4.9122454343627636e-05,
      "loss": 1.2876,
      "step": 16900
    },
    {
      "epoch": 0.05282727196227603,
      "grad_norm": 0.6413069367408752,
      "learning_rate": 4.9119857132616716e-05,
      "loss": 1.3271,
      "step": 16950
    },
    {
      "epoch": 0.05298310462293171,
      "grad_norm": 0.5778433680534363,
      "learning_rate": 4.911725992160579e-05,
      "loss": 1.2708,
      "step": 17000
    },
    {
      "epoch": 0.053138937283587394,
      "grad_norm": 0.5903887152671814,
      "learning_rate": 4.9114662710594854e-05,
      "loss": 1.2893,
      "step": 17050
    },
    {
      "epoch": 0.05329476994424307,
      "grad_norm": 0.6623691916465759,
      "learning_rate": 4.911206549958393e-05,
      "loss": 1.2756,
      "step": 17100
    },
    {
      "epoch": 0.053450602604898756,
      "grad_norm": 0.5450072884559631,
      "learning_rate": 4.9109468288573e-05,
      "loss": 1.3285,
      "step": 17150
    },
    {
      "epoch": 0.05360643526555444,
      "grad_norm": 0.6721314787864685,
      "learning_rate": 4.910687107756207e-05,
      "loss": 1.3641,
      "step": 17200
    },
    {
      "epoch": 0.05376226792621012,
      "grad_norm": 0.7195709943771362,
      "learning_rate": 4.9104273866551144e-05,
      "loss": 1.3405,
      "step": 17250
    },
    {
      "epoch": 0.0539181005868658,
      "grad_norm": 0.6551278233528137,
      "learning_rate": 4.9101676655540216e-05,
      "loss": 1.3339,
      "step": 17300
    },
    {
      "epoch": 0.05407393324752148,
      "grad_norm": 0.5622365474700928,
      "learning_rate": 4.909907944452929e-05,
      "loss": 1.3244,
      "step": 17350
    },
    {
      "epoch": 0.054229765908177165,
      "grad_norm": 0.5875991582870483,
      "learning_rate": 4.909648223351836e-05,
      "loss": 1.2706,
      "step": 17400
    },
    {
      "epoch": 0.05438559856883284,
      "grad_norm": 0.6812454462051392,
      "learning_rate": 4.9093885022507434e-05,
      "loss": 1.2863,
      "step": 17450
    },
    {
      "epoch": 0.05454143122948853,
      "grad_norm": 0.6270332336425781,
      "learning_rate": 4.9091287811496506e-05,
      "loss": 1.3164,
      "step": 17500
    },
    {
      "epoch": 0.054697263890144204,
      "grad_norm": 0.6896236538887024,
      "learning_rate": 4.908869060048558e-05,
      "loss": 1.3069,
      "step": 17550
    },
    {
      "epoch": 0.05485309655079989,
      "grad_norm": 0.5506631731987,
      "learning_rate": 4.9086093389474645e-05,
      "loss": 1.2963,
      "step": 17600
    },
    {
      "epoch": 0.05500892921145557,
      "grad_norm": 0.6167387366294861,
      "learning_rate": 4.9083496178463724e-05,
      "loss": 1.3244,
      "step": 17650
    },
    {
      "epoch": 0.05516476187211125,
      "grad_norm": 0.6447786092758179,
      "learning_rate": 4.908089896745279e-05,
      "loss": 1.3085,
      "step": 17700
    },
    {
      "epoch": 0.055320594532766935,
      "grad_norm": 0.7191133499145508,
      "learning_rate": 4.907830175644186e-05,
      "loss": 1.3436,
      "step": 17750
    },
    {
      "epoch": 0.05547642719342261,
      "grad_norm": 0.5406160950660706,
      "learning_rate": 4.907570454543094e-05,
      "loss": 1.3204,
      "step": 17800
    },
    {
      "epoch": 0.0556322598540783,
      "grad_norm": 0.4558112621307373,
      "learning_rate": 4.907310733442001e-05,
      "loss": 1.3339,
      "step": 17850
    },
    {
      "epoch": 0.055788092514733975,
      "grad_norm": 0.5859459042549133,
      "learning_rate": 4.907051012340908e-05,
      "loss": 1.3761,
      "step": 17900
    },
    {
      "epoch": 0.05594392517538966,
      "grad_norm": 0.6202095746994019,
      "learning_rate": 4.906791291239815e-05,
      "loss": 1.3178,
      "step": 17950
    },
    {
      "epoch": 0.056099757836045344,
      "grad_norm": 0.689379096031189,
      "learning_rate": 4.9065315701387225e-05,
      "loss": 1.3302,
      "step": 18000
    },
    {
      "epoch": 0.05625559049670102,
      "grad_norm": 0.5299913883209229,
      "learning_rate": 4.90627184903763e-05,
      "loss": 1.3465,
      "step": 18050
    },
    {
      "epoch": 0.056411423157356706,
      "grad_norm": 0.5098860263824463,
      "learning_rate": 4.906012127936537e-05,
      "loss": 1.3622,
      "step": 18100
    },
    {
      "epoch": 0.056567255818012384,
      "grad_norm": 0.5675342679023743,
      "learning_rate": 4.9057524068354435e-05,
      "loss": 1.3201,
      "step": 18150
    },
    {
      "epoch": 0.05672308847866807,
      "grad_norm": 0.5817385911941528,
      "learning_rate": 4.9054926857343515e-05,
      "loss": 1.3284,
      "step": 18200
    },
    {
      "epoch": 0.056878921139323746,
      "grad_norm": 0.6674662828445435,
      "learning_rate": 4.905238159055281e-05,
      "loss": 1.3009,
      "step": 18250
    },
    {
      "epoch": 0.05703475379997943,
      "grad_norm": 0.5713239908218384,
      "learning_rate": 4.904978437954187e-05,
      "loss": 1.3294,
      "step": 18300
    },
    {
      "epoch": 0.057190586460635115,
      "grad_norm": 0.6600815057754517,
      "learning_rate": 4.9047187168530945e-05,
      "loss": 1.2839,
      "step": 18350
    },
    {
      "epoch": 0.05734641912129079,
      "grad_norm": 0.5996883511543274,
      "learning_rate": 4.904458995752002e-05,
      "loss": 1.3043,
      "step": 18400
    },
    {
      "epoch": 0.05750225178194648,
      "grad_norm": 0.6518127918243408,
      "learning_rate": 4.904199274650909e-05,
      "loss": 1.3164,
      "step": 18450
    },
    {
      "epoch": 0.057658084442602155,
      "grad_norm": 0.5598846077919006,
      "learning_rate": 4.903939553549816e-05,
      "loss": 1.3695,
      "step": 18500
    },
    {
      "epoch": 0.05781391710325784,
      "grad_norm": 0.6593405604362488,
      "learning_rate": 4.9036798324487235e-05,
      "loss": 1.2691,
      "step": 18550
    },
    {
      "epoch": 0.05796974976391352,
      "grad_norm": 0.743005633354187,
      "learning_rate": 4.903420111347631e-05,
      "loss": 1.301,
      "step": 18600
    },
    {
      "epoch": 0.0581255824245692,
      "grad_norm": 0.5752042531967163,
      "learning_rate": 4.903160390246538e-05,
      "loss": 1.3439,
      "step": 18650
    },
    {
      "epoch": 0.05828141508522488,
      "grad_norm": 0.5409443974494934,
      "learning_rate": 4.9029006691454446e-05,
      "loss": 1.3284,
      "step": 18700
    },
    {
      "epoch": 0.05843724774588056,
      "grad_norm": 0.6798097491264343,
      "learning_rate": 4.9026409480443525e-05,
      "loss": 1.304,
      "step": 18750
    },
    {
      "epoch": 0.05859308040653625,
      "grad_norm": 0.5730852484703064,
      "learning_rate": 4.90238122694326e-05,
      "loss": 1.3731,
      "step": 18800
    },
    {
      "epoch": 0.058748913067191925,
      "grad_norm": 0.4426526129245758,
      "learning_rate": 4.902121505842166e-05,
      "loss": 1.279,
      "step": 18850
    },
    {
      "epoch": 0.05890474572784761,
      "grad_norm": 0.4902353286743164,
      "learning_rate": 4.9018617847410736e-05,
      "loss": 1.3224,
      "step": 18900
    },
    {
      "epoch": 0.05906057838850329,
      "grad_norm": 0.6324201822280884,
      "learning_rate": 4.9016020636399815e-05,
      "loss": 1.2755,
      "step": 18950
    },
    {
      "epoch": 0.05921641104915897,
      "grad_norm": 0.6180155277252197,
      "learning_rate": 4.901342342538888e-05,
      "loss": 1.3294,
      "step": 19000
    },
    {
      "epoch": 0.05937224370981465,
      "grad_norm": 0.5692262053489685,
      "learning_rate": 4.9010826214377953e-05,
      "loss": 1.3096,
      "step": 19050
    },
    {
      "epoch": 0.059528076370470334,
      "grad_norm": 0.5690851807594299,
      "learning_rate": 4.9008229003367026e-05,
      "loss": 1.3216,
      "step": 19100
    },
    {
      "epoch": 0.05968390903112602,
      "grad_norm": 0.5450013279914856,
      "learning_rate": 4.90056317923561e-05,
      "loss": 1.3329,
      "step": 19150
    },
    {
      "epoch": 0.059839741691781696,
      "grad_norm": 0.7321640849113464,
      "learning_rate": 4.900303458134517e-05,
      "loss": 1.2952,
      "step": 19200
    },
    {
      "epoch": 0.05999557435243738,
      "grad_norm": 0.5724133849143982,
      "learning_rate": 4.9000437370334243e-05,
      "loss": 1.3218,
      "step": 19250
    },
    {
      "epoch": 0.06015140701309306,
      "grad_norm": 0.6162165403366089,
      "learning_rate": 4.8997840159323316e-05,
      "loss": 1.3889,
      "step": 19300
    },
    {
      "epoch": 0.06030723967374874,
      "grad_norm": 0.5620720982551575,
      "learning_rate": 4.899529489253261e-05,
      "loss": 1.313,
      "step": 19350
    },
    {
      "epoch": 0.06046307233440442,
      "grad_norm": 0.6020556092262268,
      "learning_rate": 4.8992697681521674e-05,
      "loss": 1.2723,
      "step": 19400
    },
    {
      "epoch": 0.060618904995060105,
      "grad_norm": 0.5881445407867432,
      "learning_rate": 4.8990100470510746e-05,
      "loss": 1.3593,
      "step": 19450
    },
    {
      "epoch": 0.06077473765571579,
      "grad_norm": 0.5690835118293762,
      "learning_rate": 4.8987503259499826e-05,
      "loss": 1.3196,
      "step": 19500
    },
    {
      "epoch": 0.06093057031637147,
      "grad_norm": 0.5006747245788574,
      "learning_rate": 4.898490604848889e-05,
      "loss": 1.3004,
      "step": 19550
    },
    {
      "epoch": 0.06108640297702715,
      "grad_norm": 0.6575213670730591,
      "learning_rate": 4.8982308837477964e-05,
      "loss": 1.3203,
      "step": 19600
    },
    {
      "epoch": 0.06124223563768283,
      "grad_norm": 0.4860774278640747,
      "learning_rate": 4.897971162646704e-05,
      "loss": 1.3231,
      "step": 19650
    },
    {
      "epoch": 0.061398068298338514,
      "grad_norm": 0.6008149981498718,
      "learning_rate": 4.897711441545611e-05,
      "loss": 1.3628,
      "step": 19700
    },
    {
      "epoch": 0.06155390095899419,
      "grad_norm": 0.4796428084373474,
      "learning_rate": 4.897451720444518e-05,
      "loss": 1.3454,
      "step": 19750
    },
    {
      "epoch": 0.061709733619649876,
      "grad_norm": 0.5289350748062134,
      "learning_rate": 4.8971919993434254e-05,
      "loss": 1.3141,
      "step": 19800
    },
    {
      "epoch": 0.06186556628030555,
      "grad_norm": 0.5922225117683411,
      "learning_rate": 4.8969322782423326e-05,
      "loss": 1.286,
      "step": 19850
    },
    {
      "epoch": 0.06202139894096124,
      "grad_norm": 0.501168429851532,
      "learning_rate": 4.89667255714124e-05,
      "loss": 1.3518,
      "step": 19900
    },
    {
      "epoch": 0.06217723160161692,
      "grad_norm": 0.4977298676967621,
      "learning_rate": 4.8964128360401465e-05,
      "loss": 1.3765,
      "step": 19950
    },
    {
      "epoch": 0.0623330642622726,
      "grad_norm": 0.5489521622657776,
      "learning_rate": 4.896153114939054e-05,
      "loss": 1.2961,
      "step": 20000
    },
    {
      "epoch": 0.062488896922928285,
      "grad_norm": 0.5225569009780884,
      "learning_rate": 4.8958933938379616e-05,
      "loss": 1.3535,
      "step": 20050
    },
    {
      "epoch": 0.06264472958358397,
      "grad_norm": 0.5628852844238281,
      "learning_rate": 4.895633672736868e-05,
      "loss": 1.3331,
      "step": 20100
    },
    {
      "epoch": 0.06280056224423965,
      "grad_norm": 0.6725828647613525,
      "learning_rate": 4.8953739516357755e-05,
      "loss": 1.2977,
      "step": 20150
    },
    {
      "epoch": 0.06295639490489532,
      "grad_norm": 0.5704392194747925,
      "learning_rate": 4.8951142305346834e-05,
      "loss": 1.2599,
      "step": 20200
    },
    {
      "epoch": 0.063112227565551,
      "grad_norm": 0.44948986172676086,
      "learning_rate": 4.89485450943359e-05,
      "loss": 1.3113,
      "step": 20250
    },
    {
      "epoch": 0.0632680602262067,
      "grad_norm": 0.7019140124320984,
      "learning_rate": 4.894594788332497e-05,
      "loss": 1.3302,
      "step": 20300
    },
    {
      "epoch": 0.06342389288686237,
      "grad_norm": 0.5210559368133545,
      "learning_rate": 4.8943350672314045e-05,
      "loss": 1.2972,
      "step": 20350
    },
    {
      "epoch": 0.06357972554751805,
      "grad_norm": 0.7642370462417603,
      "learning_rate": 4.894075346130312e-05,
      "loss": 1.321,
      "step": 20400
    },
    {
      "epoch": 0.06373555820817374,
      "grad_norm": 0.5920374393463135,
      "learning_rate": 4.893815625029219e-05,
      "loss": 1.2618,
      "step": 20450
    },
    {
      "epoch": 0.06389139086882942,
      "grad_norm": 0.5976455211639404,
      "learning_rate": 4.893555903928126e-05,
      "loss": 1.3206,
      "step": 20500
    },
    {
      "epoch": 0.0640472235294851,
      "grad_norm": 0.6907004117965698,
      "learning_rate": 4.8932961828270335e-05,
      "loss": 1.321,
      "step": 20550
    },
    {
      "epoch": 0.06420305619014077,
      "grad_norm": 0.5480582118034363,
      "learning_rate": 4.893036461725941e-05,
      "loss": 1.3481,
      "step": 20600
    },
    {
      "epoch": 0.06435888885079646,
      "grad_norm": 0.44297078251838684,
      "learning_rate": 4.892776740624847e-05,
      "loss": 1.3043,
      "step": 20650
    },
    {
      "epoch": 0.06451472151145214,
      "grad_norm": 0.574670135974884,
      "learning_rate": 4.8925170195237545e-05,
      "loss": 1.3462,
      "step": 20700
    },
    {
      "epoch": 0.06467055417210782,
      "grad_norm": 0.5024979114532471,
      "learning_rate": 4.8922572984226625e-05,
      "loss": 1.3528,
      "step": 20750
    },
    {
      "epoch": 0.06482638683276351,
      "grad_norm": 0.557810366153717,
      "learning_rate": 4.891997577321569e-05,
      "loss": 1.3051,
      "step": 20800
    },
    {
      "epoch": 0.06498221949341919,
      "grad_norm": 0.6091673374176025,
      "learning_rate": 4.891737856220476e-05,
      "loss": 1.3088,
      "step": 20850
    },
    {
      "epoch": 0.06513805215407487,
      "grad_norm": 0.6039398312568665,
      "learning_rate": 4.891478135119384e-05,
      "loss": 1.2198,
      "step": 20900
    },
    {
      "epoch": 0.06529388481473054,
      "grad_norm": 0.6169371008872986,
      "learning_rate": 4.891218414018291e-05,
      "loss": 1.3189,
      "step": 20950
    },
    {
      "epoch": 0.06544971747538623,
      "grad_norm": 0.6271533370018005,
      "learning_rate": 4.890958692917198e-05,
      "loss": 1.2992,
      "step": 21000
    },
    {
      "epoch": 0.06560555013604191,
      "grad_norm": 0.4997614026069641,
      "learning_rate": 4.890698971816105e-05,
      "loss": 1.3626,
      "step": 21050
    },
    {
      "epoch": 0.06576138279669759,
      "grad_norm": 0.5651363730430603,
      "learning_rate": 4.8904392507150125e-05,
      "loss": 1.3593,
      "step": 21100
    },
    {
      "epoch": 0.06591721545735328,
      "grad_norm": 0.5678090453147888,
      "learning_rate": 4.89017952961392e-05,
      "loss": 1.3216,
      "step": 21150
    },
    {
      "epoch": 0.06607304811800896,
      "grad_norm": 0.6098047494888306,
      "learning_rate": 4.889919808512827e-05,
      "loss": 1.3337,
      "step": 21200
    },
    {
      "epoch": 0.06622888077866464,
      "grad_norm": 0.5465180277824402,
      "learning_rate": 4.8896600874117336e-05,
      "loss": 1.3683,
      "step": 21250
    },
    {
      "epoch": 0.06638471343932031,
      "grad_norm": 0.7699115872383118,
      "learning_rate": 4.8894003663106415e-05,
      "loss": 1.3562,
      "step": 21300
    },
    {
      "epoch": 0.066540546099976,
      "grad_norm": 0.5809499621391296,
      "learning_rate": 4.889140645209548e-05,
      "loss": 1.3,
      "step": 21350
    },
    {
      "epoch": 0.06669637876063168,
      "grad_norm": 0.5701432228088379,
      "learning_rate": 4.8888809241084554e-05,
      "loss": 1.2709,
      "step": 21400
    },
    {
      "epoch": 0.06685221142128736,
      "grad_norm": 0.758977472782135,
      "learning_rate": 4.8886263974293846e-05,
      "loss": 1.3257,
      "step": 21450
    },
    {
      "epoch": 0.06700804408194305,
      "grad_norm": 0.6445344090461731,
      "learning_rate": 4.888366676328292e-05,
      "loss": 1.2743,
      "step": 21500
    },
    {
      "epoch": 0.06716387674259873,
      "grad_norm": 0.555061936378479,
      "learning_rate": 4.888106955227199e-05,
      "loss": 1.3595,
      "step": 21550
    },
    {
      "epoch": 0.06731970940325441,
      "grad_norm": 0.5705407857894897,
      "learning_rate": 4.887847234126106e-05,
      "loss": 1.3765,
      "step": 21600
    },
    {
      "epoch": 0.06747554206391009,
      "grad_norm": 0.6184156537055969,
      "learning_rate": 4.8875875130250136e-05,
      "loss": 1.286,
      "step": 21650
    },
    {
      "epoch": 0.06763137472456578,
      "grad_norm": 0.6059477925300598,
      "learning_rate": 4.887327791923921e-05,
      "loss": 1.3143,
      "step": 21700
    },
    {
      "epoch": 0.06778720738522145,
      "grad_norm": 0.5727159976959229,
      "learning_rate": 4.887068070822828e-05,
      "loss": 1.3399,
      "step": 21750
    },
    {
      "epoch": 0.06794304004587713,
      "grad_norm": 0.5234657526016235,
      "learning_rate": 4.8868083497217346e-05,
      "loss": 1.3489,
      "step": 21800
    },
    {
      "epoch": 0.06809887270653282,
      "grad_norm": 0.6317455768585205,
      "learning_rate": 4.8865486286206426e-05,
      "loss": 1.3089,
      "step": 21850
    },
    {
      "epoch": 0.0682547053671885,
      "grad_norm": 0.5059946179389954,
      "learning_rate": 4.88628890751955e-05,
      "loss": 1.3558,
      "step": 21900
    },
    {
      "epoch": 0.06841053802784418,
      "grad_norm": 0.6959717273712158,
      "learning_rate": 4.8860291864184564e-05,
      "loss": 1.3109,
      "step": 21950
    },
    {
      "epoch": 0.06856637068849986,
      "grad_norm": 0.49464496970176697,
      "learning_rate": 4.8857694653173636e-05,
      "loss": 1.3246,
      "step": 22000
    },
    {
      "epoch": 0.06872220334915555,
      "grad_norm": 0.625086784362793,
      "learning_rate": 4.885509744216271e-05,
      "loss": 1.3414,
      "step": 22050
    },
    {
      "epoch": 0.06887803600981122,
      "grad_norm": 0.47617632150650024,
      "learning_rate": 4.885250023115178e-05,
      "loss": 1.2982,
      "step": 22100
    },
    {
      "epoch": 0.0690338686704669,
      "grad_norm": 0.6678531169891357,
      "learning_rate": 4.8849903020140854e-05,
      "loss": 1.3273,
      "step": 22150
    },
    {
      "epoch": 0.0691897013311226,
      "grad_norm": 0.7217774987220764,
      "learning_rate": 4.8847305809129926e-05,
      "loss": 1.3316,
      "step": 22200
    },
    {
      "epoch": 0.06934553399177827,
      "grad_norm": 0.6119312644004822,
      "learning_rate": 4.8844708598119e-05,
      "loss": 1.2744,
      "step": 22250
    },
    {
      "epoch": 0.06950136665243395,
      "grad_norm": 0.6433269381523132,
      "learning_rate": 4.884211138710807e-05,
      "loss": 1.3378,
      "step": 22300
    },
    {
      "epoch": 0.06965719931308963,
      "grad_norm": 0.490968257188797,
      "learning_rate": 4.883951417609714e-05,
      "loss": 1.2953,
      "step": 22350
    },
    {
      "epoch": 0.06981303197374532,
      "grad_norm": 0.6703467965126038,
      "learning_rate": 4.8836916965086216e-05,
      "loss": 1.3439,
      "step": 22400
    },
    {
      "epoch": 0.069968864634401,
      "grad_norm": 0.56235271692276,
      "learning_rate": 4.883431975407529e-05,
      "loss": 1.2425,
      "step": 22450
    },
    {
      "epoch": 0.07012469729505667,
      "grad_norm": 0.609748363494873,
      "learning_rate": 4.8831722543064355e-05,
      "loss": 1.3285,
      "step": 22500
    },
    {
      "epoch": 0.07028052995571236,
      "grad_norm": 0.6493157148361206,
      "learning_rate": 4.8829125332053434e-05,
      "loss": 1.349,
      "step": 22550
    },
    {
      "epoch": 0.07043636261636804,
      "grad_norm": 0.604738712310791,
      "learning_rate": 4.88265281210425e-05,
      "loss": 1.3572,
      "step": 22600
    },
    {
      "epoch": 0.07059219527702372,
      "grad_norm": 0.8235921859741211,
      "learning_rate": 4.882393091003157e-05,
      "loss": 1.2884,
      "step": 22650
    },
    {
      "epoch": 0.0707480279376794,
      "grad_norm": 0.5615694522857666,
      "learning_rate": 4.8821333699020645e-05,
      "loss": 1.2759,
      "step": 22700
    },
    {
      "epoch": 0.07090386059833509,
      "grad_norm": 0.48990097641944885,
      "learning_rate": 4.881873648800972e-05,
      "loss": 1.3041,
      "step": 22750
    },
    {
      "epoch": 0.07105969325899077,
      "grad_norm": 0.5821700692176819,
      "learning_rate": 4.881613927699879e-05,
      "loss": 1.3166,
      "step": 22800
    },
    {
      "epoch": 0.07121552591964644,
      "grad_norm": 0.5834290981292725,
      "learning_rate": 4.881354206598786e-05,
      "loss": 1.3873,
      "step": 22850
    },
    {
      "epoch": 0.07137135858030212,
      "grad_norm": 0.6749212145805359,
      "learning_rate": 4.8810944854976935e-05,
      "loss": 1.3138,
      "step": 22900
    },
    {
      "epoch": 0.07152719124095781,
      "grad_norm": 0.6298218965530396,
      "learning_rate": 4.880834764396601e-05,
      "loss": 1.3097,
      "step": 22950
    },
    {
      "epoch": 0.07168302390161349,
      "grad_norm": 0.6802829504013062,
      "learning_rate": 4.880575043295508e-05,
      "loss": 1.3189,
      "step": 23000
    },
    {
      "epoch": 0.07183885656226917,
      "grad_norm": 0.5389738082885742,
      "learning_rate": 4.8803153221944145e-05,
      "loss": 1.3235,
      "step": 23050
    },
    {
      "epoch": 0.07199468922292486,
      "grad_norm": 0.5327343344688416,
      "learning_rate": 4.8800556010933225e-05,
      "loss": 1.3033,
      "step": 23100
    },
    {
      "epoch": 0.07215052188358054,
      "grad_norm": 0.5894644260406494,
      "learning_rate": 4.87979587999223e-05,
      "loss": 1.3107,
      "step": 23150
    },
    {
      "epoch": 0.07230635454423621,
      "grad_norm": 0.44949355721473694,
      "learning_rate": 4.879536158891136e-05,
      "loss": 1.3812,
      "step": 23200
    },
    {
      "epoch": 0.07246218720489189,
      "grad_norm": 0.5796131491661072,
      "learning_rate": 4.8792764377900435e-05,
      "loss": 1.3049,
      "step": 23250
    },
    {
      "epoch": 0.07261801986554758,
      "grad_norm": 0.6458222270011902,
      "learning_rate": 4.879016716688951e-05,
      "loss": 1.2745,
      "step": 23300
    },
    {
      "epoch": 0.07277385252620326,
      "grad_norm": 0.7238809466362,
      "learning_rate": 4.878756995587858e-05,
      "loss": 1.2985,
      "step": 23350
    },
    {
      "epoch": 0.07292968518685894,
      "grad_norm": 0.6198951005935669,
      "learning_rate": 4.878497274486765e-05,
      "loss": 1.2667,
      "step": 23400
    },
    {
      "epoch": 0.07308551784751463,
      "grad_norm": 0.5496981739997864,
      "learning_rate": 4.8782375533856725e-05,
      "loss": 1.2791,
      "step": 23450
    },
    {
      "epoch": 0.07324135050817031,
      "grad_norm": 0.6416502594947815,
      "learning_rate": 4.87797783228458e-05,
      "loss": 1.3842,
      "step": 23500
    },
    {
      "epoch": 0.07339718316882599,
      "grad_norm": 0.5842108130455017,
      "learning_rate": 4.877718111183487e-05,
      "loss": 1.3241,
      "step": 23550
    },
    {
      "epoch": 0.07355301582948166,
      "grad_norm": 0.6848686337471008,
      "learning_rate": 4.8774583900823936e-05,
      "loss": 1.3133,
      "step": 23600
    },
    {
      "epoch": 0.07370884849013735,
      "grad_norm": 1.0583744049072266,
      "learning_rate": 4.8771986689813015e-05,
      "loss": 1.3289,
      "step": 23650
    },
    {
      "epoch": 0.07386468115079303,
      "grad_norm": 0.7801441550254822,
      "learning_rate": 4.876938947880209e-05,
      "loss": 1.2918,
      "step": 23700
    },
    {
      "epoch": 0.07402051381144871,
      "grad_norm": 0.45920199155807495,
      "learning_rate": 4.8766792267791154e-05,
      "loss": 1.3642,
      "step": 23750
    },
    {
      "epoch": 0.0741763464721044,
      "grad_norm": 0.7062236666679382,
      "learning_rate": 4.876419505678023e-05,
      "loss": 1.3213,
      "step": 23800
    },
    {
      "epoch": 0.07433217913276008,
      "grad_norm": 0.7822418212890625,
      "learning_rate": 4.8761597845769306e-05,
      "loss": 1.327,
      "step": 23850
    },
    {
      "epoch": 0.07448801179341576,
      "grad_norm": 0.667065441608429,
      "learning_rate": 4.875900063475837e-05,
      "loss": 1.3143,
      "step": 23900
    },
    {
      "epoch": 0.07464384445407143,
      "grad_norm": 0.631476104259491,
      "learning_rate": 4.8756403423747444e-05,
      "loss": 1.2901,
      "step": 23950
    },
    {
      "epoch": 0.07479967711472713,
      "grad_norm": 0.5849165320396423,
      "learning_rate": 4.8753806212736516e-05,
      "loss": 1.3239,
      "step": 24000
    },
    {
      "epoch": 0.0749555097753828,
      "grad_norm": 0.5540856122970581,
      "learning_rate": 4.875120900172559e-05,
      "loss": 1.288,
      "step": 24050
    },
    {
      "epoch": 0.07511134243603848,
      "grad_norm": 0.6739856004714966,
      "learning_rate": 4.874861179071466e-05,
      "loss": 1.302,
      "step": 24100
    },
    {
      "epoch": 0.07526717509669417,
      "grad_norm": 0.4955354630947113,
      "learning_rate": 4.8746014579703734e-05,
      "loss": 1.2983,
      "step": 24150
    },
    {
      "epoch": 0.07542300775734985,
      "grad_norm": 0.6656898856163025,
      "learning_rate": 4.8743417368692806e-05,
      "loss": 1.3576,
      "step": 24200
    },
    {
      "epoch": 0.07557884041800553,
      "grad_norm": 0.5592969655990601,
      "learning_rate": 4.874082015768188e-05,
      "loss": 1.3123,
      "step": 24250
    },
    {
      "epoch": 0.0757346730786612,
      "grad_norm": 0.5264102816581726,
      "learning_rate": 4.8738222946670944e-05,
      "loss": 1.3005,
      "step": 24300
    },
    {
      "epoch": 0.0758905057393169,
      "grad_norm": 0.7291808724403381,
      "learning_rate": 4.8735625735660024e-05,
      "loss": 1.3282,
      "step": 24350
    },
    {
      "epoch": 0.07604633839997257,
      "grad_norm": 0.45666784048080444,
      "learning_rate": 4.8733028524649096e-05,
      "loss": 1.342,
      "step": 24400
    },
    {
      "epoch": 0.07620217106062825,
      "grad_norm": 0.6577968597412109,
      "learning_rate": 4.873043131363816e-05,
      "loss": 1.3268,
      "step": 24450
    },
    {
      "epoch": 0.07635800372128394,
      "grad_norm": 0.5946276783943176,
      "learning_rate": 4.8727834102627234e-05,
      "loss": 1.345,
      "step": 24500
    },
    {
      "epoch": 0.07651383638193962,
      "grad_norm": 0.6477664709091187,
      "learning_rate": 4.8725236891616314e-05,
      "loss": 1.3052,
      "step": 24550
    },
    {
      "epoch": 0.0766696690425953,
      "grad_norm": 0.668798565864563,
      "learning_rate": 4.872263968060538e-05,
      "loss": 1.3234,
      "step": 24600
    },
    {
      "epoch": 0.07682550170325098,
      "grad_norm": 0.5122951865196228,
      "learning_rate": 4.872004246959445e-05,
      "loss": 1.3431,
      "step": 24650
    },
    {
      "epoch": 0.07698133436390667,
      "grad_norm": 0.5365049242973328,
      "learning_rate": 4.8717445258583525e-05,
      "loss": 1.3337,
      "step": 24700
    },
    {
      "epoch": 0.07713716702456234,
      "grad_norm": 0.6942126750946045,
      "learning_rate": 4.87148480475726e-05,
      "loss": 1.3485,
      "step": 24750
    },
    {
      "epoch": 0.07729299968521802,
      "grad_norm": 0.5132336616516113,
      "learning_rate": 4.871225083656167e-05,
      "loss": 1.2878,
      "step": 24800
    },
    {
      "epoch": 0.07744883234587371,
      "grad_norm": 0.6851671934127808,
      "learning_rate": 4.8709653625550735e-05,
      "loss": 1.3406,
      "step": 24850
    },
    {
      "epoch": 0.07760466500652939,
      "grad_norm": 0.6723471283912659,
      "learning_rate": 4.8707056414539815e-05,
      "loss": 1.3033,
      "step": 24900
    },
    {
      "epoch": 0.07776049766718507,
      "grad_norm": 0.6415911316871643,
      "learning_rate": 4.870445920352889e-05,
      "loss": 1.3112,
      "step": 24950
    },
    {
      "epoch": 0.07791633032784075,
      "grad_norm": 0.6979864239692688,
      "learning_rate": 4.870186199251795e-05,
      "loss": 1.3454,
      "step": 25000
    },
    {
      "epoch": 0.07807216298849644,
      "grad_norm": 0.6185047626495361,
      "learning_rate": 4.869926478150703e-05,
      "loss": 1.2904,
      "step": 25050
    },
    {
      "epoch": 0.07822799564915212,
      "grad_norm": 0.6900172829627991,
      "learning_rate": 4.8696667570496105e-05,
      "loss": 1.2874,
      "step": 25100
    },
    {
      "epoch": 0.0783838283098078,
      "grad_norm": 0.673552930355072,
      "learning_rate": 4.869407035948517e-05,
      "loss": 1.2543,
      "step": 25150
    },
    {
      "epoch": 0.07853966097046347,
      "grad_norm": 0.5815678238868713,
      "learning_rate": 4.869147314847424e-05,
      "loss": 1.3205,
      "step": 25200
    },
    {
      "epoch": 0.07869549363111916,
      "grad_norm": 0.5036421418190002,
      "learning_rate": 4.8688875937463315e-05,
      "loss": 1.2586,
      "step": 25250
    },
    {
      "epoch": 0.07885132629177484,
      "grad_norm": 0.5760003924369812,
      "learning_rate": 4.868627872645239e-05,
      "loss": 1.3012,
      "step": 25300
    },
    {
      "epoch": 0.07900715895243052,
      "grad_norm": 0.6121105551719666,
      "learning_rate": 4.868368151544146e-05,
      "loss": 1.3072,
      "step": 25350
    },
    {
      "epoch": 0.07916299161308621,
      "grad_norm": 0.5517157316207886,
      "learning_rate": 4.868108430443053e-05,
      "loss": 1.2943,
      "step": 25400
    },
    {
      "epoch": 0.07931882427374189,
      "grad_norm": 0.5097983479499817,
      "learning_rate": 4.8678487093419605e-05,
      "loss": 1.2294,
      "step": 25450
    },
    {
      "epoch": 0.07947465693439756,
      "grad_norm": 0.543792724609375,
      "learning_rate": 4.867588988240868e-05,
      "loss": 1.362,
      "step": 25500
    },
    {
      "epoch": 0.07963048959505324,
      "grad_norm": 0.5806373357772827,
      "learning_rate": 4.8673292671397743e-05,
      "loss": 1.3715,
      "step": 25550
    },
    {
      "epoch": 0.07978632225570893,
      "grad_norm": 0.5713666081428528,
      "learning_rate": 4.867069546038682e-05,
      "loss": 1.3047,
      "step": 25600
    },
    {
      "epoch": 0.07994215491636461,
      "grad_norm": 0.6790603399276733,
      "learning_rate": 4.8668098249375895e-05,
      "loss": 1.3286,
      "step": 25650
    },
    {
      "epoch": 0.08009798757702029,
      "grad_norm": 0.6284769773483276,
      "learning_rate": 4.866550103836496e-05,
      "loss": 1.3309,
      "step": 25700
    },
    {
      "epoch": 0.08025382023767598,
      "grad_norm": 0.543469250202179,
      "learning_rate": 4.8662903827354034e-05,
      "loss": 1.3061,
      "step": 25750
    },
    {
      "epoch": 0.08040965289833166,
      "grad_norm": 0.6739099621772766,
      "learning_rate": 4.866035856056333e-05,
      "loss": 1.283,
      "step": 25800
    },
    {
      "epoch": 0.08056548555898733,
      "grad_norm": 0.675101101398468,
      "learning_rate": 4.86577613495524e-05,
      "loss": 1.3333,
      "step": 25850
    },
    {
      "epoch": 0.08072131821964301,
      "grad_norm": 0.7747710347175598,
      "learning_rate": 4.865516413854147e-05,
      "loss": 1.314,
      "step": 25900
    },
    {
      "epoch": 0.0808771508802987,
      "grad_norm": 0.7223939895629883,
      "learning_rate": 4.865256692753054e-05,
      "loss": 1.3309,
      "step": 25950
    },
    {
      "epoch": 0.08103298354095438,
      "grad_norm": 0.59212327003479,
      "learning_rate": 4.8649969716519616e-05,
      "loss": 1.3182,
      "step": 26000
    },
    {
      "epoch": 0.08118881620161006,
      "grad_norm": 0.5804698467254639,
      "learning_rate": 4.864737250550869e-05,
      "loss": 1.3078,
      "step": 26050
    },
    {
      "epoch": 0.08134464886226575,
      "grad_norm": 0.5777401328086853,
      "learning_rate": 4.864477529449776e-05,
      "loss": 1.2909,
      "step": 26100
    },
    {
      "epoch": 0.08150048152292143,
      "grad_norm": 0.5187009572982788,
      "learning_rate": 4.864217808348683e-05,
      "loss": 1.3015,
      "step": 26150
    },
    {
      "epoch": 0.0816563141835771,
      "grad_norm": 0.5848783254623413,
      "learning_rate": 4.8639580872475906e-05,
      "loss": 1.2547,
      "step": 26200
    },
    {
      "epoch": 0.08181214684423278,
      "grad_norm": 0.5879573225975037,
      "learning_rate": 4.863698366146497e-05,
      "loss": 1.3055,
      "step": 26250
    },
    {
      "epoch": 0.08196797950488847,
      "grad_norm": 0.6756590008735657,
      "learning_rate": 4.8634386450454044e-05,
      "loss": 1.3036,
      "step": 26300
    },
    {
      "epoch": 0.08212381216554415,
      "grad_norm": 0.5728438496589661,
      "learning_rate": 4.863178923944312e-05,
      "loss": 1.2937,
      "step": 26350
    },
    {
      "epoch": 0.08227964482619983,
      "grad_norm": 1.0252790451049805,
      "learning_rate": 4.862919202843219e-05,
      "loss": 1.3057,
      "step": 26400
    },
    {
      "epoch": 0.08243547748685552,
      "grad_norm": 0.6106794476509094,
      "learning_rate": 4.862659481742126e-05,
      "loss": 1.2939,
      "step": 26450
    },
    {
      "epoch": 0.0825913101475112,
      "grad_norm": 0.6229058504104614,
      "learning_rate": 4.8623997606410334e-05,
      "loss": 1.371,
      "step": 26500
    },
    {
      "epoch": 0.08274714280816688,
      "grad_norm": 0.60804283618927,
      "learning_rate": 4.8621400395399406e-05,
      "loss": 1.3505,
      "step": 26550
    },
    {
      "epoch": 0.08290297546882255,
      "grad_norm": 0.4263008236885071,
      "learning_rate": 4.861880318438848e-05,
      "loss": 1.2995,
      "step": 26600
    },
    {
      "epoch": 0.08305880812947825,
      "grad_norm": 0.589276909828186,
      "learning_rate": 4.861620597337755e-05,
      "loss": 1.2985,
      "step": 26650
    },
    {
      "epoch": 0.08321464079013392,
      "grad_norm": 0.6521909236907959,
      "learning_rate": 4.8613608762366624e-05,
      "loss": 1.2953,
      "step": 26700
    },
    {
      "epoch": 0.0833704734507896,
      "grad_norm": 0.5743646025657654,
      "learning_rate": 4.8611011551355696e-05,
      "loss": 1.3062,
      "step": 26750
    },
    {
      "epoch": 0.08352630611144529,
      "grad_norm": 0.44747593998908997,
      "learning_rate": 4.860841434034477e-05,
      "loss": 1.3363,
      "step": 26800
    },
    {
      "epoch": 0.08368213877210097,
      "grad_norm": 0.5838092565536499,
      "learning_rate": 4.8605817129333835e-05,
      "loss": 1.2979,
      "step": 26850
    },
    {
      "epoch": 0.08383797143275665,
      "grad_norm": 0.6615011096000671,
      "learning_rate": 4.8603271862543134e-05,
      "loss": 1.2849,
      "step": 26900
    },
    {
      "epoch": 0.08399380409341232,
      "grad_norm": 0.6520996689796448,
      "learning_rate": 4.86006746515322e-05,
      "loss": 1.3914,
      "step": 26950
    },
    {
      "epoch": 0.08414963675406802,
      "grad_norm": 0.7245404720306396,
      "learning_rate": 4.859807744052127e-05,
      "loss": 1.2696,
      "step": 27000
    },
    {
      "epoch": 0.0843054694147237,
      "grad_norm": 0.6013381481170654,
      "learning_rate": 4.8595480229510344e-05,
      "loss": 1.2848,
      "step": 27050
    },
    {
      "epoch": 0.08446130207537937,
      "grad_norm": 0.5838293433189392,
      "learning_rate": 4.859288301849942e-05,
      "loss": 1.3175,
      "step": 27100
    },
    {
      "epoch": 0.08461713473603506,
      "grad_norm": 0.6606952548027039,
      "learning_rate": 4.859028580748849e-05,
      "loss": 1.331,
      "step": 27150
    },
    {
      "epoch": 0.08477296739669074,
      "grad_norm": 0.5730252265930176,
      "learning_rate": 4.858768859647756e-05,
      "loss": 1.2974,
      "step": 27200
    },
    {
      "epoch": 0.08492880005734642,
      "grad_norm": 0.7196479439735413,
      "learning_rate": 4.858509138546663e-05,
      "loss": 1.3651,
      "step": 27250
    },
    {
      "epoch": 0.0850846327180021,
      "grad_norm": 0.6210955381393433,
      "learning_rate": 4.858249417445571e-05,
      "loss": 1.3465,
      "step": 27300
    },
    {
      "epoch": 0.08524046537865779,
      "grad_norm": 0.6250020265579224,
      "learning_rate": 4.857989696344478e-05,
      "loss": 1.3093,
      "step": 27350
    },
    {
      "epoch": 0.08539629803931346,
      "grad_norm": 0.5691606402397156,
      "learning_rate": 4.8577299752433845e-05,
      "loss": 1.319,
      "step": 27400
    },
    {
      "epoch": 0.08555213069996914,
      "grad_norm": 0.5642447471618652,
      "learning_rate": 4.8574702541422924e-05,
      "loss": 1.2774,
      "step": 27450
    },
    {
      "epoch": 0.08570796336062482,
      "grad_norm": 0.4889324903488159,
      "learning_rate": 4.857210533041199e-05,
      "loss": 1.3399,
      "step": 27500
    },
    {
      "epoch": 0.08586379602128051,
      "grad_norm": 0.5687410235404968,
      "learning_rate": 4.856950811940106e-05,
      "loss": 1.3352,
      "step": 27550
    },
    {
      "epoch": 0.08601962868193619,
      "grad_norm": 0.6841719746589661,
      "learning_rate": 4.8566910908390135e-05,
      "loss": 1.3415,
      "step": 27600
    },
    {
      "epoch": 0.08617546134259187,
      "grad_norm": 0.5788951516151428,
      "learning_rate": 4.856431369737921e-05,
      "loss": 1.3495,
      "step": 27650
    },
    {
      "epoch": 0.08633129400324756,
      "grad_norm": 0.6272716522216797,
      "learning_rate": 4.856171648636828e-05,
      "loss": 1.3136,
      "step": 27700
    },
    {
      "epoch": 0.08648712666390324,
      "grad_norm": 0.6443812847137451,
      "learning_rate": 4.855911927535735e-05,
      "loss": 1.2993,
      "step": 27750
    },
    {
      "epoch": 0.08664295932455891,
      "grad_norm": 0.5322339534759521,
      "learning_rate": 4.8556522064346425e-05,
      "loss": 1.4067,
      "step": 27800
    },
    {
      "epoch": 0.08679879198521459,
      "grad_norm": 0.5557043552398682,
      "learning_rate": 4.85539248533355e-05,
      "loss": 1.3156,
      "step": 27850
    },
    {
      "epoch": 0.08695462464587028,
      "grad_norm": 0.5964245796203613,
      "learning_rate": 4.855132764232457e-05,
      "loss": 1.3394,
      "step": 27900
    },
    {
      "epoch": 0.08711045730652596,
      "grad_norm": 0.47169971466064453,
      "learning_rate": 4.8548730431313636e-05,
      "loss": 1.2855,
      "step": 27950
    },
    {
      "epoch": 0.08726628996718164,
      "grad_norm": 0.5035881400108337,
      "learning_rate": 4.8546133220302715e-05,
      "loss": 1.2851,
      "step": 28000
    },
    {
      "epoch": 0.08742212262783733,
      "grad_norm": 0.5847326517105103,
      "learning_rate": 4.854353600929179e-05,
      "loss": 1.305,
      "step": 28050
    },
    {
      "epoch": 0.087577955288493,
      "grad_norm": 0.58565753698349,
      "learning_rate": 4.854093879828085e-05,
      "loss": 1.3296,
      "step": 28100
    },
    {
      "epoch": 0.08773378794914868,
      "grad_norm": 0.7690196633338928,
      "learning_rate": 4.853834158726993e-05,
      "loss": 1.3273,
      "step": 28150
    },
    {
      "epoch": 0.08788962060980436,
      "grad_norm": 0.6867075562477112,
      "learning_rate": 4.8535744376259e-05,
      "loss": 1.3355,
      "step": 28200
    },
    {
      "epoch": 0.08804545327046005,
      "grad_norm": 0.6657630801200867,
      "learning_rate": 4.853314716524807e-05,
      "loss": 1.3064,
      "step": 28250
    },
    {
      "epoch": 0.08820128593111573,
      "grad_norm": 0.639108419418335,
      "learning_rate": 4.853054995423714e-05,
      "loss": 1.2596,
      "step": 28300
    },
    {
      "epoch": 0.08835711859177141,
      "grad_norm": 0.7092931270599365,
      "learning_rate": 4.8527952743226216e-05,
      "loss": 1.3412,
      "step": 28350
    },
    {
      "epoch": 0.0885129512524271,
      "grad_norm": 0.7189709544181824,
      "learning_rate": 4.852535553221529e-05,
      "loss": 1.3589,
      "step": 28400
    },
    {
      "epoch": 0.08866878391308278,
      "grad_norm": 0.6375130414962769,
      "learning_rate": 4.852275832120436e-05,
      "loss": 1.2475,
      "step": 28450
    },
    {
      "epoch": 0.08882461657373845,
      "grad_norm": 0.5045791864395142,
      "learning_rate": 4.8520161110193427e-05,
      "loss": 1.3512,
      "step": 28500
    },
    {
      "epoch": 0.08898044923439413,
      "grad_norm": 0.5924463868141174,
      "learning_rate": 4.8517563899182506e-05,
      "loss": 1.3154,
      "step": 28550
    },
    {
      "epoch": 0.08913628189504982,
      "grad_norm": 0.5922365784645081,
      "learning_rate": 4.851496668817158e-05,
      "loss": 1.369,
      "step": 28600
    },
    {
      "epoch": 0.0892921145557055,
      "grad_norm": 0.5737619400024414,
      "learning_rate": 4.8512369477160644e-05,
      "loss": 1.3245,
      "step": 28650
    },
    {
      "epoch": 0.08944794721636118,
      "grad_norm": 0.6110926866531372,
      "learning_rate": 4.850977226614972e-05,
      "loss": 1.2792,
      "step": 28700
    },
    {
      "epoch": 0.08960377987701687,
      "grad_norm": 0.7110636830329895,
      "learning_rate": 4.8507175055138796e-05,
      "loss": 1.3757,
      "step": 28750
    },
    {
      "epoch": 0.08975961253767255,
      "grad_norm": 0.5630603432655334,
      "learning_rate": 4.850457784412786e-05,
      "loss": 1.253,
      "step": 28800
    },
    {
      "epoch": 0.08991544519832823,
      "grad_norm": 0.48793330788612366,
      "learning_rate": 4.8501980633116934e-05,
      "loss": 1.2487,
      "step": 28850
    },
    {
      "epoch": 0.0900712778589839,
      "grad_norm": 0.6111130118370056,
      "learning_rate": 4.8499383422106007e-05,
      "loss": 1.2961,
      "step": 28900
    },
    {
      "epoch": 0.0902271105196396,
      "grad_norm": 0.6781116127967834,
      "learning_rate": 4.849678621109508e-05,
      "loss": 1.2819,
      "step": 28950
    },
    {
      "epoch": 0.09038294318029527,
      "grad_norm": 0.5462363362312317,
      "learning_rate": 4.849418900008415e-05,
      "loss": 1.2619,
      "step": 29000
    },
    {
      "epoch": 0.09053877584095095,
      "grad_norm": 0.6337351202964783,
      "learning_rate": 4.8491591789073224e-05,
      "loss": 1.3435,
      "step": 29050
    },
    {
      "epoch": 0.09069460850160664,
      "grad_norm": 0.5700948238372803,
      "learning_rate": 4.8488994578062297e-05,
      "loss": 1.3488,
      "step": 29100
    },
    {
      "epoch": 0.09085044116226232,
      "grad_norm": 0.6760162711143494,
      "learning_rate": 4.848639736705137e-05,
      "loss": 1.3072,
      "step": 29150
    },
    {
      "epoch": 0.091006273822918,
      "grad_norm": 0.6979495882987976,
      "learning_rate": 4.8483800156040435e-05,
      "loss": 1.3451,
      "step": 29200
    },
    {
      "epoch": 0.09116210648357367,
      "grad_norm": 0.5887072682380676,
      "learning_rate": 4.8481202945029514e-05,
      "loss": 1.3197,
      "step": 29250
    },
    {
      "epoch": 0.09131793914422937,
      "grad_norm": 0.5707601308822632,
      "learning_rate": 4.8478605734018587e-05,
      "loss": 1.2965,
      "step": 29300
    },
    {
      "epoch": 0.09147377180488504,
      "grad_norm": 0.6921617388725281,
      "learning_rate": 4.847600852300765e-05,
      "loss": 1.3195,
      "step": 29350
    },
    {
      "epoch": 0.09162960446554072,
      "grad_norm": 0.5937468409538269,
      "learning_rate": 4.847341131199673e-05,
      "loss": 1.274,
      "step": 29400
    },
    {
      "epoch": 0.09178543712619641,
      "grad_norm": 0.7560850381851196,
      "learning_rate": 4.8470814100985804e-05,
      "loss": 1.285,
      "step": 29450
    },
    {
      "epoch": 0.09194126978685209,
      "grad_norm": 0.6644518375396729,
      "learning_rate": 4.846821688997487e-05,
      "loss": 1.3182,
      "step": 29500
    },
    {
      "epoch": 0.09209710244750777,
      "grad_norm": 0.5182138085365295,
      "learning_rate": 4.846561967896394e-05,
      "loss": 1.328,
      "step": 29550
    },
    {
      "epoch": 0.09225293510816344,
      "grad_norm": 0.580568790435791,
      "learning_rate": 4.8463022467953015e-05,
      "loss": 1.3349,
      "step": 29600
    },
    {
      "epoch": 0.09240876776881914,
      "grad_norm": 0.7215962409973145,
      "learning_rate": 4.846042525694209e-05,
      "loss": 1.3256,
      "step": 29650
    },
    {
      "epoch": 0.09256460042947481,
      "grad_norm": 0.5717561841011047,
      "learning_rate": 4.845782804593116e-05,
      "loss": 1.3353,
      "step": 29700
    },
    {
      "epoch": 0.09272043309013049,
      "grad_norm": 0.7208356857299805,
      "learning_rate": 4.8455230834920226e-05,
      "loss": 1.3124,
      "step": 29750
    },
    {
      "epoch": 0.09287626575078617,
      "grad_norm": 0.5762004256248474,
      "learning_rate": 4.8452633623909305e-05,
      "loss": 1.2875,
      "step": 29800
    },
    {
      "epoch": 0.09303209841144186,
      "grad_norm": 0.6728041768074036,
      "learning_rate": 4.845003641289838e-05,
      "loss": 1.3319,
      "step": 29850
    },
    {
      "epoch": 0.09318793107209754,
      "grad_norm": 0.5460833311080933,
      "learning_rate": 4.844743920188744e-05,
      "loss": 1.2729,
      "step": 29900
    },
    {
      "epoch": 0.09334376373275322,
      "grad_norm": 0.7213008403778076,
      "learning_rate": 4.844484199087652e-05,
      "loss": 1.3078,
      "step": 29950
    },
    {
      "epoch": 0.0934995963934089,
      "grad_norm": 0.6473243832588196,
      "learning_rate": 4.8442244779865595e-05,
      "loss": 1.314,
      "step": 30000
    },
    {
      "epoch": 0.09365542905406458,
      "grad_norm": 0.5441765189170837,
      "learning_rate": 4.843964756885466e-05,
      "loss": 1.3227,
      "step": 30050
    },
    {
      "epoch": 0.09381126171472026,
      "grad_norm": 0.5184540152549744,
      "learning_rate": 4.843705035784373e-05,
      "loss": 1.3302,
      "step": 30100
    },
    {
      "epoch": 0.09396709437537594,
      "grad_norm": 0.7708337903022766,
      "learning_rate": 4.8434453146832806e-05,
      "loss": 1.2945,
      "step": 30150
    },
    {
      "epoch": 0.09412292703603163,
      "grad_norm": 0.6713712215423584,
      "learning_rate": 4.843185593582188e-05,
      "loss": 1.2931,
      "step": 30200
    },
    {
      "epoch": 0.09427875969668731,
      "grad_norm": 0.6589187383651733,
      "learning_rate": 4.842925872481095e-05,
      "loss": 1.3589,
      "step": 30250
    },
    {
      "epoch": 0.09443459235734299,
      "grad_norm": 0.7123727798461914,
      "learning_rate": 4.842666151380002e-05,
      "loss": 1.3498,
      "step": 30300
    },
    {
      "epoch": 0.09459042501799868,
      "grad_norm": 0.6041285991668701,
      "learning_rate": 4.8424064302789096e-05,
      "loss": 1.355,
      "step": 30350
    },
    {
      "epoch": 0.09474625767865436,
      "grad_norm": 0.6531980633735657,
      "learning_rate": 4.842146709177817e-05,
      "loss": 1.3322,
      "step": 30400
    },
    {
      "epoch": 0.09490209033931003,
      "grad_norm": 0.6043731570243835,
      "learning_rate": 4.8418869880767234e-05,
      "loss": 1.3165,
      "step": 30450
    },
    {
      "epoch": 0.09505792299996571,
      "grad_norm": 0.609575092792511,
      "learning_rate": 4.841627266975631e-05,
      "loss": 1.2803,
      "step": 30500
    },
    {
      "epoch": 0.0952137556606214,
      "grad_norm": 0.5066163539886475,
      "learning_rate": 4.8413675458745386e-05,
      "loss": 1.2885,
      "step": 30550
    },
    {
      "epoch": 0.09536958832127708,
      "grad_norm": 0.545536994934082,
      "learning_rate": 4.841107824773445e-05,
      "loss": 1.3191,
      "step": 30600
    },
    {
      "epoch": 0.09552542098193276,
      "grad_norm": 0.6939616203308105,
      "learning_rate": 4.840848103672353e-05,
      "loss": 1.2908,
      "step": 30650
    },
    {
      "epoch": 0.09568125364258845,
      "grad_norm": 0.4374024569988251,
      "learning_rate": 4.84058838257126e-05,
      "loss": 1.306,
      "step": 30700
    },
    {
      "epoch": 0.09583708630324413,
      "grad_norm": 0.527031421661377,
      "learning_rate": 4.840328661470167e-05,
      "loss": 1.3422,
      "step": 30750
    },
    {
      "epoch": 0.0959929189638998,
      "grad_norm": 0.5746809840202332,
      "learning_rate": 4.840068940369074e-05,
      "loss": 1.3062,
      "step": 30800
    },
    {
      "epoch": 0.09614875162455548,
      "grad_norm": 0.5726195573806763,
      "learning_rate": 4.8398092192679814e-05,
      "loss": 1.3166,
      "step": 30850
    },
    {
      "epoch": 0.09630458428521117,
      "grad_norm": 0.5534207224845886,
      "learning_rate": 4.8395494981668886e-05,
      "loss": 1.3172,
      "step": 30900
    },
    {
      "epoch": 0.09646041694586685,
      "grad_norm": 0.5792598128318787,
      "learning_rate": 4.839289777065796e-05,
      "loss": 1.3117,
      "step": 30950
    },
    {
      "epoch": 0.09661624960652253,
      "grad_norm": 0.620458722114563,
      "learning_rate": 4.839030055964703e-05,
      "loss": 1.2681,
      "step": 31000
    },
    {
      "epoch": 0.09677208226717822,
      "grad_norm": 0.6190546751022339,
      "learning_rate": 4.8387703348636104e-05,
      "loss": 1.3065,
      "step": 31050
    },
    {
      "epoch": 0.0969279149278339,
      "grad_norm": 0.6716495752334595,
      "learning_rate": 4.8385106137625176e-05,
      "loss": 1.2888,
      "step": 31100
    },
    {
      "epoch": 0.09708374758848957,
      "grad_norm": 0.660619854927063,
      "learning_rate": 4.838250892661424e-05,
      "loss": 1.3128,
      "step": 31150
    },
    {
      "epoch": 0.09723958024914525,
      "grad_norm": 0.6048681735992432,
      "learning_rate": 4.837991171560332e-05,
      "loss": 1.2794,
      "step": 31200
    },
    {
      "epoch": 0.09739541290980094,
      "grad_norm": 0.7884244918823242,
      "learning_rate": 4.8377314504592394e-05,
      "loss": 1.3108,
      "step": 31250
    },
    {
      "epoch": 0.09755124557045662,
      "grad_norm": 0.568787157535553,
      "learning_rate": 4.837471729358146e-05,
      "loss": 1.2872,
      "step": 31300
    },
    {
      "epoch": 0.0977070782311123,
      "grad_norm": 0.5467450618743896,
      "learning_rate": 4.837212008257053e-05,
      "loss": 1.3512,
      "step": 31350
    },
    {
      "epoch": 0.09786291089176799,
      "grad_norm": 0.5566503405570984,
      "learning_rate": 4.836952287155961e-05,
      "loss": 1.3166,
      "step": 31400
    },
    {
      "epoch": 0.09801874355242367,
      "grad_norm": 0.6660345196723938,
      "learning_rate": 4.836692566054868e-05,
      "loss": 1.2875,
      "step": 31450
    },
    {
      "epoch": 0.09817457621307935,
      "grad_norm": 0.6394785642623901,
      "learning_rate": 4.836432844953775e-05,
      "loss": 1.3256,
      "step": 31500
    },
    {
      "epoch": 0.09833040887373502,
      "grad_norm": 0.519582986831665,
      "learning_rate": 4.836173123852682e-05,
      "loss": 1.3096,
      "step": 31550
    },
    {
      "epoch": 0.09848624153439071,
      "grad_norm": 0.48336130380630493,
      "learning_rate": 4.8359134027515895e-05,
      "loss": 1.288,
      "step": 31600
    },
    {
      "epoch": 0.09864207419504639,
      "grad_norm": 0.6389699578285217,
      "learning_rate": 4.835653681650497e-05,
      "loss": 1.3037,
      "step": 31650
    },
    {
      "epoch": 0.09879790685570207,
      "grad_norm": 0.6254711151123047,
      "learning_rate": 4.835393960549404e-05,
      "loss": 1.3147,
      "step": 31700
    },
    {
      "epoch": 0.09895373951635776,
      "grad_norm": 0.6716580390930176,
      "learning_rate": 4.835134239448311e-05,
      "loss": 1.3356,
      "step": 31750
    },
    {
      "epoch": 0.09910957217701344,
      "grad_norm": 0.522241473197937,
      "learning_rate": 4.8348745183472185e-05,
      "loss": 1.2809,
      "step": 31800
    },
    {
      "epoch": 0.09926540483766912,
      "grad_norm": 0.6442894339561462,
      "learning_rate": 4.834614797246125e-05,
      "loss": 1.2725,
      "step": 31850
    },
    {
      "epoch": 0.0994212374983248,
      "grad_norm": 0.5778121948242188,
      "learning_rate": 4.834355076145033e-05,
      "loss": 1.3021,
      "step": 31900
    },
    {
      "epoch": 0.09957707015898049,
      "grad_norm": 0.45215466618537903,
      "learning_rate": 4.83409535504394e-05,
      "loss": 1.3506,
      "step": 31950
    },
    {
      "epoch": 0.09973290281963616,
      "grad_norm": 0.5922526717185974,
      "learning_rate": 4.833835633942847e-05,
      "loss": 1.3121,
      "step": 32000
    },
    {
      "epoch": 0.09988873548029184,
      "grad_norm": 0.7810489535331726,
      "learning_rate": 4.833575912841754e-05,
      "loss": 1.3259,
      "step": 32050
    },
    {
      "epoch": 0.10004456814094753,
      "grad_norm": 0.6372601389884949,
      "learning_rate": 4.833316191740662e-05,
      "loss": 1.2568,
      "step": 32100
    },
    {
      "epoch": 0.10020040080160321,
      "grad_norm": 0.6752703785896301,
      "learning_rate": 4.8330564706395685e-05,
      "loss": 1.2793,
      "step": 32150
    },
    {
      "epoch": 0.10035623346225889,
      "grad_norm": 0.6448495984077454,
      "learning_rate": 4.832796749538476e-05,
      "loss": 1.2854,
      "step": 32200
    },
    {
      "epoch": 0.10051206612291456,
      "grad_norm": 0.6271458268165588,
      "learning_rate": 4.832537028437383e-05,
      "loss": 1.3319,
      "step": 32250
    },
    {
      "epoch": 0.10066789878357026,
      "grad_norm": 0.5990386009216309,
      "learning_rate": 4.83227730733629e-05,
      "loss": 1.2997,
      "step": 32300
    },
    {
      "epoch": 0.10082373144422593,
      "grad_norm": 0.7323005199432373,
      "learning_rate": 4.8320175862351975e-05,
      "loss": 1.2957,
      "step": 32350
    },
    {
      "epoch": 0.10097956410488161,
      "grad_norm": 0.5498114228248596,
      "learning_rate": 4.831757865134104e-05,
      "loss": 1.3127,
      "step": 32400
    },
    {
      "epoch": 0.10113539676553729,
      "grad_norm": 0.5603175163269043,
      "learning_rate": 4.831498144033012e-05,
      "loss": 1.3446,
      "step": 32450
    },
    {
      "epoch": 0.10129122942619298,
      "grad_norm": 0.7006092667579651,
      "learning_rate": 4.831238422931919e-05,
      "loss": 1.316,
      "step": 32500
    },
    {
      "epoch": 0.10144706208684866,
      "grad_norm": 0.6053645610809326,
      "learning_rate": 4.830978701830826e-05,
      "loss": 1.3087,
      "step": 32550
    },
    {
      "epoch": 0.10160289474750434,
      "grad_norm": 0.5856729745864868,
      "learning_rate": 4.830718980729733e-05,
      "loss": 1.2513,
      "step": 32600
    },
    {
      "epoch": 0.10175872740816003,
      "grad_norm": 0.6290066838264465,
      "learning_rate": 4.830459259628641e-05,
      "loss": 1.3177,
      "step": 32650
    },
    {
      "epoch": 0.1019145600688157,
      "grad_norm": 0.6063239574432373,
      "learning_rate": 4.8301995385275476e-05,
      "loss": 1.2919,
      "step": 32700
    },
    {
      "epoch": 0.10207039272947138,
      "grad_norm": 0.6281803250312805,
      "learning_rate": 4.829939817426455e-05,
      "loss": 1.2575,
      "step": 32750
    },
    {
      "epoch": 0.10222622539012706,
      "grad_norm": 0.6185901761054993,
      "learning_rate": 4.829680096325362e-05,
      "loss": 1.3389,
      "step": 32800
    },
    {
      "epoch": 0.10238205805078275,
      "grad_norm": 0.5265676379203796,
      "learning_rate": 4.8294203752242694e-05,
      "loss": 1.3009,
      "step": 32850
    },
    {
      "epoch": 0.10253789071143843,
      "grad_norm": 0.7399351596832275,
      "learning_rate": 4.8291658485451986e-05,
      "loss": 1.3322,
      "step": 32900
    },
    {
      "epoch": 0.1026937233720941,
      "grad_norm": 0.7405065894126892,
      "learning_rate": 4.828906127444106e-05,
      "loss": 1.2711,
      "step": 32950
    },
    {
      "epoch": 0.1028495560327498,
      "grad_norm": 1.237826943397522,
      "learning_rate": 4.8286464063430124e-05,
      "loss": 1.2937,
      "step": 33000
    },
    {
      "epoch": 0.10300538869340548,
      "grad_norm": 0.607592761516571,
      "learning_rate": 4.82838668524192e-05,
      "loss": 1.2742,
      "step": 33050
    },
    {
      "epoch": 0.10316122135406115,
      "grad_norm": 0.604324221611023,
      "learning_rate": 4.828126964140827e-05,
      "loss": 1.2735,
      "step": 33100
    },
    {
      "epoch": 0.10331705401471683,
      "grad_norm": 0.4758050739765167,
      "learning_rate": 4.827867243039734e-05,
      "loss": 1.3064,
      "step": 33150
    },
    {
      "epoch": 0.10347288667537252,
      "grad_norm": 0.6848807334899902,
      "learning_rate": 4.827607521938642e-05,
      "loss": 1.2605,
      "step": 33200
    },
    {
      "epoch": 0.1036287193360282,
      "grad_norm": 0.6704609990119934,
      "learning_rate": 4.8273478008375487e-05,
      "loss": 1.282,
      "step": 33250
    },
    {
      "epoch": 0.10378455199668388,
      "grad_norm": 0.7627711296081543,
      "learning_rate": 4.827088079736456e-05,
      "loss": 1.2346,
      "step": 33300
    },
    {
      "epoch": 0.10394038465733957,
      "grad_norm": 0.5919960141181946,
      "learning_rate": 4.826828358635363e-05,
      "loss": 1.2679,
      "step": 33350
    },
    {
      "epoch": 0.10409621731799525,
      "grad_norm": 0.6013283729553223,
      "learning_rate": 4.8265686375342704e-05,
      "loss": 1.2776,
      "step": 33400
    },
    {
      "epoch": 0.10425204997865092,
      "grad_norm": 0.4607877731323242,
      "learning_rate": 4.8263089164331777e-05,
      "loss": 1.347,
      "step": 33450
    },
    {
      "epoch": 0.1044078826393066,
      "grad_norm": 0.5587279200553894,
      "learning_rate": 4.826049195332085e-05,
      "loss": 1.303,
      "step": 33500
    },
    {
      "epoch": 0.10456371529996229,
      "grad_norm": 0.5672995448112488,
      "learning_rate": 4.825789474230992e-05,
      "loss": 1.3452,
      "step": 33550
    },
    {
      "epoch": 0.10471954796061797,
      "grad_norm": 0.7008445858955383,
      "learning_rate": 4.8255297531298994e-05,
      "loss": 1.3274,
      "step": 33600
    },
    {
      "epoch": 0.10487538062127365,
      "grad_norm": 1.1993765830993652,
      "learning_rate": 4.8252700320288067e-05,
      "loss": 1.3349,
      "step": 33650
    },
    {
      "epoch": 0.10503121328192934,
      "grad_norm": 0.5641416907310486,
      "learning_rate": 4.825010310927713e-05,
      "loss": 1.3087,
      "step": 33700
    },
    {
      "epoch": 0.10518704594258502,
      "grad_norm": 0.6554998755455017,
      "learning_rate": 4.824750589826621e-05,
      "loss": 1.3109,
      "step": 33750
    },
    {
      "epoch": 0.1053428786032407,
      "grad_norm": 0.6819440722465515,
      "learning_rate": 4.824490868725528e-05,
      "loss": 1.3133,
      "step": 33800
    },
    {
      "epoch": 0.10549871126389637,
      "grad_norm": 0.5821626782417297,
      "learning_rate": 4.824231147624435e-05,
      "loss": 1.3177,
      "step": 33850
    },
    {
      "epoch": 0.10565454392455206,
      "grad_norm": 0.6140492558479309,
      "learning_rate": 4.823971426523343e-05,
      "loss": 1.2768,
      "step": 33900
    },
    {
      "epoch": 0.10581037658520774,
      "grad_norm": 0.6448777914047241,
      "learning_rate": 4.8237117054222495e-05,
      "loss": 1.3052,
      "step": 33950
    },
    {
      "epoch": 0.10596620924586342,
      "grad_norm": 0.5951988697052002,
      "learning_rate": 4.823451984321157e-05,
      "loss": 1.2747,
      "step": 34000
    },
    {
      "epoch": 0.10612204190651911,
      "grad_norm": 0.600569486618042,
      "learning_rate": 4.823192263220064e-05,
      "loss": 1.2833,
      "step": 34050
    },
    {
      "epoch": 0.10627787456717479,
      "grad_norm": 0.6354680061340332,
      "learning_rate": 4.822932542118971e-05,
      "loss": 1.3122,
      "step": 34100
    },
    {
      "epoch": 0.10643370722783047,
      "grad_norm": 0.7064590454101562,
      "learning_rate": 4.8226728210178785e-05,
      "loss": 1.3248,
      "step": 34150
    },
    {
      "epoch": 0.10658953988848614,
      "grad_norm": 0.609530508518219,
      "learning_rate": 4.822413099916786e-05,
      "loss": 1.2965,
      "step": 34200
    },
    {
      "epoch": 0.10674537254914183,
      "grad_norm": 0.6773823499679565,
      "learning_rate": 4.822153378815692e-05,
      "loss": 1.2743,
      "step": 34250
    },
    {
      "epoch": 0.10690120520979751,
      "grad_norm": 0.5117906332015991,
      "learning_rate": 4.8218936577146e-05,
      "loss": 1.3022,
      "step": 34300
    },
    {
      "epoch": 0.10705703787045319,
      "grad_norm": 0.5690749287605286,
      "learning_rate": 4.8216339366135075e-05,
      "loss": 1.2871,
      "step": 34350
    },
    {
      "epoch": 0.10721287053110888,
      "grad_norm": 0.6989483833312988,
      "learning_rate": 4.821374215512414e-05,
      "loss": 1.3626,
      "step": 34400
    },
    {
      "epoch": 0.10736870319176456,
      "grad_norm": 0.7673261761665344,
      "learning_rate": 4.821114494411322e-05,
      "loss": 1.3209,
      "step": 34450
    },
    {
      "epoch": 0.10752453585242024,
      "grad_norm": 0.5640439391136169,
      "learning_rate": 4.8208547733102286e-05,
      "loss": 1.3024,
      "step": 34500
    },
    {
      "epoch": 0.10768036851307591,
      "grad_norm": 0.5638662576675415,
      "learning_rate": 4.820595052209136e-05,
      "loss": 1.2448,
      "step": 34550
    },
    {
      "epoch": 0.1078362011737316,
      "grad_norm": 0.7025564312934875,
      "learning_rate": 4.820335331108043e-05,
      "loss": 1.3207,
      "step": 34600
    },
    {
      "epoch": 0.10799203383438728,
      "grad_norm": 0.5030674934387207,
      "learning_rate": 4.82007561000695e-05,
      "loss": 1.3153,
      "step": 34650
    },
    {
      "epoch": 0.10814786649504296,
      "grad_norm": 0.7745028138160706,
      "learning_rate": 4.8198158889058576e-05,
      "loss": 1.2773,
      "step": 34700
    },
    {
      "epoch": 0.10830369915569864,
      "grad_norm": 0.68645840883255,
      "learning_rate": 4.819556167804765e-05,
      "loss": 1.3051,
      "step": 34750
    },
    {
      "epoch": 0.10845953181635433,
      "grad_norm": 0.596472442150116,
      "learning_rate": 4.819296446703672e-05,
      "loss": 1.3545,
      "step": 34800
    },
    {
      "epoch": 0.10861536447701,
      "grad_norm": 0.6518841981887817,
      "learning_rate": 4.819036725602579e-05,
      "loss": 1.3201,
      "step": 34850
    },
    {
      "epoch": 0.10877119713766568,
      "grad_norm": 0.5931610465049744,
      "learning_rate": 4.8187770045014866e-05,
      "loss": 1.3203,
      "step": 34900
    },
    {
      "epoch": 0.10892702979832138,
      "grad_norm": 0.6665224432945251,
      "learning_rate": 4.818517283400393e-05,
      "loss": 1.2707,
      "step": 34950
    },
    {
      "epoch": 0.10908286245897705,
      "grad_norm": 0.5224091410636902,
      "learning_rate": 4.818257562299301e-05,
      "loss": 1.3082,
      "step": 35000
    },
    {
      "epoch": 0.10923869511963273,
      "grad_norm": 0.5619295835494995,
      "learning_rate": 4.8179978411982076e-05,
      "loss": 1.3352,
      "step": 35050
    },
    {
      "epoch": 0.10939452778028841,
      "grad_norm": 0.5159406661987305,
      "learning_rate": 4.817738120097115e-05,
      "loss": 1.335,
      "step": 35100
    },
    {
      "epoch": 0.1095503604409441,
      "grad_norm": 0.6865335702896118,
      "learning_rate": 4.817478398996023e-05,
      "loss": 1.279,
      "step": 35150
    },
    {
      "epoch": 0.10970619310159978,
      "grad_norm": 0.8152860999107361,
      "learning_rate": 4.8172186778949294e-05,
      "loss": 1.3077,
      "step": 35200
    },
    {
      "epoch": 0.10986202576225546,
      "grad_norm": 0.6178269982337952,
      "learning_rate": 4.8169589567938366e-05,
      "loss": 1.2582,
      "step": 35250
    },
    {
      "epoch": 0.11001785842291115,
      "grad_norm": 0.6713851094245911,
      "learning_rate": 4.816699235692744e-05,
      "loss": 1.3354,
      "step": 35300
    },
    {
      "epoch": 0.11017369108356682,
      "grad_norm": 0.48045849800109863,
      "learning_rate": 4.816439514591651e-05,
      "loss": 1.3417,
      "step": 35350
    },
    {
      "epoch": 0.1103295237442225,
      "grad_norm": 0.5223333239555359,
      "learning_rate": 4.8161797934905584e-05,
      "loss": 1.3391,
      "step": 35400
    },
    {
      "epoch": 0.11048535640487818,
      "grad_norm": 0.6113173961639404,
      "learning_rate": 4.8159200723894656e-05,
      "loss": 1.3117,
      "step": 35450
    },
    {
      "epoch": 0.11064118906553387,
      "grad_norm": 0.6141672730445862,
      "learning_rate": 4.815660351288372e-05,
      "loss": 1.2732,
      "step": 35500
    },
    {
      "epoch": 0.11079702172618955,
      "grad_norm": 0.5782865881919861,
      "learning_rate": 4.81540063018728e-05,
      "loss": 1.314,
      "step": 35550
    },
    {
      "epoch": 0.11095285438684523,
      "grad_norm": 0.642562747001648,
      "learning_rate": 4.8151409090861874e-05,
      "loss": 1.3028,
      "step": 35600
    },
    {
      "epoch": 0.11110868704750092,
      "grad_norm": 0.6634269952774048,
      "learning_rate": 4.814881187985094e-05,
      "loss": 1.2821,
      "step": 35650
    },
    {
      "epoch": 0.1112645197081566,
      "grad_norm": 0.5801329612731934,
      "learning_rate": 4.814621466884002e-05,
      "loss": 1.3377,
      "step": 35700
    },
    {
      "epoch": 0.11142035236881227,
      "grad_norm": 0.6966496706008911,
      "learning_rate": 4.8143617457829085e-05,
      "loss": 1.3436,
      "step": 35750
    },
    {
      "epoch": 0.11157618502946795,
      "grad_norm": 0.5621349215507507,
      "learning_rate": 4.814102024681816e-05,
      "loss": 1.2695,
      "step": 35800
    },
    {
      "epoch": 0.11173201769012364,
      "grad_norm": 0.7216249108314514,
      "learning_rate": 4.813842303580723e-05,
      "loss": 1.2801,
      "step": 35850
    },
    {
      "epoch": 0.11188785035077932,
      "grad_norm": 0.4429109990596771,
      "learning_rate": 4.81358258247963e-05,
      "loss": 1.2925,
      "step": 35900
    },
    {
      "epoch": 0.112043683011435,
      "grad_norm": 0.7279146909713745,
      "learning_rate": 4.8133228613785375e-05,
      "loss": 1.2854,
      "step": 35950
    },
    {
      "epoch": 0.11219951567209069,
      "grad_norm": 0.6892352104187012,
      "learning_rate": 4.813063140277445e-05,
      "loss": 1.2998,
      "step": 36000
    },
    {
      "epoch": 0.11235534833274637,
      "grad_norm": 0.5421904921531677,
      "learning_rate": 4.812803419176352e-05,
      "loss": 1.336,
      "step": 36050
    },
    {
      "epoch": 0.11251118099340204,
      "grad_norm": 0.6102074384689331,
      "learning_rate": 4.812543698075259e-05,
      "loss": 1.3253,
      "step": 36100
    },
    {
      "epoch": 0.11266701365405772,
      "grad_norm": 0.6387940049171448,
      "learning_rate": 4.8122839769741665e-05,
      "loss": 1.2908,
      "step": 36150
    },
    {
      "epoch": 0.11282284631471341,
      "grad_norm": 0.5687815546989441,
      "learning_rate": 4.812024255873073e-05,
      "loss": 1.3258,
      "step": 36200
    },
    {
      "epoch": 0.11297867897536909,
      "grad_norm": 0.6267993450164795,
      "learning_rate": 4.811764534771981e-05,
      "loss": 1.3195,
      "step": 36250
    },
    {
      "epoch": 0.11313451163602477,
      "grad_norm": 0.542776882648468,
      "learning_rate": 4.811504813670888e-05,
      "loss": 1.2904,
      "step": 36300
    },
    {
      "epoch": 0.11329034429668046,
      "grad_norm": 0.7760037183761597,
      "learning_rate": 4.811245092569795e-05,
      "loss": 1.2871,
      "step": 36350
    },
    {
      "epoch": 0.11344617695733614,
      "grad_norm": 0.6195964813232422,
      "learning_rate": 4.810985371468703e-05,
      "loss": 1.3082,
      "step": 36400
    },
    {
      "epoch": 0.11360200961799181,
      "grad_norm": 0.7235004901885986,
      "learning_rate": 4.810725650367609e-05,
      "loss": 1.3091,
      "step": 36450
    },
    {
      "epoch": 0.11375784227864749,
      "grad_norm": 0.618630588054657,
      "learning_rate": 4.8104659292665165e-05,
      "loss": 1.3053,
      "step": 36500
    },
    {
      "epoch": 0.11391367493930318,
      "grad_norm": 0.5345221757888794,
      "learning_rate": 4.810206208165424e-05,
      "loss": 1.2734,
      "step": 36550
    },
    {
      "epoch": 0.11406950759995886,
      "grad_norm": 0.5118734836578369,
      "learning_rate": 4.809946487064331e-05,
      "loss": 1.2871,
      "step": 36600
    },
    {
      "epoch": 0.11422534026061454,
      "grad_norm": 0.7908303737640381,
      "learning_rate": 4.809686765963238e-05,
      "loss": 1.2945,
      "step": 36650
    },
    {
      "epoch": 0.11438117292127023,
      "grad_norm": 0.5498906970024109,
      "learning_rate": 4.8094270448621455e-05,
      "loss": 1.234,
      "step": 36700
    },
    {
      "epoch": 0.11453700558192591,
      "grad_norm": 0.6774536371231079,
      "learning_rate": 4.809167323761052e-05,
      "loss": 1.2747,
      "step": 36750
    },
    {
      "epoch": 0.11469283824258159,
      "grad_norm": 0.6534583568572998,
      "learning_rate": 4.80890760265996e-05,
      "loss": 1.3055,
      "step": 36800
    },
    {
      "epoch": 0.11484867090323726,
      "grad_norm": 0.7130045294761658,
      "learning_rate": 4.808647881558867e-05,
      "loss": 1.3014,
      "step": 36850
    },
    {
      "epoch": 0.11500450356389295,
      "grad_norm": 0.6176275014877319,
      "learning_rate": 4.808388160457774e-05,
      "loss": 1.3135,
      "step": 36900
    },
    {
      "epoch": 0.11516033622454863,
      "grad_norm": 0.5602632761001587,
      "learning_rate": 4.808128439356682e-05,
      "loss": 1.3118,
      "step": 36950
    },
    {
      "epoch": 0.11531616888520431,
      "grad_norm": 0.5282571315765381,
      "learning_rate": 4.807868718255589e-05,
      "loss": 1.3323,
      "step": 37000
    },
    {
      "epoch": 0.11547200154585999,
      "grad_norm": 0.5654685497283936,
      "learning_rate": 4.8076089971544956e-05,
      "loss": 1.303,
      "step": 37050
    },
    {
      "epoch": 0.11562783420651568,
      "grad_norm": 0.5661150217056274,
      "learning_rate": 4.807349276053403e-05,
      "loss": 1.3374,
      "step": 37100
    },
    {
      "epoch": 0.11578366686717136,
      "grad_norm": 0.6020643711090088,
      "learning_rate": 4.80708955495231e-05,
      "loss": 1.2701,
      "step": 37150
    },
    {
      "epoch": 0.11593949952782703,
      "grad_norm": 0.6891269683837891,
      "learning_rate": 4.8068298338512174e-05,
      "loss": 1.3087,
      "step": 37200
    },
    {
      "epoch": 0.11609533218848272,
      "grad_norm": 0.666938841342926,
      "learning_rate": 4.8065701127501246e-05,
      "loss": 1.294,
      "step": 37250
    },
    {
      "epoch": 0.1162511648491384,
      "grad_norm": 0.7183475494384766,
      "learning_rate": 4.806310391649032e-05,
      "loss": 1.2727,
      "step": 37300
    },
    {
      "epoch": 0.11640699750979408,
      "grad_norm": 0.62204909324646,
      "learning_rate": 4.806050670547939e-05,
      "loss": 1.3147,
      "step": 37350
    },
    {
      "epoch": 0.11656283017044976,
      "grad_norm": 0.5770717263221741,
      "learning_rate": 4.8057909494468464e-05,
      "loss": 1.3205,
      "step": 37400
    },
    {
      "epoch": 0.11671866283110545,
      "grad_norm": 0.7261562347412109,
      "learning_rate": 4.805531228345753e-05,
      "loss": 1.3426,
      "step": 37450
    },
    {
      "epoch": 0.11687449549176113,
      "grad_norm": 0.6893677711486816,
      "learning_rate": 4.805276701666683e-05,
      "loss": 1.2651,
      "step": 37500
    },
    {
      "epoch": 0.1170303281524168,
      "grad_norm": 0.6405282020568848,
      "learning_rate": 4.80501698056559e-05,
      "loss": 1.2545,
      "step": 37550
    },
    {
      "epoch": 0.1171861608130725,
      "grad_norm": 0.5758815407752991,
      "learning_rate": 4.8047572594644966e-05,
      "loss": 1.3105,
      "step": 37600
    },
    {
      "epoch": 0.11734199347372817,
      "grad_norm": 0.5582353472709656,
      "learning_rate": 4.804497538363404e-05,
      "loss": 1.2543,
      "step": 37650
    },
    {
      "epoch": 0.11749782613438385,
      "grad_norm": 0.45709720253944397,
      "learning_rate": 4.804237817262311e-05,
      "loss": 1.2736,
      "step": 37700
    },
    {
      "epoch": 0.11765365879503953,
      "grad_norm": 0.5566271543502808,
      "learning_rate": 4.8039780961612184e-05,
      "loss": 1.2852,
      "step": 37750
    },
    {
      "epoch": 0.11780949145569522,
      "grad_norm": 0.6098825335502625,
      "learning_rate": 4.8037183750601257e-05,
      "loss": 1.2848,
      "step": 37800
    },
    {
      "epoch": 0.1179653241163509,
      "grad_norm": 0.6571801900863647,
      "learning_rate": 4.803458653959033e-05,
      "loss": 1.2786,
      "step": 37850
    },
    {
      "epoch": 0.11812115677700658,
      "grad_norm": 0.5777910947799683,
      "learning_rate": 4.80319893285794e-05,
      "loss": 1.2962,
      "step": 37900
    },
    {
      "epoch": 0.11827698943766227,
      "grad_norm": 0.9396817684173584,
      "learning_rate": 4.8029392117568474e-05,
      "loss": 1.3003,
      "step": 37950
    },
    {
      "epoch": 0.11843282209831794,
      "grad_norm": 0.6254543662071228,
      "learning_rate": 4.802679490655754e-05,
      "loss": 1.3428,
      "step": 38000
    },
    {
      "epoch": 0.11858865475897362,
      "grad_norm": 0.6974159479141235,
      "learning_rate": 4.802419769554662e-05,
      "loss": 1.2912,
      "step": 38050
    },
    {
      "epoch": 0.1187444874196293,
      "grad_norm": 0.6924845576286316,
      "learning_rate": 4.802160048453569e-05,
      "loss": 1.3808,
      "step": 38100
    },
    {
      "epoch": 0.11890032008028499,
      "grad_norm": 0.47615769505500793,
      "learning_rate": 4.801900327352476e-05,
      "loss": 1.2885,
      "step": 38150
    },
    {
      "epoch": 0.11905615274094067,
      "grad_norm": 0.6110733151435852,
      "learning_rate": 4.801640606251383e-05,
      "loss": 1.3132,
      "step": 38200
    },
    {
      "epoch": 0.11921198540159635,
      "grad_norm": 0.5641067028045654,
      "learning_rate": 4.801380885150291e-05,
      "loss": 1.2955,
      "step": 38250
    },
    {
      "epoch": 0.11936781806225204,
      "grad_norm": 0.6146008968353271,
      "learning_rate": 4.8011211640491975e-05,
      "loss": 1.3437,
      "step": 38300
    },
    {
      "epoch": 0.11952365072290771,
      "grad_norm": 0.6478571891784668,
      "learning_rate": 4.800861442948105e-05,
      "loss": 1.2836,
      "step": 38350
    },
    {
      "epoch": 0.11967948338356339,
      "grad_norm": 0.7837452292442322,
      "learning_rate": 4.800601721847012e-05,
      "loss": 1.3256,
      "step": 38400
    },
    {
      "epoch": 0.11983531604421907,
      "grad_norm": 0.5896023511886597,
      "learning_rate": 4.800342000745919e-05,
      "loss": 1.253,
      "step": 38450
    },
    {
      "epoch": 0.11999114870487476,
      "grad_norm": 0.5360631942749023,
      "learning_rate": 4.8000822796448265e-05,
      "loss": 1.3111,
      "step": 38500
    },
    {
      "epoch": 0.12014698136553044,
      "grad_norm": 0.583428144454956,
      "learning_rate": 4.799822558543734e-05,
      "loss": 1.3257,
      "step": 38550
    },
    {
      "epoch": 0.12030281402618612,
      "grad_norm": 0.48544856905937195,
      "learning_rate": 4.799562837442641e-05,
      "loss": 1.3161,
      "step": 38600
    },
    {
      "epoch": 0.12045864668684181,
      "grad_norm": 0.6622384190559387,
      "learning_rate": 4.799303116341548e-05,
      "loss": 1.3258,
      "step": 38650
    },
    {
      "epoch": 0.12061447934749749,
      "grad_norm": 0.6809834241867065,
      "learning_rate": 4.799043395240455e-05,
      "loss": 1.306,
      "step": 38700
    },
    {
      "epoch": 0.12077031200815316,
      "grad_norm": 0.6607571840286255,
      "learning_rate": 4.798783674139362e-05,
      "loss": 1.3095,
      "step": 38750
    },
    {
      "epoch": 0.12092614466880884,
      "grad_norm": 0.6357582211494446,
      "learning_rate": 4.79852395303827e-05,
      "loss": 1.3349,
      "step": 38800
    },
    {
      "epoch": 0.12108197732946453,
      "grad_norm": 0.6992290019989014,
      "learning_rate": 4.7982642319371766e-05,
      "loss": 1.3099,
      "step": 38850
    },
    {
      "epoch": 0.12123780999012021,
      "grad_norm": 0.7286826968193054,
      "learning_rate": 4.798004510836084e-05,
      "loss": 1.3105,
      "step": 38900
    },
    {
      "epoch": 0.12139364265077589,
      "grad_norm": 0.5787908434867859,
      "learning_rate": 4.797744789734992e-05,
      "loss": 1.3318,
      "step": 38950
    },
    {
      "epoch": 0.12154947531143158,
      "grad_norm": 0.5071535110473633,
      "learning_rate": 4.797485068633898e-05,
      "loss": 1.3426,
      "step": 39000
    },
    {
      "epoch": 0.12170530797208726,
      "grad_norm": 0.679426372051239,
      "learning_rate": 4.7972253475328056e-05,
      "loss": 1.3095,
      "step": 39050
    },
    {
      "epoch": 0.12186114063274293,
      "grad_norm": 0.5976461172103882,
      "learning_rate": 4.796965626431713e-05,
      "loss": 1.3135,
      "step": 39100
    },
    {
      "epoch": 0.12201697329339861,
      "grad_norm": 0.6141932606697083,
      "learning_rate": 4.79670590533062e-05,
      "loss": 1.3331,
      "step": 39150
    },
    {
      "epoch": 0.1221728059540543,
      "grad_norm": 0.6228541135787964,
      "learning_rate": 4.796451378651549e-05,
      "loss": 1.2762,
      "step": 39200
    },
    {
      "epoch": 0.12232863861470998,
      "grad_norm": 0.6197887659072876,
      "learning_rate": 4.7961916575504565e-05,
      "loss": 1.3103,
      "step": 39250
    },
    {
      "epoch": 0.12248447127536566,
      "grad_norm": 0.6343642473220825,
      "learning_rate": 4.795931936449363e-05,
      "loss": 1.3408,
      "step": 39300
    },
    {
      "epoch": 0.12264030393602134,
      "grad_norm": 0.6833229660987854,
      "learning_rate": 4.795672215348271e-05,
      "loss": 1.3265,
      "step": 39350
    },
    {
      "epoch": 0.12279613659667703,
      "grad_norm": 0.4940321743488312,
      "learning_rate": 4.7954124942471776e-05,
      "loss": 1.298,
      "step": 39400
    },
    {
      "epoch": 0.1229519692573327,
      "grad_norm": 0.6605017185211182,
      "learning_rate": 4.795152773146085e-05,
      "loss": 1.3137,
      "step": 39450
    },
    {
      "epoch": 0.12310780191798838,
      "grad_norm": 0.5250045657157898,
      "learning_rate": 4.794893052044993e-05,
      "loss": 1.3596,
      "step": 39500
    },
    {
      "epoch": 0.12326363457864407,
      "grad_norm": 0.6672757863998413,
      "learning_rate": 4.7946333309438993e-05,
      "loss": 1.3054,
      "step": 39550
    },
    {
      "epoch": 0.12341946723929975,
      "grad_norm": 0.6032199859619141,
      "learning_rate": 4.7943736098428066e-05,
      "loss": 1.3034,
      "step": 39600
    },
    {
      "epoch": 0.12357529989995543,
      "grad_norm": 0.49301764369010925,
      "learning_rate": 4.794113888741714e-05,
      "loss": 1.3207,
      "step": 39650
    },
    {
      "epoch": 0.1237311325606111,
      "grad_norm": 0.6685217022895813,
      "learning_rate": 4.793854167640621e-05,
      "loss": 1.3246,
      "step": 39700
    },
    {
      "epoch": 0.1238869652212668,
      "grad_norm": 0.5751984119415283,
      "learning_rate": 4.7935944465395283e-05,
      "loss": 1.3289,
      "step": 39750
    },
    {
      "epoch": 0.12404279788192248,
      "grad_norm": 0.5758429169654846,
      "learning_rate": 4.7933347254384356e-05,
      "loss": 1.3439,
      "step": 39800
    },
    {
      "epoch": 0.12419863054257815,
      "grad_norm": 0.7467831373214722,
      "learning_rate": 4.793075004337342e-05,
      "loss": 1.2799,
      "step": 39850
    },
    {
      "epoch": 0.12435446320323384,
      "grad_norm": 0.6555393934249878,
      "learning_rate": 4.79281528323625e-05,
      "loss": 1.3146,
      "step": 39900
    },
    {
      "epoch": 0.12451029586388952,
      "grad_norm": 0.6441541910171509,
      "learning_rate": 4.792555562135157e-05,
      "loss": 1.3121,
      "step": 39950
    },
    {
      "epoch": 0.1246661285245452,
      "grad_norm": 0.6305166482925415,
      "learning_rate": 4.792295841034064e-05,
      "loss": 1.3029,
      "step": 40000
    },
    {
      "epoch": 0.12482196118520088,
      "grad_norm": 0.8011161088943481,
      "learning_rate": 4.792036119932972e-05,
      "loss": 1.3883,
      "step": 40050
    },
    {
      "epoch": 0.12497779384585657,
      "grad_norm": 0.5882392525672913,
      "learning_rate": 4.7917763988318784e-05,
      "loss": 1.2985,
      "step": 40100
    },
    {
      "epoch": 0.12513362650651225,
      "grad_norm": 0.5609350204467773,
      "learning_rate": 4.791516677730786e-05,
      "loss": 1.2476,
      "step": 40150
    },
    {
      "epoch": 0.12528945916716794,
      "grad_norm": 0.622124195098877,
      "learning_rate": 4.791256956629693e-05,
      "loss": 1.3238,
      "step": 40200
    },
    {
      "epoch": 0.1254452918278236,
      "grad_norm": 0.6311288475990295,
      "learning_rate": 4.7909972355286e-05,
      "loss": 1.372,
      "step": 40250
    },
    {
      "epoch": 0.1256011244884793,
      "grad_norm": 0.594477653503418,
      "learning_rate": 4.7907375144275074e-05,
      "loss": 1.2927,
      "step": 40300
    },
    {
      "epoch": 0.12575695714913498,
      "grad_norm": 0.6637527942657471,
      "learning_rate": 4.790477793326415e-05,
      "loss": 1.2839,
      "step": 40350
    },
    {
      "epoch": 0.12591278980979065,
      "grad_norm": 0.5592416524887085,
      "learning_rate": 4.790218072225322e-05,
      "loss": 1.2559,
      "step": 40400
    },
    {
      "epoch": 0.12606862247044634,
      "grad_norm": 0.5094160437583923,
      "learning_rate": 4.789958351124229e-05,
      "loss": 1.2733,
      "step": 40450
    },
    {
      "epoch": 0.126224455131102,
      "grad_norm": 0.5733274221420288,
      "learning_rate": 4.7896986300231364e-05,
      "loss": 1.2305,
      "step": 40500
    },
    {
      "epoch": 0.1263802877917577,
      "grad_norm": 0.8550841808319092,
      "learning_rate": 4.789438908922043e-05,
      "loss": 1.3029,
      "step": 40550
    },
    {
      "epoch": 0.1265361204524134,
      "grad_norm": 0.7201706767082214,
      "learning_rate": 4.789179187820951e-05,
      "loss": 1.2904,
      "step": 40600
    },
    {
      "epoch": 0.12669195311306905,
      "grad_norm": 0.7317982316017151,
      "learning_rate": 4.7889194667198575e-05,
      "loss": 1.2636,
      "step": 40650
    },
    {
      "epoch": 0.12684778577372474,
      "grad_norm": 0.695854127407074,
      "learning_rate": 4.788659745618765e-05,
      "loss": 1.2735,
      "step": 40700
    },
    {
      "epoch": 0.12700361843438043,
      "grad_norm": 0.637054979801178,
      "learning_rate": 4.788400024517673e-05,
      "loss": 1.3367,
      "step": 40750
    },
    {
      "epoch": 0.1271594510950361,
      "grad_norm": 0.5099891424179077,
      "learning_rate": 4.788140303416579e-05,
      "loss": 1.3128,
      "step": 40800
    },
    {
      "epoch": 0.1273152837556918,
      "grad_norm": 0.8625943064689636,
      "learning_rate": 4.7878805823154865e-05,
      "loss": 1.305,
      "step": 40850
    },
    {
      "epoch": 0.12747111641634748,
      "grad_norm": 0.5268903970718384,
      "learning_rate": 4.787620861214394e-05,
      "loss": 1.2943,
      "step": 40900
    },
    {
      "epoch": 0.12762694907700314,
      "grad_norm": 0.5531526803970337,
      "learning_rate": 4.787361140113301e-05,
      "loss": 1.275,
      "step": 40950
    },
    {
      "epoch": 0.12778278173765883,
      "grad_norm": 0.5226430892944336,
      "learning_rate": 4.787101419012208e-05,
      "loss": 1.2725,
      "step": 41000
    },
    {
      "epoch": 0.12793861439831453,
      "grad_norm": 0.5908145308494568,
      "learning_rate": 4.7868416979111155e-05,
      "loss": 1.3492,
      "step": 41050
    },
    {
      "epoch": 0.1280944470589702,
      "grad_norm": 0.4799811840057373,
      "learning_rate": 4.786581976810022e-05,
      "loss": 1.2759,
      "step": 41100
    },
    {
      "epoch": 0.12825027971962588,
      "grad_norm": 0.49830639362335205,
      "learning_rate": 4.78632225570893e-05,
      "loss": 1.2931,
      "step": 41150
    },
    {
      "epoch": 0.12840611238028155,
      "grad_norm": 0.642104983329773,
      "learning_rate": 4.786062534607837e-05,
      "loss": 1.285,
      "step": 41200
    },
    {
      "epoch": 0.12856194504093724,
      "grad_norm": 0.7594215273857117,
      "learning_rate": 4.785802813506744e-05,
      "loss": 1.2428,
      "step": 41250
    },
    {
      "epoch": 0.12871777770159293,
      "grad_norm": 0.661133348941803,
      "learning_rate": 4.785543092405652e-05,
      "loss": 1.3163,
      "step": 41300
    },
    {
      "epoch": 0.1288736103622486,
      "grad_norm": 0.591550350189209,
      "learning_rate": 4.785283371304558e-05,
      "loss": 1.2864,
      "step": 41350
    },
    {
      "epoch": 0.12902944302290428,
      "grad_norm": 0.494907945394516,
      "learning_rate": 4.7850236502034656e-05,
      "loss": 1.2782,
      "step": 41400
    },
    {
      "epoch": 0.12918527568355997,
      "grad_norm": 0.6424683332443237,
      "learning_rate": 4.784763929102373e-05,
      "loss": 1.3288,
      "step": 41450
    },
    {
      "epoch": 0.12934110834421564,
      "grad_norm": 0.6834307909011841,
      "learning_rate": 4.78450420800128e-05,
      "loss": 1.2702,
      "step": 41500
    },
    {
      "epoch": 0.12949694100487133,
      "grad_norm": 0.6448318362236023,
      "learning_rate": 4.784244486900187e-05,
      "loss": 1.3224,
      "step": 41550
    },
    {
      "epoch": 0.12965277366552702,
      "grad_norm": 0.7109874486923218,
      "learning_rate": 4.7839847657990946e-05,
      "loss": 1.3016,
      "step": 41600
    },
    {
      "epoch": 0.12980860632618269,
      "grad_norm": 0.5444416403770447,
      "learning_rate": 4.783725044698002e-05,
      "loss": 1.2113,
      "step": 41650
    },
    {
      "epoch": 0.12996443898683838,
      "grad_norm": 0.5855161547660828,
      "learning_rate": 4.783465323596909e-05,
      "loss": 1.3364,
      "step": 41700
    },
    {
      "epoch": 0.13012027164749407,
      "grad_norm": 0.6828435659408569,
      "learning_rate": 4.783205602495816e-05,
      "loss": 1.3315,
      "step": 41750
    },
    {
      "epoch": 0.13027610430814973,
      "grad_norm": 0.6214451789855957,
      "learning_rate": 4.782945881394723e-05,
      "loss": 1.2882,
      "step": 41800
    },
    {
      "epoch": 0.13043193696880542,
      "grad_norm": 0.6646784543991089,
      "learning_rate": 4.782686160293631e-05,
      "loss": 1.2959,
      "step": 41850
    },
    {
      "epoch": 0.1305877696294611,
      "grad_norm": 0.6028434634208679,
      "learning_rate": 4.782426439192538e-05,
      "loss": 1.2547,
      "step": 41900
    },
    {
      "epoch": 0.13074360229011678,
      "grad_norm": 0.7210351228713989,
      "learning_rate": 4.7821667180914446e-05,
      "loss": 1.2832,
      "step": 41950
    },
    {
      "epoch": 0.13089943495077247,
      "grad_norm": 0.6228259205818176,
      "learning_rate": 4.7819069969903526e-05,
      "loss": 1.312,
      "step": 42000
    },
    {
      "epoch": 0.13105526761142813,
      "grad_norm": 0.5069441199302673,
      "learning_rate": 4.781647275889259e-05,
      "loss": 1.3074,
      "step": 42050
    },
    {
      "epoch": 0.13121110027208382,
      "grad_norm": 0.45655709505081177,
      "learning_rate": 4.7813875547881664e-05,
      "loss": 1.3005,
      "step": 42100
    },
    {
      "epoch": 0.13136693293273952,
      "grad_norm": 0.8035102486610413,
      "learning_rate": 4.7811278336870736e-05,
      "loss": 1.3398,
      "step": 42150
    },
    {
      "epoch": 0.13152276559339518,
      "grad_norm": 0.5357477068901062,
      "learning_rate": 4.780868112585981e-05,
      "loss": 1.3246,
      "step": 42200
    },
    {
      "epoch": 0.13167859825405087,
      "grad_norm": 0.4984484910964966,
      "learning_rate": 4.780608391484888e-05,
      "loss": 1.305,
      "step": 42250
    },
    {
      "epoch": 0.13183443091470656,
      "grad_norm": 0.5974494814872742,
      "learning_rate": 4.7803486703837954e-05,
      "loss": 1.2888,
      "step": 42300
    },
    {
      "epoch": 0.13199026357536223,
      "grad_norm": 0.6469826102256775,
      "learning_rate": 4.780088949282702e-05,
      "loss": 1.2738,
      "step": 42350
    },
    {
      "epoch": 0.13214609623601792,
      "grad_norm": 0.6579638123512268,
      "learning_rate": 4.77982922818161e-05,
      "loss": 1.2759,
      "step": 42400
    },
    {
      "epoch": 0.1323019288966736,
      "grad_norm": 0.7683864235877991,
      "learning_rate": 4.779569507080517e-05,
      "loss": 1.2895,
      "step": 42450
    },
    {
      "epoch": 0.13245776155732927,
      "grad_norm": 0.6605919599533081,
      "learning_rate": 4.779309785979424e-05,
      "loss": 1.3028,
      "step": 42500
    },
    {
      "epoch": 0.13261359421798496,
      "grad_norm": 0.9098865389823914,
      "learning_rate": 4.7790500648783316e-05,
      "loss": 1.3026,
      "step": 42550
    },
    {
      "epoch": 0.13276942687864063,
      "grad_norm": 0.5549512505531311,
      "learning_rate": 4.778790343777238e-05,
      "loss": 1.2849,
      "step": 42600
    },
    {
      "epoch": 0.13292525953929632,
      "grad_norm": 0.6708370447158813,
      "learning_rate": 4.7785306226761455e-05,
      "loss": 1.3095,
      "step": 42650
    },
    {
      "epoch": 0.133081092199952,
      "grad_norm": 0.6053194999694824,
      "learning_rate": 4.778270901575053e-05,
      "loss": 1.25,
      "step": 42700
    },
    {
      "epoch": 0.13323692486060768,
      "grad_norm": 0.5775591731071472,
      "learning_rate": 4.77801118047396e-05,
      "loss": 1.2985,
      "step": 42750
    },
    {
      "epoch": 0.13339275752126337,
      "grad_norm": 1.0243196487426758,
      "learning_rate": 4.777751459372867e-05,
      "loss": 1.3086,
      "step": 42800
    },
    {
      "epoch": 0.13354859018191906,
      "grad_norm": 0.7350090742111206,
      "learning_rate": 4.7774917382717745e-05,
      "loss": 1.3137,
      "step": 42850
    },
    {
      "epoch": 0.13370442284257472,
      "grad_norm": 0.732199490070343,
      "learning_rate": 4.777232017170682e-05,
      "loss": 1.3172,
      "step": 42900
    },
    {
      "epoch": 0.1338602555032304,
      "grad_norm": 0.5910337567329407,
      "learning_rate": 4.776972296069589e-05,
      "loss": 1.251,
      "step": 42950
    },
    {
      "epoch": 0.1340160881638861,
      "grad_norm": 0.6084860563278198,
      "learning_rate": 4.776717769390518e-05,
      "loss": 1.2369,
      "step": 43000
    },
    {
      "epoch": 0.13417192082454177,
      "grad_norm": 0.6138707399368286,
      "learning_rate": 4.776458048289425e-05,
      "loss": 1.3068,
      "step": 43050
    },
    {
      "epoch": 0.13432775348519746,
      "grad_norm": 0.5429355502128601,
      "learning_rate": 4.776198327188332e-05,
      "loss": 1.282,
      "step": 43100
    },
    {
      "epoch": 0.13448358614585312,
      "grad_norm": 0.626960039138794,
      "learning_rate": 4.77593860608724e-05,
      "loss": 1.2671,
      "step": 43150
    },
    {
      "epoch": 0.13463941880650881,
      "grad_norm": 0.7030003666877747,
      "learning_rate": 4.7756788849861465e-05,
      "loss": 1.2787,
      "step": 43200
    },
    {
      "epoch": 0.1347952514671645,
      "grad_norm": 0.5922453999519348,
      "learning_rate": 4.775419163885054e-05,
      "loss": 1.2614,
      "step": 43250
    },
    {
      "epoch": 0.13495108412782017,
      "grad_norm": 0.7580147981643677,
      "learning_rate": 4.775159442783961e-05,
      "loss": 1.2837,
      "step": 43300
    },
    {
      "epoch": 0.13510691678847586,
      "grad_norm": 0.5236717462539673,
      "learning_rate": 4.774899721682868e-05,
      "loss": 1.2952,
      "step": 43350
    },
    {
      "epoch": 0.13526274944913155,
      "grad_norm": 0.6583983302116394,
      "learning_rate": 4.7746400005817755e-05,
      "loss": 1.3854,
      "step": 43400
    },
    {
      "epoch": 0.13541858210978722,
      "grad_norm": 0.4796546995639801,
      "learning_rate": 4.774380279480683e-05,
      "loss": 1.238,
      "step": 43450
    },
    {
      "epoch": 0.1355744147704429,
      "grad_norm": 0.5967694520950317,
      "learning_rate": 4.77412055837959e-05,
      "loss": 1.308,
      "step": 43500
    },
    {
      "epoch": 0.1357302474310986,
      "grad_norm": 0.6152626276016235,
      "learning_rate": 4.773860837278497e-05,
      "loss": 1.2835,
      "step": 43550
    },
    {
      "epoch": 0.13588608009175426,
      "grad_norm": 0.562749981880188,
      "learning_rate": 4.773601116177404e-05,
      "loss": 1.3484,
      "step": 43600
    },
    {
      "epoch": 0.13604191275240995,
      "grad_norm": 0.5785967707633972,
      "learning_rate": 4.773341395076312e-05,
      "loss": 1.2886,
      "step": 43650
    },
    {
      "epoch": 0.13619774541306565,
      "grad_norm": 0.7335153222084045,
      "learning_rate": 4.773081673975219e-05,
      "loss": 1.2703,
      "step": 43700
    },
    {
      "epoch": 0.1363535780737213,
      "grad_norm": 0.7303630709648132,
      "learning_rate": 4.7728219528741256e-05,
      "loss": 1.2925,
      "step": 43750
    },
    {
      "epoch": 0.136509410734377,
      "grad_norm": 0.4848690330982208,
      "learning_rate": 4.772562231773033e-05,
      "loss": 1.275,
      "step": 43800
    },
    {
      "epoch": 0.13666524339503267,
      "grad_norm": 0.6308820247650146,
      "learning_rate": 4.772302510671941e-05,
      "loss": 1.3131,
      "step": 43850
    },
    {
      "epoch": 0.13682107605568836,
      "grad_norm": 0.6194983124732971,
      "learning_rate": 4.772042789570847e-05,
      "loss": 1.2833,
      "step": 43900
    },
    {
      "epoch": 0.13697690871634405,
      "grad_norm": 0.5377551913261414,
      "learning_rate": 4.7717830684697546e-05,
      "loss": 1.3128,
      "step": 43950
    },
    {
      "epoch": 0.1371327413769997,
      "grad_norm": 0.7132671475410461,
      "learning_rate": 4.771523347368662e-05,
      "loss": 1.3163,
      "step": 44000
    },
    {
      "epoch": 0.1372885740376554,
      "grad_norm": 0.648773193359375,
      "learning_rate": 4.771263626267569e-05,
      "loss": 1.3471,
      "step": 44050
    },
    {
      "epoch": 0.1374444066983111,
      "grad_norm": 0.5465928912162781,
      "learning_rate": 4.771003905166476e-05,
      "loss": 1.322,
      "step": 44100
    },
    {
      "epoch": 0.13760023935896676,
      "grad_norm": 0.6114341616630554,
      "learning_rate": 4.7707441840653836e-05,
      "loss": 1.2783,
      "step": 44150
    },
    {
      "epoch": 0.13775607201962245,
      "grad_norm": 0.7200212478637695,
      "learning_rate": 4.770484462964291e-05,
      "loss": 1.3512,
      "step": 44200
    },
    {
      "epoch": 0.13791190468027814,
      "grad_norm": 0.6343104243278503,
      "learning_rate": 4.770224741863198e-05,
      "loss": 1.2732,
      "step": 44250
    },
    {
      "epoch": 0.1380677373409338,
      "grad_norm": 0.5095133185386658,
      "learning_rate": 4.769965020762105e-05,
      "loss": 1.3404,
      "step": 44300
    },
    {
      "epoch": 0.1382235700015895,
      "grad_norm": 0.6715269088745117,
      "learning_rate": 4.769705299661012e-05,
      "loss": 1.2858,
      "step": 44350
    },
    {
      "epoch": 0.1383794026622452,
      "grad_norm": 0.7005615234375,
      "learning_rate": 4.76944557855992e-05,
      "loss": 1.3115,
      "step": 44400
    },
    {
      "epoch": 0.13853523532290085,
      "grad_norm": 0.6152716279029846,
      "learning_rate": 4.7691858574588264e-05,
      "loss": 1.2708,
      "step": 44450
    },
    {
      "epoch": 0.13869106798355654,
      "grad_norm": 0.48650383949279785,
      "learning_rate": 4.768926136357734e-05,
      "loss": 1.2827,
      "step": 44500
    },
    {
      "epoch": 0.1388469006442122,
      "grad_norm": 0.5549106001853943,
      "learning_rate": 4.7686664152566416e-05,
      "loss": 1.3089,
      "step": 44550
    },
    {
      "epoch": 0.1390027333048679,
      "grad_norm": 0.527289867401123,
      "learning_rate": 4.768406694155548e-05,
      "loss": 1.3186,
      "step": 44600
    },
    {
      "epoch": 0.1391585659655236,
      "grad_norm": 0.5855532884597778,
      "learning_rate": 4.7681469730544554e-05,
      "loss": 1.2571,
      "step": 44650
    },
    {
      "epoch": 0.13931439862617925,
      "grad_norm": 0.6664409041404724,
      "learning_rate": 4.767887251953363e-05,
      "loss": 1.3079,
      "step": 44700
    },
    {
      "epoch": 0.13947023128683494,
      "grad_norm": 0.6634602546691895,
      "learning_rate": 4.76762753085227e-05,
      "loss": 1.2909,
      "step": 44750
    },
    {
      "epoch": 0.13962606394749064,
      "grad_norm": 0.6854048371315002,
      "learning_rate": 4.767367809751177e-05,
      "loss": 1.2783,
      "step": 44800
    },
    {
      "epoch": 0.1397818966081463,
      "grad_norm": 0.7053347229957581,
      "learning_rate": 4.767108088650084e-05,
      "loss": 1.2984,
      "step": 44850
    },
    {
      "epoch": 0.139937729268802,
      "grad_norm": 0.6066932082176208,
      "learning_rate": 4.766848367548992e-05,
      "loss": 1.2631,
      "step": 44900
    },
    {
      "epoch": 0.14009356192945768,
      "grad_norm": 0.7622117400169373,
      "learning_rate": 4.766588646447899e-05,
      "loss": 1.3384,
      "step": 44950
    },
    {
      "epoch": 0.14024939459011335,
      "grad_norm": 0.7526707649230957,
      "learning_rate": 4.7663289253468055e-05,
      "loss": 1.3255,
      "step": 45000
    },
    {
      "epoch": 0.14040522725076904,
      "grad_norm": 0.5595806241035461,
      "learning_rate": 4.766069204245713e-05,
      "loss": 1.2993,
      "step": 45050
    },
    {
      "epoch": 0.14056105991142473,
      "grad_norm": 0.5261533260345459,
      "learning_rate": 4.765809483144621e-05,
      "loss": 1.2996,
      "step": 45100
    },
    {
      "epoch": 0.1407168925720804,
      "grad_norm": 0.5121748447418213,
      "learning_rate": 4.765549762043527e-05,
      "loss": 1.27,
      "step": 45150
    },
    {
      "epoch": 0.14087272523273608,
      "grad_norm": 0.5103694200515747,
      "learning_rate": 4.7652900409424345e-05,
      "loss": 1.3109,
      "step": 45200
    },
    {
      "epoch": 0.14102855789339175,
      "grad_norm": 0.5206372737884521,
      "learning_rate": 4.765030319841342e-05,
      "loss": 1.2882,
      "step": 45250
    },
    {
      "epoch": 0.14118439055404744,
      "grad_norm": 0.6091538667678833,
      "learning_rate": 4.764770598740249e-05,
      "loss": 1.25,
      "step": 45300
    },
    {
      "epoch": 0.14134022321470313,
      "grad_norm": 0.583690345287323,
      "learning_rate": 4.764510877639156e-05,
      "loss": 1.3268,
      "step": 45350
    },
    {
      "epoch": 0.1414960558753588,
      "grad_norm": 0.5274431705474854,
      "learning_rate": 4.7642511565380635e-05,
      "loss": 1.2737,
      "step": 45400
    },
    {
      "epoch": 0.1416518885360145,
      "grad_norm": 1.0306389331817627,
      "learning_rate": 4.763991435436971e-05,
      "loss": 1.2967,
      "step": 45450
    },
    {
      "epoch": 0.14180772119667018,
      "grad_norm": 0.6288632154464722,
      "learning_rate": 4.763731714335878e-05,
      "loss": 1.274,
      "step": 45500
    },
    {
      "epoch": 0.14196355385732584,
      "grad_norm": 0.6862047910690308,
      "learning_rate": 4.7634719932347846e-05,
      "loss": 1.2857,
      "step": 45550
    },
    {
      "epoch": 0.14211938651798153,
      "grad_norm": 0.6466913223266602,
      "learning_rate": 4.763212272133692e-05,
      "loss": 1.2605,
      "step": 45600
    },
    {
      "epoch": 0.14227521917863722,
      "grad_norm": 0.44355010986328125,
      "learning_rate": 4.7629525510326e-05,
      "loss": 1.2609,
      "step": 45650
    },
    {
      "epoch": 0.1424310518392929,
      "grad_norm": 0.5979684591293335,
      "learning_rate": 4.762692829931506e-05,
      "loss": 1.3904,
      "step": 45700
    },
    {
      "epoch": 0.14258688449994858,
      "grad_norm": 0.6139285564422607,
      "learning_rate": 4.7624331088304136e-05,
      "loss": 1.3007,
      "step": 45750
    },
    {
      "epoch": 0.14274271716060424,
      "grad_norm": 0.7001574635505676,
      "learning_rate": 4.7621733877293215e-05,
      "loss": 1.2474,
      "step": 45800
    },
    {
      "epoch": 0.14289854982125993,
      "grad_norm": 0.5156061053276062,
      "learning_rate": 4.761913666628228e-05,
      "loss": 1.2905,
      "step": 45850
    },
    {
      "epoch": 0.14305438248191563,
      "grad_norm": 0.6082873344421387,
      "learning_rate": 4.761653945527135e-05,
      "loss": 1.2814,
      "step": 45900
    },
    {
      "epoch": 0.1432102151425713,
      "grad_norm": 0.5992400050163269,
      "learning_rate": 4.7613942244260426e-05,
      "loss": 1.2918,
      "step": 45950
    },
    {
      "epoch": 0.14336604780322698,
      "grad_norm": 0.6718001961708069,
      "learning_rate": 4.76113450332495e-05,
      "loss": 1.2228,
      "step": 46000
    },
    {
      "epoch": 0.14352188046388267,
      "grad_norm": 0.6390703916549683,
      "learning_rate": 4.760874782223857e-05,
      "loss": 1.348,
      "step": 46050
    },
    {
      "epoch": 0.14367771312453834,
      "grad_norm": 0.6935913562774658,
      "learning_rate": 4.760615061122764e-05,
      "loss": 1.341,
      "step": 46100
    },
    {
      "epoch": 0.14383354578519403,
      "grad_norm": 0.5469261407852173,
      "learning_rate": 4.7603553400216716e-05,
      "loss": 1.3196,
      "step": 46150
    },
    {
      "epoch": 0.14398937844584972,
      "grad_norm": 0.497283399105072,
      "learning_rate": 4.760095618920579e-05,
      "loss": 1.277,
      "step": 46200
    },
    {
      "epoch": 0.14414521110650538,
      "grad_norm": 0.5855477452278137,
      "learning_rate": 4.7598358978194854e-05,
      "loss": 1.2888,
      "step": 46250
    },
    {
      "epoch": 0.14430104376716107,
      "grad_norm": 0.6780020594596863,
      "learning_rate": 4.7595761767183926e-05,
      "loss": 1.3435,
      "step": 46300
    },
    {
      "epoch": 0.14445687642781677,
      "grad_norm": 0.631712794303894,
      "learning_rate": 4.7593164556173006e-05,
      "loss": 1.2982,
      "step": 46350
    },
    {
      "epoch": 0.14461270908847243,
      "grad_norm": 0.629278838634491,
      "learning_rate": 4.759056734516207e-05,
      "loss": 1.2163,
      "step": 46400
    },
    {
      "epoch": 0.14476854174912812,
      "grad_norm": 0.5930646657943726,
      "learning_rate": 4.7587970134151144e-05,
      "loss": 1.3104,
      "step": 46450
    },
    {
      "epoch": 0.14492437440978378,
      "grad_norm": 0.5432794690132141,
      "learning_rate": 4.758537292314022e-05,
      "loss": 1.3126,
      "step": 46500
    },
    {
      "epoch": 0.14508020707043948,
      "grad_norm": 0.6498282551765442,
      "learning_rate": 4.758277571212929e-05,
      "loss": 1.3508,
      "step": 46550
    },
    {
      "epoch": 0.14523603973109517,
      "grad_norm": 0.6671502590179443,
      "learning_rate": 4.758017850111836e-05,
      "loss": 1.2987,
      "step": 46600
    },
    {
      "epoch": 0.14539187239175083,
      "grad_norm": 0.6619839072227478,
      "learning_rate": 4.7577581290107434e-05,
      "loss": 1.3545,
      "step": 46650
    },
    {
      "epoch": 0.14554770505240652,
      "grad_norm": 0.5610492825508118,
      "learning_rate": 4.7574984079096506e-05,
      "loss": 1.2768,
      "step": 46700
    },
    {
      "epoch": 0.14570353771306221,
      "grad_norm": 0.5148947238922119,
      "learning_rate": 4.757238686808558e-05,
      "loss": 1.3472,
      "step": 46750
    },
    {
      "epoch": 0.14585937037371788,
      "grad_norm": 0.6535899639129639,
      "learning_rate": 4.756978965707465e-05,
      "loss": 1.236,
      "step": 46800
    },
    {
      "epoch": 0.14601520303437357,
      "grad_norm": 0.716399610042572,
      "learning_rate": 4.756719244606372e-05,
      "loss": 1.3596,
      "step": 46850
    },
    {
      "epoch": 0.14617103569502926,
      "grad_norm": 0.5054202079772949,
      "learning_rate": 4.7564595235052796e-05,
      "loss": 1.2714,
      "step": 46900
    },
    {
      "epoch": 0.14632686835568492,
      "grad_norm": 0.6806410551071167,
      "learning_rate": 4.756199802404186e-05,
      "loss": 1.2655,
      "step": 46950
    },
    {
      "epoch": 0.14648270101634062,
      "grad_norm": 0.6615918874740601,
      "learning_rate": 4.7559400813030935e-05,
      "loss": 1.3279,
      "step": 47000
    },
    {
      "epoch": 0.1466385336769963,
      "grad_norm": 0.5518030524253845,
      "learning_rate": 4.7556803602020014e-05,
      "loss": 1.2662,
      "step": 47050
    },
    {
      "epoch": 0.14679436633765197,
      "grad_norm": 0.7467600703239441,
      "learning_rate": 4.75542583352293e-05,
      "loss": 1.3342,
      "step": 47100
    },
    {
      "epoch": 0.14695019899830766,
      "grad_norm": 0.7006278038024902,
      "learning_rate": 4.755166112421837e-05,
      "loss": 1.3333,
      "step": 47150
    },
    {
      "epoch": 0.14710603165896333,
      "grad_norm": 0.748554527759552,
      "learning_rate": 4.7549063913207444e-05,
      "loss": 1.2765,
      "step": 47200
    },
    {
      "epoch": 0.14726186431961902,
      "grad_norm": 0.5702926516532898,
      "learning_rate": 4.754646670219652e-05,
      "loss": 1.2998,
      "step": 47250
    },
    {
      "epoch": 0.1474176969802747,
      "grad_norm": 0.7072381973266602,
      "learning_rate": 4.754386949118559e-05,
      "loss": 1.2935,
      "step": 47300
    },
    {
      "epoch": 0.14757352964093037,
      "grad_norm": 0.5056155323982239,
      "learning_rate": 4.754127228017466e-05,
      "loss": 1.2518,
      "step": 47350
    },
    {
      "epoch": 0.14772936230158606,
      "grad_norm": 0.7123218774795532,
      "learning_rate": 4.753867506916373e-05,
      "loss": 1.3274,
      "step": 47400
    },
    {
      "epoch": 0.14788519496224176,
      "grad_norm": 0.5381593704223633,
      "learning_rate": 4.753607785815281e-05,
      "loss": 1.2138,
      "step": 47450
    },
    {
      "epoch": 0.14804102762289742,
      "grad_norm": 0.6295660734176636,
      "learning_rate": 4.753348064714187e-05,
      "loss": 1.2946,
      "step": 47500
    },
    {
      "epoch": 0.1481968602835531,
      "grad_norm": 0.6624152660369873,
      "learning_rate": 4.7530883436130945e-05,
      "loss": 1.3117,
      "step": 47550
    },
    {
      "epoch": 0.1483526929442088,
      "grad_norm": 0.461669921875,
      "learning_rate": 4.752828622512002e-05,
      "loss": 1.2501,
      "step": 47600
    },
    {
      "epoch": 0.14850852560486447,
      "grad_norm": 0.7476308345794678,
      "learning_rate": 4.752568901410909e-05,
      "loss": 1.3033,
      "step": 47650
    },
    {
      "epoch": 0.14866435826552016,
      "grad_norm": 0.5894016027450562,
      "learning_rate": 4.752309180309816e-05,
      "loss": 1.2955,
      "step": 47700
    },
    {
      "epoch": 0.14882019092617582,
      "grad_norm": 0.5479288697242737,
      "learning_rate": 4.7520494592087235e-05,
      "loss": 1.3109,
      "step": 47750
    },
    {
      "epoch": 0.1489760235868315,
      "grad_norm": 0.6771673560142517,
      "learning_rate": 4.751789738107631e-05,
      "loss": 1.2525,
      "step": 47800
    },
    {
      "epoch": 0.1491318562474872,
      "grad_norm": 0.6551806330680847,
      "learning_rate": 4.751530017006538e-05,
      "loss": 1.2812,
      "step": 47850
    },
    {
      "epoch": 0.14928768890814287,
      "grad_norm": 0.5975406169891357,
      "learning_rate": 4.751270295905445e-05,
      "loss": 1.3285,
      "step": 47900
    },
    {
      "epoch": 0.14944352156879856,
      "grad_norm": 0.7483738660812378,
      "learning_rate": 4.751010574804352e-05,
      "loss": 1.3364,
      "step": 47950
    },
    {
      "epoch": 0.14959935422945425,
      "grad_norm": 0.7158468961715698,
      "learning_rate": 4.75075085370326e-05,
      "loss": 1.3399,
      "step": 48000
    },
    {
      "epoch": 0.14975518689010991,
      "grad_norm": 0.526233434677124,
      "learning_rate": 4.750491132602167e-05,
      "loss": 1.2677,
      "step": 48050
    },
    {
      "epoch": 0.1499110195507656,
      "grad_norm": 0.7907243967056274,
      "learning_rate": 4.7502314115010736e-05,
      "loss": 1.2429,
      "step": 48100
    },
    {
      "epoch": 0.1500668522114213,
      "grad_norm": 0.49132493138313293,
      "learning_rate": 4.7499716903999815e-05,
      "loss": 1.3135,
      "step": 48150
    },
    {
      "epoch": 0.15022268487207696,
      "grad_norm": 0.7562090754508972,
      "learning_rate": 4.749711969298888e-05,
      "loss": 1.2948,
      "step": 48200
    },
    {
      "epoch": 0.15037851753273265,
      "grad_norm": 0.8394302129745483,
      "learning_rate": 4.749452248197795e-05,
      "loss": 1.3507,
      "step": 48250
    },
    {
      "epoch": 0.15053435019338834,
      "grad_norm": 0.5934147238731384,
      "learning_rate": 4.7491925270967026e-05,
      "loss": 1.3175,
      "step": 48300
    },
    {
      "epoch": 0.150690182854044,
      "grad_norm": 0.6099887490272522,
      "learning_rate": 4.74893280599561e-05,
      "loss": 1.2897,
      "step": 48350
    },
    {
      "epoch": 0.1508460155146997,
      "grad_norm": 0.49642619490623474,
      "learning_rate": 4.748673084894517e-05,
      "loss": 1.3421,
      "step": 48400
    },
    {
      "epoch": 0.15100184817535536,
      "grad_norm": 0.6047653555870056,
      "learning_rate": 4.748413363793424e-05,
      "loss": 1.2742,
      "step": 48450
    },
    {
      "epoch": 0.15115768083601105,
      "grad_norm": 0.6022976040840149,
      "learning_rate": 4.7481536426923316e-05,
      "loss": 1.2742,
      "step": 48500
    },
    {
      "epoch": 0.15131351349666675,
      "grad_norm": 0.6360905170440674,
      "learning_rate": 4.747893921591239e-05,
      "loss": 1.3057,
      "step": 48550
    },
    {
      "epoch": 0.1514693461573224,
      "grad_norm": 0.551357090473175,
      "learning_rate": 4.747634200490146e-05,
      "loss": 1.2872,
      "step": 48600
    },
    {
      "epoch": 0.1516251788179781,
      "grad_norm": 0.6255840063095093,
      "learning_rate": 4.7473744793890527e-05,
      "loss": 1.3019,
      "step": 48650
    },
    {
      "epoch": 0.1517810114786338,
      "grad_norm": 0.6032414436340332,
      "learning_rate": 4.747119952709982e-05,
      "loss": 1.2696,
      "step": 48700
    },
    {
      "epoch": 0.15193684413928946,
      "grad_norm": 0.5515727996826172,
      "learning_rate": 4.74686023160889e-05,
      "loss": 1.2824,
      "step": 48750
    },
    {
      "epoch": 0.15209267679994515,
      "grad_norm": 0.520797073841095,
      "learning_rate": 4.7466005105077964e-05,
      "loss": 1.3204,
      "step": 48800
    },
    {
      "epoch": 0.15224850946060084,
      "grad_norm": 0.6213647723197937,
      "learning_rate": 4.7463407894067036e-05,
      "loss": 1.3306,
      "step": 48850
    },
    {
      "epoch": 0.1524043421212565,
      "grad_norm": 0.544164776802063,
      "learning_rate": 4.746081068305611e-05,
      "loss": 1.2429,
      "step": 48900
    },
    {
      "epoch": 0.1525601747819122,
      "grad_norm": 0.523759126663208,
      "learning_rate": 4.745821347204518e-05,
      "loss": 1.2291,
      "step": 48950
    },
    {
      "epoch": 0.15271600744256789,
      "grad_norm": 0.7201810479164124,
      "learning_rate": 4.7455616261034254e-05,
      "loss": 1.3042,
      "step": 49000
    },
    {
      "epoch": 0.15287184010322355,
      "grad_norm": 0.6231229901313782,
      "learning_rate": 4.7453019050023326e-05,
      "loss": 1.3077,
      "step": 49050
    },
    {
      "epoch": 0.15302767276387924,
      "grad_norm": 0.6407829523086548,
      "learning_rate": 4.74504218390124e-05,
      "loss": 1.2665,
      "step": 49100
    },
    {
      "epoch": 0.1531835054245349,
      "grad_norm": 0.5942546725273132,
      "learning_rate": 4.744782462800147e-05,
      "loss": 1.2979,
      "step": 49150
    },
    {
      "epoch": 0.1533393380851906,
      "grad_norm": 0.5979925394058228,
      "learning_rate": 4.744522741699054e-05,
      "loss": 1.2374,
      "step": 49200
    },
    {
      "epoch": 0.1534951707458463,
      "grad_norm": 0.6734188795089722,
      "learning_rate": 4.7442630205979616e-05,
      "loss": 1.3095,
      "step": 49250
    },
    {
      "epoch": 0.15365100340650195,
      "grad_norm": 0.6103375554084778,
      "learning_rate": 4.744003299496869e-05,
      "loss": 1.2826,
      "step": 49300
    },
    {
      "epoch": 0.15380683606715764,
      "grad_norm": 0.5991376638412476,
      "learning_rate": 4.7437435783957754e-05,
      "loss": 1.2649,
      "step": 49350
    },
    {
      "epoch": 0.15396266872781333,
      "grad_norm": 0.7735300660133362,
      "learning_rate": 4.743483857294683e-05,
      "loss": 1.3173,
      "step": 49400
    },
    {
      "epoch": 0.154118501388469,
      "grad_norm": 0.6168805360794067,
      "learning_rate": 4.7432241361935906e-05,
      "loss": 1.3437,
      "step": 49450
    },
    {
      "epoch": 0.1542743340491247,
      "grad_norm": 0.5506919026374817,
      "learning_rate": 4.742964415092497e-05,
      "loss": 1.2792,
      "step": 49500
    },
    {
      "epoch": 0.15443016670978038,
      "grad_norm": 0.65177983045578,
      "learning_rate": 4.7427046939914044e-05,
      "loss": 1.2938,
      "step": 49550
    },
    {
      "epoch": 0.15458599937043604,
      "grad_norm": 0.6306948661804199,
      "learning_rate": 4.742444972890312e-05,
      "loss": 1.301,
      "step": 49600
    },
    {
      "epoch": 0.15474183203109174,
      "grad_norm": 0.6506693363189697,
      "learning_rate": 4.742185251789219e-05,
      "loss": 1.3025,
      "step": 49650
    },
    {
      "epoch": 0.15489766469174743,
      "grad_norm": 0.6562357544898987,
      "learning_rate": 4.741925530688126e-05,
      "loss": 1.3146,
      "step": 49700
    },
    {
      "epoch": 0.1550534973524031,
      "grad_norm": 0.5832184553146362,
      "learning_rate": 4.741665809587033e-05,
      "loss": 1.2902,
      "step": 49750
    },
    {
      "epoch": 0.15520933001305878,
      "grad_norm": 0.6198565363883972,
      "learning_rate": 4.741406088485941e-05,
      "loss": 1.3143,
      "step": 49800
    },
    {
      "epoch": 0.15536516267371445,
      "grad_norm": 0.6375946402549744,
      "learning_rate": 4.741146367384848e-05,
      "loss": 1.25,
      "step": 49850
    },
    {
      "epoch": 0.15552099533437014,
      "grad_norm": 0.7925576567649841,
      "learning_rate": 4.7408866462837545e-05,
      "loss": 1.2343,
      "step": 49900
    },
    {
      "epoch": 0.15567682799502583,
      "grad_norm": 0.5241273045539856,
      "learning_rate": 4.740626925182662e-05,
      "loss": 1.3047,
      "step": 49950
    },
    {
      "epoch": 0.1558326606556815,
      "grad_norm": 0.6378611326217651,
      "learning_rate": 4.74036720408157e-05,
      "loss": 1.3249,
      "step": 50000
    },
    {
      "epoch": 0.15598849331633718,
      "grad_norm": 0.6073411107063293,
      "learning_rate": 4.740107482980476e-05,
      "loss": 1.3362,
      "step": 50050
    },
    {
      "epoch": 0.15614432597699288,
      "grad_norm": 0.6360065937042236,
      "learning_rate": 4.7398477618793835e-05,
      "loss": 1.2765,
      "step": 50100
    },
    {
      "epoch": 0.15630015863764854,
      "grad_norm": 0.4784819781780243,
      "learning_rate": 4.739588040778291e-05,
      "loss": 1.2834,
      "step": 50150
    },
    {
      "epoch": 0.15645599129830423,
      "grad_norm": 0.6639899015426636,
      "learning_rate": 4.739328319677198e-05,
      "loss": 1.3134,
      "step": 50200
    },
    {
      "epoch": 0.15661182395895992,
      "grad_norm": 0.565961480140686,
      "learning_rate": 4.739068598576105e-05,
      "loss": 1.2494,
      "step": 50250
    },
    {
      "epoch": 0.1567676566196156,
      "grad_norm": 0.6243980526924133,
      "learning_rate": 4.7388088774750125e-05,
      "loss": 1.3138,
      "step": 50300
    },
    {
      "epoch": 0.15692348928027128,
      "grad_norm": 0.5265403389930725,
      "learning_rate": 4.73854915637392e-05,
      "loss": 1.3218,
      "step": 50350
    },
    {
      "epoch": 0.15707932194092694,
      "grad_norm": 0.5159313678741455,
      "learning_rate": 4.738289435272827e-05,
      "loss": 1.2397,
      "step": 50400
    },
    {
      "epoch": 0.15723515460158263,
      "grad_norm": 0.5974559187889099,
      "learning_rate": 4.7380297141717336e-05,
      "loss": 1.3008,
      "step": 50450
    },
    {
      "epoch": 0.15739098726223832,
      "grad_norm": 0.5885030627250671,
      "learning_rate": 4.7377699930706415e-05,
      "loss": 1.3078,
      "step": 50500
    },
    {
      "epoch": 0.157546819922894,
      "grad_norm": 0.592151939868927,
      "learning_rate": 4.737510271969549e-05,
      "loss": 1.3624,
      "step": 50550
    },
    {
      "epoch": 0.15770265258354968,
      "grad_norm": 0.58753502368927,
      "learning_rate": 4.7372505508684553e-05,
      "loss": 1.319,
      "step": 50600
    },
    {
      "epoch": 0.15785848524420537,
      "grad_norm": 0.6585125923156738,
      "learning_rate": 4.7369908297673626e-05,
      "loss": 1.2259,
      "step": 50650
    },
    {
      "epoch": 0.15801431790486103,
      "grad_norm": 0.5793206691741943,
      "learning_rate": 4.7367311086662705e-05,
      "loss": 1.2308,
      "step": 50700
    },
    {
      "epoch": 0.15817015056551673,
      "grad_norm": 0.5937303304672241,
      "learning_rate": 4.736471387565177e-05,
      "loss": 1.3012,
      "step": 50750
    },
    {
      "epoch": 0.15832598322617242,
      "grad_norm": 0.6383602023124695,
      "learning_rate": 4.7362116664640844e-05,
      "loss": 1.2795,
      "step": 50800
    },
    {
      "epoch": 0.15848181588682808,
      "grad_norm": 0.6071351170539856,
      "learning_rate": 4.7359519453629916e-05,
      "loss": 1.2741,
      "step": 50850
    },
    {
      "epoch": 0.15863764854748377,
      "grad_norm": 0.4669966399669647,
      "learning_rate": 4.735692224261899e-05,
      "loss": 1.3153,
      "step": 50900
    },
    {
      "epoch": 0.15879348120813946,
      "grad_norm": 0.4810694754123688,
      "learning_rate": 4.735432503160806e-05,
      "loss": 1.2926,
      "step": 50950
    },
    {
      "epoch": 0.15894931386879513,
      "grad_norm": 0.5982159972190857,
      "learning_rate": 4.7351727820597134e-05,
      "loss": 1.3164,
      "step": 51000
    },
    {
      "epoch": 0.15910514652945082,
      "grad_norm": 0.6321678161621094,
      "learning_rate": 4.7349130609586206e-05,
      "loss": 1.248,
      "step": 51050
    },
    {
      "epoch": 0.15926097919010648,
      "grad_norm": 0.664496660232544,
      "learning_rate": 4.734653339857528e-05,
      "loss": 1.2982,
      "step": 51100
    },
    {
      "epoch": 0.15941681185076217,
      "grad_norm": 0.6804054379463196,
      "learning_rate": 4.7343936187564344e-05,
      "loss": 1.2199,
      "step": 51150
    },
    {
      "epoch": 0.15957264451141787,
      "grad_norm": 0.5332902669906616,
      "learning_rate": 4.734133897655342e-05,
      "loss": 1.3142,
      "step": 51200
    },
    {
      "epoch": 0.15972847717207353,
      "grad_norm": 0.4563589096069336,
      "learning_rate": 4.7338741765542496e-05,
      "loss": 1.2966,
      "step": 51250
    },
    {
      "epoch": 0.15988430983272922,
      "grad_norm": 0.6743607521057129,
      "learning_rate": 4.733614455453156e-05,
      "loss": 1.2759,
      "step": 51300
    },
    {
      "epoch": 0.1600401424933849,
      "grad_norm": 0.7027798295021057,
      "learning_rate": 4.7333547343520634e-05,
      "loss": 1.2976,
      "step": 51350
    },
    {
      "epoch": 0.16019597515404058,
      "grad_norm": 0.5894086360931396,
      "learning_rate": 4.7330950132509714e-05,
      "loss": 1.2721,
      "step": 51400
    },
    {
      "epoch": 0.16035180781469627,
      "grad_norm": 0.7255103588104248,
      "learning_rate": 4.732835292149878e-05,
      "loss": 1.2995,
      "step": 51450
    },
    {
      "epoch": 0.16050764047535196,
      "grad_norm": 0.631500780582428,
      "learning_rate": 4.732575571048785e-05,
      "loss": 1.2828,
      "step": 51500
    },
    {
      "epoch": 0.16066347313600762,
      "grad_norm": 0.7047225832939148,
      "learning_rate": 4.7323158499476924e-05,
      "loss": 1.2995,
      "step": 51550
    },
    {
      "epoch": 0.16081930579666331,
      "grad_norm": 0.5260810852050781,
      "learning_rate": 4.7320561288466e-05,
      "loss": 1.3346,
      "step": 51600
    },
    {
      "epoch": 0.160975138457319,
      "grad_norm": 0.5934505462646484,
      "learning_rate": 4.731796407745507e-05,
      "loss": 1.2494,
      "step": 51650
    },
    {
      "epoch": 0.16113097111797467,
      "grad_norm": 0.6663302183151245,
      "learning_rate": 4.731536686644414e-05,
      "loss": 1.2597,
      "step": 51700
    },
    {
      "epoch": 0.16128680377863036,
      "grad_norm": 0.6263468861579895,
      "learning_rate": 4.7312769655433214e-05,
      "loss": 1.182,
      "step": 51750
    },
    {
      "epoch": 0.16144263643928602,
      "grad_norm": 0.5685796141624451,
      "learning_rate": 4.731017244442229e-05,
      "loss": 1.3038,
      "step": 51800
    },
    {
      "epoch": 0.16159846909994172,
      "grad_norm": 0.5558069944381714,
      "learning_rate": 4.730757523341135e-05,
      "loss": 1.3132,
      "step": 51850
    },
    {
      "epoch": 0.1617543017605974,
      "grad_norm": 0.5868362188339233,
      "learning_rate": 4.7304978022400425e-05,
      "loss": 1.2622,
      "step": 51900
    },
    {
      "epoch": 0.16191013442125307,
      "grad_norm": 0.5877620577812195,
      "learning_rate": 4.7302380811389504e-05,
      "loss": 1.269,
      "step": 51950
    },
    {
      "epoch": 0.16206596708190876,
      "grad_norm": 0.5977360606193542,
      "learning_rate": 4.729978360037857e-05,
      "loss": 1.2879,
      "step": 52000
    },
    {
      "epoch": 0.16222179974256445,
      "grad_norm": 0.6452680826187134,
      "learning_rate": 4.729718638936764e-05,
      "loss": 1.3048,
      "step": 52050
    },
    {
      "epoch": 0.16237763240322012,
      "grad_norm": 0.6917064189910889,
      "learning_rate": 4.7294589178356715e-05,
      "loss": 1.3434,
      "step": 52100
    },
    {
      "epoch": 0.1625334650638758,
      "grad_norm": 0.560120165348053,
      "learning_rate": 4.729199196734579e-05,
      "loss": 1.3113,
      "step": 52150
    },
    {
      "epoch": 0.1626892977245315,
      "grad_norm": 0.6354068517684937,
      "learning_rate": 4.728939475633486e-05,
      "loss": 1.3326,
      "step": 52200
    },
    {
      "epoch": 0.16284513038518716,
      "grad_norm": 0.531467080116272,
      "learning_rate": 4.728679754532393e-05,
      "loss": 1.2942,
      "step": 52250
    },
    {
      "epoch": 0.16300096304584286,
      "grad_norm": 0.5961429476737976,
      "learning_rate": 4.7284200334313005e-05,
      "loss": 1.3128,
      "step": 52300
    },
    {
      "epoch": 0.16315679570649855,
      "grad_norm": 0.5652366280555725,
      "learning_rate": 4.728160312330208e-05,
      "loss": 1.3495,
      "step": 52350
    },
    {
      "epoch": 0.1633126283671542,
      "grad_norm": 0.6784855723381042,
      "learning_rate": 4.727900591229114e-05,
      "loss": 1.2645,
      "step": 52400
    },
    {
      "epoch": 0.1634684610278099,
      "grad_norm": 0.5327118039131165,
      "learning_rate": 4.7276408701280216e-05,
      "loss": 1.2679,
      "step": 52450
    },
    {
      "epoch": 0.16362429368846557,
      "grad_norm": 0.5225902795791626,
      "learning_rate": 4.7273811490269295e-05,
      "loss": 1.2567,
      "step": 52500
    },
    {
      "epoch": 0.16378012634912126,
      "grad_norm": 0.4770641326904297,
      "learning_rate": 4.727121427925836e-05,
      "loss": 1.2611,
      "step": 52550
    },
    {
      "epoch": 0.16393595900977695,
      "grad_norm": 0.7698712944984436,
      "learning_rate": 4.726861706824743e-05,
      "loss": 1.2859,
      "step": 52600
    },
    {
      "epoch": 0.1640917916704326,
      "grad_norm": 0.5703412890434265,
      "learning_rate": 4.726601985723651e-05,
      "loss": 1.3186,
      "step": 52650
    },
    {
      "epoch": 0.1642476243310883,
      "grad_norm": 0.56523597240448,
      "learning_rate": 4.726342264622558e-05,
      "loss": 1.3296,
      "step": 52700
    },
    {
      "epoch": 0.164403456991744,
      "grad_norm": 0.729796826839447,
      "learning_rate": 4.726082543521465e-05,
      "loss": 1.3136,
      "step": 52750
    },
    {
      "epoch": 0.16455928965239966,
      "grad_norm": 0.5407925844192505,
      "learning_rate": 4.725822822420372e-05,
      "loss": 1.2829,
      "step": 52800
    },
    {
      "epoch": 0.16471512231305535,
      "grad_norm": 0.7108252048492432,
      "learning_rate": 4.7255631013192796e-05,
      "loss": 1.2915,
      "step": 52850
    },
    {
      "epoch": 0.16487095497371104,
      "grad_norm": 0.5285722017288208,
      "learning_rate": 4.725303380218187e-05,
      "loss": 1.2813,
      "step": 52900
    },
    {
      "epoch": 0.1650267876343667,
      "grad_norm": 0.7689785361289978,
      "learning_rate": 4.725043659117094e-05,
      "loss": 1.2504,
      "step": 52950
    },
    {
      "epoch": 0.1651826202950224,
      "grad_norm": 0.4969322979450226,
      "learning_rate": 4.724783938016001e-05,
      "loss": 1.2731,
      "step": 53000
    },
    {
      "epoch": 0.16533845295567806,
      "grad_norm": 0.5848227739334106,
      "learning_rate": 4.7245242169149086e-05,
      "loss": 1.2999,
      "step": 53050
    },
    {
      "epoch": 0.16549428561633375,
      "grad_norm": 0.5453819036483765,
      "learning_rate": 4.724264495813815e-05,
      "loss": 1.2221,
      "step": 53100
    },
    {
      "epoch": 0.16565011827698944,
      "grad_norm": 0.6185139417648315,
      "learning_rate": 4.7240047747127224e-05,
      "loss": 1.265,
      "step": 53150
    },
    {
      "epoch": 0.1658059509376451,
      "grad_norm": 0.6024397611618042,
      "learning_rate": 4.72374505361163e-05,
      "loss": 1.3331,
      "step": 53200
    },
    {
      "epoch": 0.1659617835983008,
      "grad_norm": 0.4431383013725281,
      "learning_rate": 4.723485332510537e-05,
      "loss": 1.2798,
      "step": 53250
    },
    {
      "epoch": 0.1661176162589565,
      "grad_norm": 0.605276882648468,
      "learning_rate": 4.723225611409444e-05,
      "loss": 1.3355,
      "step": 53300
    },
    {
      "epoch": 0.16627344891961215,
      "grad_norm": 0.5784716010093689,
      "learning_rate": 4.7229658903083514e-05,
      "loss": 1.2687,
      "step": 53350
    },
    {
      "epoch": 0.16642928158026785,
      "grad_norm": 0.6234331727027893,
      "learning_rate": 4.7227061692072587e-05,
      "loss": 1.34,
      "step": 53400
    },
    {
      "epoch": 0.16658511424092354,
      "grad_norm": 0.553568959236145,
      "learning_rate": 4.722446448106166e-05,
      "loss": 1.2785,
      "step": 53450
    },
    {
      "epoch": 0.1667409469015792,
      "grad_norm": 0.47208699584007263,
      "learning_rate": 4.722186727005073e-05,
      "loss": 1.3165,
      "step": 53500
    },
    {
      "epoch": 0.1668967795622349,
      "grad_norm": 0.7031481266021729,
      "learning_rate": 4.7219270059039804e-05,
      "loss": 1.2737,
      "step": 53550
    },
    {
      "epoch": 0.16705261222289058,
      "grad_norm": 0.6212272644042969,
      "learning_rate": 4.7216672848028877e-05,
      "loss": 1.3033,
      "step": 53600
    },
    {
      "epoch": 0.16720844488354625,
      "grad_norm": 0.7215019464492798,
      "learning_rate": 4.721407563701795e-05,
      "loss": 1.3365,
      "step": 53650
    },
    {
      "epoch": 0.16736427754420194,
      "grad_norm": 0.6680421829223633,
      "learning_rate": 4.7211478426007015e-05,
      "loss": 1.2741,
      "step": 53700
    },
    {
      "epoch": 0.1675201102048576,
      "grad_norm": 0.5331757664680481,
      "learning_rate": 4.7208881214996094e-05,
      "loss": 1.2606,
      "step": 53750
    },
    {
      "epoch": 0.1676759428655133,
      "grad_norm": 0.6193802952766418,
      "learning_rate": 4.720628400398516e-05,
      "loss": 1.3097,
      "step": 53800
    },
    {
      "epoch": 0.16783177552616899,
      "grad_norm": 0.6230438351631165,
      "learning_rate": 4.720368679297423e-05,
      "loss": 1.2907,
      "step": 53850
    },
    {
      "epoch": 0.16798760818682465,
      "grad_norm": 0.7060507535934448,
      "learning_rate": 4.720108958196331e-05,
      "loss": 1.2449,
      "step": 53900
    },
    {
      "epoch": 0.16814344084748034,
      "grad_norm": 0.6673861145973206,
      "learning_rate": 4.719849237095238e-05,
      "loss": 1.3136,
      "step": 53950
    },
    {
      "epoch": 0.16829927350813603,
      "grad_norm": 0.68222576379776,
      "learning_rate": 4.719589515994145e-05,
      "loss": 1.3211,
      "step": 54000
    },
    {
      "epoch": 0.1684551061687917,
      "grad_norm": 0.7842297554016113,
      "learning_rate": 4.719329794893052e-05,
      "loss": 1.3064,
      "step": 54050
    },
    {
      "epoch": 0.1686109388294474,
      "grad_norm": 0.6245383024215698,
      "learning_rate": 4.7190700737919595e-05,
      "loss": 1.3797,
      "step": 54100
    },
    {
      "epoch": 0.16876677149010308,
      "grad_norm": 0.737577497959137,
      "learning_rate": 4.718810352690867e-05,
      "loss": 1.2703,
      "step": 54150
    },
    {
      "epoch": 0.16892260415075874,
      "grad_norm": 0.6095147132873535,
      "learning_rate": 4.718550631589774e-05,
      "loss": 1.2245,
      "step": 54200
    },
    {
      "epoch": 0.16907843681141443,
      "grad_norm": 0.5832446217536926,
      "learning_rate": 4.718290910488681e-05,
      "loss": 1.3105,
      "step": 54250
    },
    {
      "epoch": 0.16923426947207013,
      "grad_norm": 0.6300756931304932,
      "learning_rate": 4.7180311893875885e-05,
      "loss": 1.2529,
      "step": 54300
    },
    {
      "epoch": 0.1693901021327258,
      "grad_norm": 0.7226868867874146,
      "learning_rate": 4.717771468286496e-05,
      "loss": 1.3726,
      "step": 54350
    },
    {
      "epoch": 0.16954593479338148,
      "grad_norm": 0.5497592687606812,
      "learning_rate": 4.717511747185402e-05,
      "loss": 1.3057,
      "step": 54400
    },
    {
      "epoch": 0.16970176745403714,
      "grad_norm": 0.5992254018783569,
      "learning_rate": 4.71725202608431e-05,
      "loss": 1.2584,
      "step": 54450
    },
    {
      "epoch": 0.16985760011469284,
      "grad_norm": 0.5505867004394531,
      "learning_rate": 4.716992304983217e-05,
      "loss": 1.2852,
      "step": 54500
    },
    {
      "epoch": 0.17001343277534853,
      "grad_norm": 0.6675106883049011,
      "learning_rate": 4.716732583882124e-05,
      "loss": 1.2604,
      "step": 54550
    },
    {
      "epoch": 0.1701692654360042,
      "grad_norm": 0.6804835796356201,
      "learning_rate": 4.716483251625075e-05,
      "loss": 1.2962,
      "step": 54600
    },
    {
      "epoch": 0.17032509809665988,
      "grad_norm": 0.5279883146286011,
      "learning_rate": 4.716223530523982e-05,
      "loss": 1.2784,
      "step": 54650
    },
    {
      "epoch": 0.17048093075731557,
      "grad_norm": 0.5971202850341797,
      "learning_rate": 4.71596380942289e-05,
      "loss": 1.2252,
      "step": 54700
    },
    {
      "epoch": 0.17063676341797124,
      "grad_norm": 0.550841212272644,
      "learning_rate": 4.715704088321797e-05,
      "loss": 1.2721,
      "step": 54750
    },
    {
      "epoch": 0.17079259607862693,
      "grad_norm": 0.6309087872505188,
      "learning_rate": 4.7154443672207036e-05,
      "loss": 1.2954,
      "step": 54800
    },
    {
      "epoch": 0.17094842873928262,
      "grad_norm": 0.6179776191711426,
      "learning_rate": 4.7151846461196115e-05,
      "loss": 1.3511,
      "step": 54850
    },
    {
      "epoch": 0.17110426139993828,
      "grad_norm": 0.5271955132484436,
      "learning_rate": 4.714924925018519e-05,
      "loss": 1.2535,
      "step": 54900
    },
    {
      "epoch": 0.17126009406059398,
      "grad_norm": 0.6633601784706116,
      "learning_rate": 4.714665203917425e-05,
      "loss": 1.2825,
      "step": 54950
    },
    {
      "epoch": 0.17141592672124964,
      "grad_norm": 0.5820274353027344,
      "learning_rate": 4.7144054828163326e-05,
      "loss": 1.3176,
      "step": 55000
    },
    {
      "epoch": 0.17157175938190533,
      "grad_norm": 0.5209540128707886,
      "learning_rate": 4.71414576171524e-05,
      "loss": 1.3447,
      "step": 55050
    },
    {
      "epoch": 0.17172759204256102,
      "grad_norm": 0.5751566886901855,
      "learning_rate": 4.713886040614147e-05,
      "loss": 1.3136,
      "step": 55100
    },
    {
      "epoch": 0.1718834247032167,
      "grad_norm": 0.6558486819267273,
      "learning_rate": 4.713626319513054e-05,
      "loss": 1.2875,
      "step": 55150
    },
    {
      "epoch": 0.17203925736387238,
      "grad_norm": 0.5714756846427917,
      "learning_rate": 4.7133665984119616e-05,
      "loss": 1.2821,
      "step": 55200
    },
    {
      "epoch": 0.17219509002452807,
      "grad_norm": 0.4454645812511444,
      "learning_rate": 4.713106877310869e-05,
      "loss": 1.2816,
      "step": 55250
    },
    {
      "epoch": 0.17235092268518373,
      "grad_norm": 0.5003577470779419,
      "learning_rate": 4.712847156209776e-05,
      "loss": 1.2337,
      "step": 55300
    },
    {
      "epoch": 0.17250675534583942,
      "grad_norm": 0.6348138451576233,
      "learning_rate": 4.7125874351086826e-05,
      "loss": 1.3087,
      "step": 55350
    },
    {
      "epoch": 0.17266258800649512,
      "grad_norm": 0.6000345349311829,
      "learning_rate": 4.7123277140075906e-05,
      "loss": 1.3136,
      "step": 55400
    },
    {
      "epoch": 0.17281842066715078,
      "grad_norm": 0.5770976543426514,
      "learning_rate": 4.712067992906498e-05,
      "loss": 1.2848,
      "step": 55450
    },
    {
      "epoch": 0.17297425332780647,
      "grad_norm": 0.5976584553718567,
      "learning_rate": 4.7118082718054044e-05,
      "loss": 1.2564,
      "step": 55500
    },
    {
      "epoch": 0.17313008598846216,
      "grad_norm": 0.4788415729999542,
      "learning_rate": 4.7115485507043116e-05,
      "loss": 1.3459,
      "step": 55550
    },
    {
      "epoch": 0.17328591864911783,
      "grad_norm": 0.4703768491744995,
      "learning_rate": 4.7112888296032196e-05,
      "loss": 1.2858,
      "step": 55600
    },
    {
      "epoch": 0.17344175130977352,
      "grad_norm": 0.4698921740055084,
      "learning_rate": 4.711029108502126e-05,
      "loss": 1.3097,
      "step": 55650
    },
    {
      "epoch": 0.17359758397042918,
      "grad_norm": 0.5629891753196716,
      "learning_rate": 4.7107693874010334e-05,
      "loss": 1.3011,
      "step": 55700
    },
    {
      "epoch": 0.17375341663108487,
      "grad_norm": 0.6493116617202759,
      "learning_rate": 4.7105096662999406e-05,
      "loss": 1.3071,
      "step": 55750
    },
    {
      "epoch": 0.17390924929174056,
      "grad_norm": 0.48479267954826355,
      "learning_rate": 4.710249945198848e-05,
      "loss": 1.276,
      "step": 55800
    },
    {
      "epoch": 0.17406508195239623,
      "grad_norm": 0.9696296453475952,
      "learning_rate": 4.709990224097755e-05,
      "loss": 1.2595,
      "step": 55850
    },
    {
      "epoch": 0.17422091461305192,
      "grad_norm": 0.5265432000160217,
      "learning_rate": 4.7097305029966624e-05,
      "loss": 1.3095,
      "step": 55900
    },
    {
      "epoch": 0.1743767472737076,
      "grad_norm": 0.5796397924423218,
      "learning_rate": 4.7094707818955696e-05,
      "loss": 1.342,
      "step": 55950
    },
    {
      "epoch": 0.17453257993436327,
      "grad_norm": 0.7524457573890686,
      "learning_rate": 4.709211060794477e-05,
      "loss": 1.3127,
      "step": 56000
    },
    {
      "epoch": 0.17468841259501897,
      "grad_norm": 0.7153769135475159,
      "learning_rate": 4.7089513396933835e-05,
      "loss": 1.2983,
      "step": 56050
    },
    {
      "epoch": 0.17484424525567466,
      "grad_norm": 0.5242143869400024,
      "learning_rate": 4.7086916185922914e-05,
      "loss": 1.2663,
      "step": 56100
    },
    {
      "epoch": 0.17500007791633032,
      "grad_norm": 0.5622346997261047,
      "learning_rate": 4.7084318974911986e-05,
      "loss": 1.2721,
      "step": 56150
    },
    {
      "epoch": 0.175155910576986,
      "grad_norm": 0.536456286907196,
      "learning_rate": 4.708172176390105e-05,
      "loss": 1.2976,
      "step": 56200
    },
    {
      "epoch": 0.1753117432376417,
      "grad_norm": 0.5546011328697205,
      "learning_rate": 4.7079124552890125e-05,
      "loss": 1.3276,
      "step": 56250
    },
    {
      "epoch": 0.17546757589829737,
      "grad_norm": 0.528068482875824,
      "learning_rate": 4.7076527341879204e-05,
      "loss": 1.3021,
      "step": 56300
    },
    {
      "epoch": 0.17562340855895306,
      "grad_norm": 1.0322834253311157,
      "learning_rate": 4.707393013086827e-05,
      "loss": 1.3284,
      "step": 56350
    },
    {
      "epoch": 0.17577924121960872,
      "grad_norm": 0.6589359641075134,
      "learning_rate": 4.707133291985734e-05,
      "loss": 1.2876,
      "step": 56400
    },
    {
      "epoch": 0.17593507388026441,
      "grad_norm": 0.6678602695465088,
      "learning_rate": 4.7068735708846415e-05,
      "loss": 1.2927,
      "step": 56450
    },
    {
      "epoch": 0.1760909065409201,
      "grad_norm": 0.5709044337272644,
      "learning_rate": 4.706613849783549e-05,
      "loss": 1.3093,
      "step": 56500
    },
    {
      "epoch": 0.17624673920157577,
      "grad_norm": 0.6769551634788513,
      "learning_rate": 4.706354128682456e-05,
      "loss": 1.2624,
      "step": 56550
    },
    {
      "epoch": 0.17640257186223146,
      "grad_norm": 0.6009325981140137,
      "learning_rate": 4.706094407581363e-05,
      "loss": 1.2996,
      "step": 56600
    },
    {
      "epoch": 0.17655840452288715,
      "grad_norm": 0.5227594971656799,
      "learning_rate": 4.7058346864802705e-05,
      "loss": 1.3633,
      "step": 56650
    },
    {
      "epoch": 0.17671423718354282,
      "grad_norm": 0.5533131957054138,
      "learning_rate": 4.705574965379178e-05,
      "loss": 1.254,
      "step": 56700
    },
    {
      "epoch": 0.1768700698441985,
      "grad_norm": 0.5110976696014404,
      "learning_rate": 4.705315244278084e-05,
      "loss": 1.2804,
      "step": 56750
    },
    {
      "epoch": 0.1770259025048542,
      "grad_norm": 0.6988949775695801,
      "learning_rate": 4.7050555231769915e-05,
      "loss": 1.255,
      "step": 56800
    },
    {
      "epoch": 0.17718173516550986,
      "grad_norm": 1.1014723777770996,
      "learning_rate": 4.7047958020758995e-05,
      "loss": 1.353,
      "step": 56850
    },
    {
      "epoch": 0.17733756782616555,
      "grad_norm": 0.42577528953552246,
      "learning_rate": 4.704536080974806e-05,
      "loss": 1.2911,
      "step": 56900
    },
    {
      "epoch": 0.17749340048682125,
      "grad_norm": 0.6652510762214661,
      "learning_rate": 4.704276359873713e-05,
      "loss": 1.3362,
      "step": 56950
    },
    {
      "epoch": 0.1776492331474769,
      "grad_norm": 0.7359871864318848,
      "learning_rate": 4.704016638772621e-05,
      "loss": 1.3263,
      "step": 57000
    },
    {
      "epoch": 0.1778050658081326,
      "grad_norm": 0.5226501822471619,
      "learning_rate": 4.703756917671528e-05,
      "loss": 1.3103,
      "step": 57050
    },
    {
      "epoch": 0.17796089846878826,
      "grad_norm": 0.4817734360694885,
      "learning_rate": 4.703497196570435e-05,
      "loss": 1.2992,
      "step": 57100
    },
    {
      "epoch": 0.17811673112944396,
      "grad_norm": 0.695124626159668,
      "learning_rate": 4.703237475469342e-05,
      "loss": 1.2857,
      "step": 57150
    },
    {
      "epoch": 0.17827256379009965,
      "grad_norm": 0.6365607380867004,
      "learning_rate": 4.7029777543682495e-05,
      "loss": 1.3107,
      "step": 57200
    },
    {
      "epoch": 0.1784283964507553,
      "grad_norm": 0.47737324237823486,
      "learning_rate": 4.702718033267157e-05,
      "loss": 1.2345,
      "step": 57250
    },
    {
      "epoch": 0.178584229111411,
      "grad_norm": 0.52214515209198,
      "learning_rate": 4.7024583121660634e-05,
      "loss": 1.2795,
      "step": 57300
    },
    {
      "epoch": 0.1787400617720667,
      "grad_norm": 0.545793354511261,
      "learning_rate": 4.702198591064971e-05,
      "loss": 1.2627,
      "step": 57350
    },
    {
      "epoch": 0.17889589443272236,
      "grad_norm": 0.5430454611778259,
      "learning_rate": 4.7019388699638785e-05,
      "loss": 1.2275,
      "step": 57400
    },
    {
      "epoch": 0.17905172709337805,
      "grad_norm": 0.5432339906692505,
      "learning_rate": 4.701679148862785e-05,
      "loss": 1.2793,
      "step": 57450
    },
    {
      "epoch": 0.17920755975403374,
      "grad_norm": 0.6725878715515137,
      "learning_rate": 4.7014194277616924e-05,
      "loss": 1.2959,
      "step": 57500
    },
    {
      "epoch": 0.1793633924146894,
      "grad_norm": 0.6320638656616211,
      "learning_rate": 4.7011597066606e-05,
      "loss": 1.2601,
      "step": 57550
    },
    {
      "epoch": 0.1795192250753451,
      "grad_norm": 0.5129230618476868,
      "learning_rate": 4.700899985559507e-05,
      "loss": 1.306,
      "step": 57600
    },
    {
      "epoch": 0.17967505773600076,
      "grad_norm": 0.7840808033943176,
      "learning_rate": 4.700640264458414e-05,
      "loss": 1.3114,
      "step": 57650
    },
    {
      "epoch": 0.17983089039665645,
      "grad_norm": 0.4893730580806732,
      "learning_rate": 4.7003805433573214e-05,
      "loss": 1.2796,
      "step": 57700
    },
    {
      "epoch": 0.17998672305731214,
      "grad_norm": 0.5873516201972961,
      "learning_rate": 4.7001208222562286e-05,
      "loss": 1.2732,
      "step": 57750
    },
    {
      "epoch": 0.1801425557179678,
      "grad_norm": 0.6236594319343567,
      "learning_rate": 4.699861101155136e-05,
      "loss": 1.3082,
      "step": 57800
    },
    {
      "epoch": 0.1802983883786235,
      "grad_norm": 0.8165914416313171,
      "learning_rate": 4.699601380054043e-05,
      "loss": 1.3357,
      "step": 57850
    },
    {
      "epoch": 0.1804542210392792,
      "grad_norm": 0.6000475883483887,
      "learning_rate": 4.6993416589529504e-05,
      "loss": 1.2846,
      "step": 57900
    },
    {
      "epoch": 0.18061005369993485,
      "grad_norm": 0.6653291583061218,
      "learning_rate": 4.6990819378518576e-05,
      "loss": 1.3003,
      "step": 57950
    },
    {
      "epoch": 0.18076588636059054,
      "grad_norm": 0.613470733165741,
      "learning_rate": 4.698822216750764e-05,
      "loss": 1.3226,
      "step": 58000
    },
    {
      "epoch": 0.18092171902124624,
      "grad_norm": 0.4694667160511017,
      "learning_rate": 4.6985624956496714e-05,
      "loss": 1.2417,
      "step": 58050
    },
    {
      "epoch": 0.1810775516819019,
      "grad_norm": 0.5886856913566589,
      "learning_rate": 4.6983027745485794e-05,
      "loss": 1.3072,
      "step": 58100
    },
    {
      "epoch": 0.1812333843425576,
      "grad_norm": 0.6833328604698181,
      "learning_rate": 4.698043053447486e-05,
      "loss": 1.2924,
      "step": 58150
    },
    {
      "epoch": 0.18138921700321328,
      "grad_norm": 0.7099449634552002,
      "learning_rate": 4.697783332346393e-05,
      "loss": 1.3084,
      "step": 58200
    },
    {
      "epoch": 0.18154504966386895,
      "grad_norm": 0.5740602016448975,
      "learning_rate": 4.697523611245301e-05,
      "loss": 1.3076,
      "step": 58250
    },
    {
      "epoch": 0.18170088232452464,
      "grad_norm": 0.5194025635719299,
      "learning_rate": 4.697263890144208e-05,
      "loss": 1.2867,
      "step": 58300
    },
    {
      "epoch": 0.1818567149851803,
      "grad_norm": 0.5680169463157654,
      "learning_rate": 4.697004169043115e-05,
      "loss": 1.3025,
      "step": 58350
    },
    {
      "epoch": 0.182012547645836,
      "grad_norm": 0.4742376208305359,
      "learning_rate": 4.696744447942022e-05,
      "loss": 1.3269,
      "step": 58400
    },
    {
      "epoch": 0.18216838030649168,
      "grad_norm": 0.6269148588180542,
      "learning_rate": 4.6964847268409294e-05,
      "loss": 1.2496,
      "step": 58450
    },
    {
      "epoch": 0.18232421296714735,
      "grad_norm": 0.4865037202835083,
      "learning_rate": 4.696225005739837e-05,
      "loss": 1.2176,
      "step": 58500
    },
    {
      "epoch": 0.18248004562780304,
      "grad_norm": 0.6392529606819153,
      "learning_rate": 4.695965284638744e-05,
      "loss": 1.2816,
      "step": 58550
    },
    {
      "epoch": 0.18263587828845873,
      "grad_norm": 0.5653790831565857,
      "learning_rate": 4.6957107579596725e-05,
      "loss": 1.3278,
      "step": 58600
    },
    {
      "epoch": 0.1827917109491144,
      "grad_norm": 0.6262145042419434,
      "learning_rate": 4.6954510368585804e-05,
      "loss": 1.2705,
      "step": 58650
    },
    {
      "epoch": 0.18294754360977009,
      "grad_norm": 0.43185144662857056,
      "learning_rate": 4.695191315757487e-05,
      "loss": 1.3137,
      "step": 58700
    },
    {
      "epoch": 0.18310337627042578,
      "grad_norm": 0.5930753946304321,
      "learning_rate": 4.694931594656394e-05,
      "loss": 1.3109,
      "step": 58750
    },
    {
      "epoch": 0.18325920893108144,
      "grad_norm": 0.458350270986557,
      "learning_rate": 4.6946718735553015e-05,
      "loss": 1.2569,
      "step": 58800
    },
    {
      "epoch": 0.18341504159173713,
      "grad_norm": 0.6241365671157837,
      "learning_rate": 4.694412152454209e-05,
      "loss": 1.2142,
      "step": 58850
    },
    {
      "epoch": 0.18357087425239282,
      "grad_norm": 0.5335342884063721,
      "learning_rate": 4.694152431353116e-05,
      "loss": 1.3026,
      "step": 58900
    },
    {
      "epoch": 0.1837267069130485,
      "grad_norm": 0.602455735206604,
      "learning_rate": 4.693892710252023e-05,
      "loss": 1.3038,
      "step": 58950
    },
    {
      "epoch": 0.18388253957370418,
      "grad_norm": 0.6164147257804871,
      "learning_rate": 4.6936329891509305e-05,
      "loss": 1.2848,
      "step": 59000
    },
    {
      "epoch": 0.18403837223435984,
      "grad_norm": 0.5972850322723389,
      "learning_rate": 4.693373268049838e-05,
      "loss": 1.278,
      "step": 59050
    },
    {
      "epoch": 0.18419420489501553,
      "grad_norm": 0.5610015988349915,
      "learning_rate": 4.693113546948745e-05,
      "loss": 1.2828,
      "step": 59100
    },
    {
      "epoch": 0.18435003755567123,
      "grad_norm": 0.7036600112915039,
      "learning_rate": 4.6928538258476516e-05,
      "loss": 1.3009,
      "step": 59150
    },
    {
      "epoch": 0.1845058702163269,
      "grad_norm": 0.5208149552345276,
      "learning_rate": 4.6925941047465595e-05,
      "loss": 1.3066,
      "step": 59200
    },
    {
      "epoch": 0.18466170287698258,
      "grad_norm": 0.6510921716690063,
      "learning_rate": 4.692334383645467e-05,
      "loss": 1.2967,
      "step": 59250
    },
    {
      "epoch": 0.18481753553763827,
      "grad_norm": 0.6631600260734558,
      "learning_rate": 4.692074662544373e-05,
      "loss": 1.3074,
      "step": 59300
    },
    {
      "epoch": 0.18497336819829394,
      "grad_norm": 0.5808557271957397,
      "learning_rate": 4.691814941443281e-05,
      "loss": 1.2673,
      "step": 59350
    },
    {
      "epoch": 0.18512920085894963,
      "grad_norm": 0.5610140562057495,
      "learning_rate": 4.691555220342188e-05,
      "loss": 1.2694,
      "step": 59400
    },
    {
      "epoch": 0.18528503351960532,
      "grad_norm": 0.5310288667678833,
      "learning_rate": 4.691295499241095e-05,
      "loss": 1.2753,
      "step": 59450
    },
    {
      "epoch": 0.18544086618026098,
      "grad_norm": 0.5361382365226746,
      "learning_rate": 4.691035778140002e-05,
      "loss": 1.3334,
      "step": 59500
    },
    {
      "epoch": 0.18559669884091667,
      "grad_norm": 0.5900775194168091,
      "learning_rate": 4.6907760570389096e-05,
      "loss": 1.2472,
      "step": 59550
    },
    {
      "epoch": 0.18575253150157234,
      "grad_norm": 0.7311044931411743,
      "learning_rate": 4.690516335937817e-05,
      "loss": 1.3038,
      "step": 59600
    },
    {
      "epoch": 0.18590836416222803,
      "grad_norm": 0.8028233647346497,
      "learning_rate": 4.690256614836724e-05,
      "loss": 1.2957,
      "step": 59650
    },
    {
      "epoch": 0.18606419682288372,
      "grad_norm": 0.5221911668777466,
      "learning_rate": 4.6899968937356306e-05,
      "loss": 1.2815,
      "step": 59700
    },
    {
      "epoch": 0.18622002948353938,
      "grad_norm": 0.6366532444953918,
      "learning_rate": 4.6897371726345386e-05,
      "loss": 1.267,
      "step": 59750
    },
    {
      "epoch": 0.18637586214419508,
      "grad_norm": 0.6475458741188049,
      "learning_rate": 4.689477451533446e-05,
      "loss": 1.2794,
      "step": 59800
    },
    {
      "epoch": 0.18653169480485077,
      "grad_norm": 0.6154825687408447,
      "learning_rate": 4.6892177304323524e-05,
      "loss": 1.314,
      "step": 59850
    },
    {
      "epoch": 0.18668752746550643,
      "grad_norm": 0.7863314151763916,
      "learning_rate": 4.68895800933126e-05,
      "loss": 1.3209,
      "step": 59900
    },
    {
      "epoch": 0.18684336012616212,
      "grad_norm": 0.6910321116447449,
      "learning_rate": 4.688698288230167e-05,
      "loss": 1.3071,
      "step": 59950
    },
    {
      "epoch": 0.1869991927868178,
      "grad_norm": 0.6287990808486938,
      "learning_rate": 4.688438567129074e-05,
      "loss": 1.264,
      "step": 60000
    },
    {
      "epoch": 0.18715502544747348,
      "grad_norm": 0.6216883659362793,
      "learning_rate": 4.6881788460279814e-05,
      "loss": 1.2913,
      "step": 60050
    },
    {
      "epoch": 0.18731085810812917,
      "grad_norm": 0.5786150097846985,
      "learning_rate": 4.6879191249268886e-05,
      "loss": 1.2756,
      "step": 60100
    },
    {
      "epoch": 0.18746669076878486,
      "grad_norm": 0.6773401498794556,
      "learning_rate": 4.687659403825796e-05,
      "loss": 1.2745,
      "step": 60150
    },
    {
      "epoch": 0.18762252342944052,
      "grad_norm": 0.812893807888031,
      "learning_rate": 4.687399682724703e-05,
      "loss": 1.3258,
      "step": 60200
    },
    {
      "epoch": 0.18777835609009622,
      "grad_norm": 0.5750018954277039,
      "learning_rate": 4.6871399616236104e-05,
      "loss": 1.3111,
      "step": 60250
    },
    {
      "epoch": 0.18793418875075188,
      "grad_norm": 0.6157856583595276,
      "learning_rate": 4.6868802405225176e-05,
      "loss": 1.2769,
      "step": 60300
    },
    {
      "epoch": 0.18809002141140757,
      "grad_norm": 0.5587257146835327,
      "learning_rate": 4.686620519421425e-05,
      "loss": 1.2972,
      "step": 60350
    },
    {
      "epoch": 0.18824585407206326,
      "grad_norm": 0.4938853085041046,
      "learning_rate": 4.6863607983203315e-05,
      "loss": 1.2913,
      "step": 60400
    },
    {
      "epoch": 0.18840168673271893,
      "grad_norm": 0.6593256592750549,
      "learning_rate": 4.6861010772192394e-05,
      "loss": 1.3124,
      "step": 60450
    },
    {
      "epoch": 0.18855751939337462,
      "grad_norm": 0.5255851745605469,
      "learning_rate": 4.6858413561181466e-05,
      "loss": 1.2629,
      "step": 60500
    },
    {
      "epoch": 0.1887133520540303,
      "grad_norm": 0.7060503959655762,
      "learning_rate": 4.685581635017053e-05,
      "loss": 1.2823,
      "step": 60550
    },
    {
      "epoch": 0.18886918471468597,
      "grad_norm": 0.6652365922927856,
      "learning_rate": 4.6853271083379824e-05,
      "loss": 1.3088,
      "step": 60600
    },
    {
      "epoch": 0.18902501737534166,
      "grad_norm": 0.6527070999145508,
      "learning_rate": 4.68506738723689e-05,
      "loss": 1.264,
      "step": 60650
    },
    {
      "epoch": 0.18918085003599736,
      "grad_norm": 0.5362449884414673,
      "learning_rate": 4.684807666135797e-05,
      "loss": 1.3432,
      "step": 60700
    },
    {
      "epoch": 0.18933668269665302,
      "grad_norm": 0.5410618782043457,
      "learning_rate": 4.684547945034704e-05,
      "loss": 1.2559,
      "step": 60750
    },
    {
      "epoch": 0.1894925153573087,
      "grad_norm": 0.7672276496887207,
      "learning_rate": 4.6842882239336114e-05,
      "loss": 1.2627,
      "step": 60800
    },
    {
      "epoch": 0.1896483480179644,
      "grad_norm": 0.6490287184715271,
      "learning_rate": 4.684028502832519e-05,
      "loss": 1.2832,
      "step": 60850
    },
    {
      "epoch": 0.18980418067862007,
      "grad_norm": 0.6637516021728516,
      "learning_rate": 4.683768781731426e-05,
      "loss": 1.3015,
      "step": 60900
    },
    {
      "epoch": 0.18996001333927576,
      "grad_norm": 0.5007506012916565,
      "learning_rate": 4.6835090606303325e-05,
      "loss": 1.3082,
      "step": 60950
    },
    {
      "epoch": 0.19011584599993142,
      "grad_norm": 0.7084264159202576,
      "learning_rate": 4.6832493395292404e-05,
      "loss": 1.2714,
      "step": 61000
    },
    {
      "epoch": 0.1902716786605871,
      "grad_norm": 0.5587547421455383,
      "learning_rate": 4.682989618428148e-05,
      "loss": 1.2921,
      "step": 61050
    },
    {
      "epoch": 0.1904275113212428,
      "grad_norm": 0.6688686609268188,
      "learning_rate": 4.682729897327054e-05,
      "loss": 1.2323,
      "step": 61100
    },
    {
      "epoch": 0.19058334398189847,
      "grad_norm": 0.5860046148300171,
      "learning_rate": 4.6824701762259615e-05,
      "loss": 1.3094,
      "step": 61150
    },
    {
      "epoch": 0.19073917664255416,
      "grad_norm": 0.7459748387336731,
      "learning_rate": 4.6822104551248694e-05,
      "loss": 1.3045,
      "step": 61200
    },
    {
      "epoch": 0.19089500930320985,
      "grad_norm": 0.6785151362419128,
      "learning_rate": 4.681950734023776e-05,
      "loss": 1.3083,
      "step": 61250
    },
    {
      "epoch": 0.19105084196386551,
      "grad_norm": 0.6480129361152649,
      "learning_rate": 4.681691012922683e-05,
      "loss": 1.3168,
      "step": 61300
    },
    {
      "epoch": 0.1912066746245212,
      "grad_norm": 0.5682808756828308,
      "learning_rate": 4.6814312918215905e-05,
      "loss": 1.3117,
      "step": 61350
    },
    {
      "epoch": 0.1913625072851769,
      "grad_norm": 0.6151596307754517,
      "learning_rate": 4.681171570720498e-05,
      "loss": 1.2963,
      "step": 61400
    },
    {
      "epoch": 0.19151833994583256,
      "grad_norm": 0.6831915974617004,
      "learning_rate": 4.680911849619405e-05,
      "loss": 1.2606,
      "step": 61450
    },
    {
      "epoch": 0.19167417260648825,
      "grad_norm": 0.8361135125160217,
      "learning_rate": 4.680652128518312e-05,
      "loss": 1.3349,
      "step": 61500
    },
    {
      "epoch": 0.19183000526714394,
      "grad_norm": 0.5893268585205078,
      "learning_rate": 4.6803924074172195e-05,
      "loss": 1.2301,
      "step": 61550
    },
    {
      "epoch": 0.1919858379277996,
      "grad_norm": 0.6550257205963135,
      "learning_rate": 4.680132686316127e-05,
      "loss": 1.2891,
      "step": 61600
    },
    {
      "epoch": 0.1921416705884553,
      "grad_norm": 0.5188690423965454,
      "learning_rate": 4.679872965215033e-05,
      "loss": 1.3349,
      "step": 61650
    },
    {
      "epoch": 0.19229750324911096,
      "grad_norm": 0.5736362934112549,
      "learning_rate": 4.6796132441139406e-05,
      "loss": 1.2894,
      "step": 61700
    },
    {
      "epoch": 0.19245333590976665,
      "grad_norm": 0.576630175113678,
      "learning_rate": 4.6793535230128485e-05,
      "loss": 1.2784,
      "step": 61750
    },
    {
      "epoch": 0.19260916857042235,
      "grad_norm": 0.5815929174423218,
      "learning_rate": 4.679093801911755e-05,
      "loss": 1.3211,
      "step": 61800
    },
    {
      "epoch": 0.192765001231078,
      "grad_norm": 0.5877408385276794,
      "learning_rate": 4.678834080810662e-05,
      "loss": 1.307,
      "step": 61850
    },
    {
      "epoch": 0.1929208338917337,
      "grad_norm": 0.7317957878112793,
      "learning_rate": 4.67857435970957e-05,
      "loss": 1.2889,
      "step": 61900
    },
    {
      "epoch": 0.1930766665523894,
      "grad_norm": 0.49923935532569885,
      "learning_rate": 4.678314638608477e-05,
      "loss": 1.3166,
      "step": 61950
    },
    {
      "epoch": 0.19323249921304506,
      "grad_norm": 0.7177022099494934,
      "learning_rate": 4.678054917507384e-05,
      "loss": 1.3077,
      "step": 62000
    },
    {
      "epoch": 0.19338833187370075,
      "grad_norm": 0.6664307117462158,
      "learning_rate": 4.677795196406291e-05,
      "loss": 1.3558,
      "step": 62050
    },
    {
      "epoch": 0.19354416453435644,
      "grad_norm": 0.5353426933288574,
      "learning_rate": 4.6775354753051986e-05,
      "loss": 1.3572,
      "step": 62100
    },
    {
      "epoch": 0.1936999971950121,
      "grad_norm": 0.5262667536735535,
      "learning_rate": 4.677275754204106e-05,
      "loss": 1.2732,
      "step": 62150
    },
    {
      "epoch": 0.1938558298556678,
      "grad_norm": 0.5939745306968689,
      "learning_rate": 4.6770160331030124e-05,
      "loss": 1.244,
      "step": 62200
    },
    {
      "epoch": 0.19401166251632346,
      "grad_norm": 0.693976640701294,
      "learning_rate": 4.67675631200192e-05,
      "loss": 1.2725,
      "step": 62250
    },
    {
      "epoch": 0.19416749517697915,
      "grad_norm": 0.6894903779029846,
      "learning_rate": 4.6764965909008276e-05,
      "loss": 1.3027,
      "step": 62300
    },
    {
      "epoch": 0.19432332783763484,
      "grad_norm": 0.7556319832801819,
      "learning_rate": 4.676236869799734e-05,
      "loss": 1.2921,
      "step": 62350
    },
    {
      "epoch": 0.1944791604982905,
      "grad_norm": 0.5148922801017761,
      "learning_rate": 4.6759771486986414e-05,
      "loss": 1.2683,
      "step": 62400
    },
    {
      "epoch": 0.1946349931589462,
      "grad_norm": 0.5306206345558167,
      "learning_rate": 4.675717427597549e-05,
      "loss": 1.2964,
      "step": 62450
    },
    {
      "epoch": 0.1947908258196019,
      "grad_norm": 0.5471022725105286,
      "learning_rate": 4.675457706496456e-05,
      "loss": 1.3016,
      "step": 62500
    },
    {
      "epoch": 0.19494665848025755,
      "grad_norm": 0.6895194053649902,
      "learning_rate": 4.675197985395363e-05,
      "loss": 1.2919,
      "step": 62550
    },
    {
      "epoch": 0.19510249114091324,
      "grad_norm": 0.5279228091239929,
      "learning_rate": 4.6749382642942704e-05,
      "loss": 1.2743,
      "step": 62600
    },
    {
      "epoch": 0.19525832380156893,
      "grad_norm": 0.61063152551651,
      "learning_rate": 4.6746785431931776e-05,
      "loss": 1.2859,
      "step": 62650
    },
    {
      "epoch": 0.1954141564622246,
      "grad_norm": 0.6058220863342285,
      "learning_rate": 4.674418822092085e-05,
      "loss": 1.3146,
      "step": 62700
    },
    {
      "epoch": 0.1955699891228803,
      "grad_norm": 0.6152732372283936,
      "learning_rate": 4.674159100990992e-05,
      "loss": 1.3165,
      "step": 62750
    },
    {
      "epoch": 0.19572582178353598,
      "grad_norm": 0.6374164819717407,
      "learning_rate": 4.6738993798898994e-05,
      "loss": 1.276,
      "step": 62800
    },
    {
      "epoch": 0.19588165444419164,
      "grad_norm": 0.5670260190963745,
      "learning_rate": 4.6736396587888066e-05,
      "loss": 1.3247,
      "step": 62850
    },
    {
      "epoch": 0.19603748710484734,
      "grad_norm": 0.7136686444282532,
      "learning_rate": 4.673379937687713e-05,
      "loss": 1.2817,
      "step": 62900
    },
    {
      "epoch": 0.196193319765503,
      "grad_norm": 0.5999757647514343,
      "learning_rate": 4.6731202165866205e-05,
      "loss": 1.2342,
      "step": 62950
    },
    {
      "epoch": 0.1963491524261587,
      "grad_norm": 0.8270983695983887,
      "learning_rate": 4.6728604954855284e-05,
      "loss": 1.2666,
      "step": 63000
    },
    {
      "epoch": 0.19650498508681438,
      "grad_norm": 0.6094933152198792,
      "learning_rate": 4.672600774384435e-05,
      "loss": 1.3077,
      "step": 63050
    },
    {
      "epoch": 0.19666081774747005,
      "grad_norm": 0.35794368386268616,
      "learning_rate": 4.672341053283342e-05,
      "loss": 1.2706,
      "step": 63100
    },
    {
      "epoch": 0.19681665040812574,
      "grad_norm": 0.4591834247112274,
      "learning_rate": 4.67208133218225e-05,
      "loss": 1.2844,
      "step": 63150
    },
    {
      "epoch": 0.19697248306878143,
      "grad_norm": 0.5197128653526306,
      "learning_rate": 4.671821611081157e-05,
      "loss": 1.2565,
      "step": 63200
    },
    {
      "epoch": 0.1971283157294371,
      "grad_norm": 0.5964681506156921,
      "learning_rate": 4.671561889980064e-05,
      "loss": 1.2632,
      "step": 63250
    },
    {
      "epoch": 0.19728414839009278,
      "grad_norm": 0.61607426404953,
      "learning_rate": 4.671302168878971e-05,
      "loss": 1.2789,
      "step": 63300
    },
    {
      "epoch": 0.19743998105074848,
      "grad_norm": 0.5795137286186218,
      "learning_rate": 4.6710424477778785e-05,
      "loss": 1.2833,
      "step": 63350
    },
    {
      "epoch": 0.19759581371140414,
      "grad_norm": 0.5214031338691711,
      "learning_rate": 4.670782726676786e-05,
      "loss": 1.2673,
      "step": 63400
    },
    {
      "epoch": 0.19775164637205983,
      "grad_norm": 0.469007283449173,
      "learning_rate": 4.670528199997715e-05,
      "loss": 1.2718,
      "step": 63450
    },
    {
      "epoch": 0.19790747903271552,
      "grad_norm": 0.6888889670372009,
      "learning_rate": 4.6702684788966215e-05,
      "loss": 1.2942,
      "step": 63500
    },
    {
      "epoch": 0.19806331169337119,
      "grad_norm": 0.5400487184524536,
      "learning_rate": 4.6700087577955294e-05,
      "loss": 1.2491,
      "step": 63550
    },
    {
      "epoch": 0.19821914435402688,
      "grad_norm": 0.43381232023239136,
      "learning_rate": 4.669749036694436e-05,
      "loss": 1.2621,
      "step": 63600
    },
    {
      "epoch": 0.19837497701468254,
      "grad_norm": 0.759233832359314,
      "learning_rate": 4.669489315593343e-05,
      "loss": 1.2726,
      "step": 63650
    },
    {
      "epoch": 0.19853080967533823,
      "grad_norm": 0.4882024824619293,
      "learning_rate": 4.6692295944922505e-05,
      "loss": 1.2999,
      "step": 63700
    },
    {
      "epoch": 0.19868664233599392,
      "grad_norm": 0.5794182419776917,
      "learning_rate": 4.668969873391158e-05,
      "loss": 1.2727,
      "step": 63750
    },
    {
      "epoch": 0.1988424749966496,
      "grad_norm": 0.7788763046264648,
      "learning_rate": 4.668710152290065e-05,
      "loss": 1.2853,
      "step": 63800
    },
    {
      "epoch": 0.19899830765730528,
      "grad_norm": 0.4552573263645172,
      "learning_rate": 4.668450431188972e-05,
      "loss": 1.2877,
      "step": 63850
    },
    {
      "epoch": 0.19915414031796097,
      "grad_norm": 0.536320686340332,
      "learning_rate": 4.6681907100878795e-05,
      "loss": 1.3078,
      "step": 63900
    },
    {
      "epoch": 0.19930997297861663,
      "grad_norm": 0.7263410091400146,
      "learning_rate": 4.667930988986787e-05,
      "loss": 1.2873,
      "step": 63950
    },
    {
      "epoch": 0.19946580563927233,
      "grad_norm": 0.7510873675346375,
      "learning_rate": 4.667671267885694e-05,
      "loss": 1.2952,
      "step": 64000
    },
    {
      "epoch": 0.19962163829992802,
      "grad_norm": 0.7078385949134827,
      "learning_rate": 4.6674115467846006e-05,
      "loss": 1.2906,
      "step": 64050
    },
    {
      "epoch": 0.19977747096058368,
      "grad_norm": 0.5398227572441101,
      "learning_rate": 4.6671518256835085e-05,
      "loss": 1.2587,
      "step": 64100
    },
    {
      "epoch": 0.19993330362123937,
      "grad_norm": 0.8083759546279907,
      "learning_rate": 4.666892104582416e-05,
      "loss": 1.2827,
      "step": 64150
    },
    {
      "epoch": 0.20008913628189506,
      "grad_norm": 0.5607863664627075,
      "learning_rate": 4.666632383481322e-05,
      "loss": 1.2189,
      "step": 64200
    },
    {
      "epoch": 0.20024496894255073,
      "grad_norm": 0.5749654769897461,
      "learning_rate": 4.66637266238023e-05,
      "loss": 1.2696,
      "step": 64250
    },
    {
      "epoch": 0.20040080160320642,
      "grad_norm": 0.508385956287384,
      "learning_rate": 4.666112941279137e-05,
      "loss": 1.257,
      "step": 64300
    },
    {
      "epoch": 0.20055663426386208,
      "grad_norm": 0.5113421082496643,
      "learning_rate": 4.665853220178044e-05,
      "loss": 1.2471,
      "step": 64350
    },
    {
      "epoch": 0.20071246692451777,
      "grad_norm": 0.591773509979248,
      "learning_rate": 4.6655934990769513e-05,
      "loss": 1.2728,
      "step": 64400
    },
    {
      "epoch": 0.20086829958517347,
      "grad_norm": 0.6883741617202759,
      "learning_rate": 4.6653337779758586e-05,
      "loss": 1.2983,
      "step": 64450
    },
    {
      "epoch": 0.20102413224582913,
      "grad_norm": 0.6746755242347717,
      "learning_rate": 4.665074056874766e-05,
      "loss": 1.2335,
      "step": 64500
    },
    {
      "epoch": 0.20117996490648482,
      "grad_norm": 0.6022537350654602,
      "learning_rate": 4.664814335773673e-05,
      "loss": 1.3162,
      "step": 64550
    },
    {
      "epoch": 0.2013357975671405,
      "grad_norm": 0.6761201024055481,
      "learning_rate": 4.6645546146725803e-05,
      "loss": 1.2771,
      "step": 64600
    },
    {
      "epoch": 0.20149163022779618,
      "grad_norm": 0.647148072719574,
      "learning_rate": 4.6642948935714876e-05,
      "loss": 1.348,
      "step": 64650
    },
    {
      "epoch": 0.20164746288845187,
      "grad_norm": 0.5826601982116699,
      "learning_rate": 4.664035172470395e-05,
      "loss": 1.2329,
      "step": 64700
    },
    {
      "epoch": 0.20180329554910756,
      "grad_norm": 0.628215491771698,
      "learning_rate": 4.6637754513693014e-05,
      "loss": 1.2789,
      "step": 64750
    },
    {
      "epoch": 0.20195912820976322,
      "grad_norm": 0.6472412943840027,
      "learning_rate": 4.6635157302682093e-05,
      "loss": 1.3025,
      "step": 64800
    },
    {
      "epoch": 0.2021149608704189,
      "grad_norm": 0.5969371795654297,
      "learning_rate": 4.663256009167116e-05,
      "loss": 1.2825,
      "step": 64850
    },
    {
      "epoch": 0.20227079353107458,
      "grad_norm": 0.5639589428901672,
      "learning_rate": 4.662996288066023e-05,
      "loss": 1.3038,
      "step": 64900
    },
    {
      "epoch": 0.20242662619173027,
      "grad_norm": 0.5382996797561646,
      "learning_rate": 4.6627365669649304e-05,
      "loss": 1.3117,
      "step": 64950
    },
    {
      "epoch": 0.20258245885238596,
      "grad_norm": 0.5715367794036865,
      "learning_rate": 4.662476845863838e-05,
      "loss": 1.2814,
      "step": 65000
    },
    {
      "epoch": 0.20273829151304162,
      "grad_norm": 0.7551167607307434,
      "learning_rate": 4.662217124762745e-05,
      "loss": 1.3443,
      "step": 65050
    },
    {
      "epoch": 0.20289412417369732,
      "grad_norm": 0.4995189309120178,
      "learning_rate": 4.661957403661652e-05,
      "loss": 1.3279,
      "step": 65100
    },
    {
      "epoch": 0.203049956834353,
      "grad_norm": 0.728512167930603,
      "learning_rate": 4.6616976825605594e-05,
      "loss": 1.3124,
      "step": 65150
    },
    {
      "epoch": 0.20320578949500867,
      "grad_norm": 0.5708598494529724,
      "learning_rate": 4.661437961459467e-05,
      "loss": 1.3464,
      "step": 65200
    },
    {
      "epoch": 0.20336162215566436,
      "grad_norm": 0.5918168425559998,
      "learning_rate": 4.661178240358374e-05,
      "loss": 1.315,
      "step": 65250
    },
    {
      "epoch": 0.20351745481632005,
      "grad_norm": 0.5681039690971375,
      "learning_rate": 4.6609185192572805e-05,
      "loss": 1.2649,
      "step": 65300
    },
    {
      "epoch": 0.20367328747697572,
      "grad_norm": 0.7084054946899414,
      "learning_rate": 4.6606587981561884e-05,
      "loss": 1.3076,
      "step": 65350
    },
    {
      "epoch": 0.2038291201376314,
      "grad_norm": 0.6380575895309448,
      "learning_rate": 4.660399077055096e-05,
      "loss": 1.3857,
      "step": 65400
    },
    {
      "epoch": 0.2039849527982871,
      "grad_norm": 0.63450026512146,
      "learning_rate": 4.660139355954002e-05,
      "loss": 1.2811,
      "step": 65450
    },
    {
      "epoch": 0.20414078545894276,
      "grad_norm": 0.49069610238075256,
      "learning_rate": 4.65987963485291e-05,
      "loss": 1.31,
      "step": 65500
    },
    {
      "epoch": 0.20429661811959846,
      "grad_norm": 0.7103584408760071,
      "learning_rate": 4.659619913751817e-05,
      "loss": 1.2822,
      "step": 65550
    },
    {
      "epoch": 0.20445245078025412,
      "grad_norm": 0.47147369384765625,
      "learning_rate": 4.659360192650724e-05,
      "loss": 1.276,
      "step": 65600
    },
    {
      "epoch": 0.2046082834409098,
      "grad_norm": 0.5865196585655212,
      "learning_rate": 4.659100471549631e-05,
      "loss": 1.2518,
      "step": 65650
    },
    {
      "epoch": 0.2047641161015655,
      "grad_norm": 0.5246824026107788,
      "learning_rate": 4.6588407504485385e-05,
      "loss": 1.2537,
      "step": 65700
    },
    {
      "epoch": 0.20491994876222117,
      "grad_norm": 0.6241628527641296,
      "learning_rate": 4.658581029347446e-05,
      "loss": 1.2999,
      "step": 65750
    },
    {
      "epoch": 0.20507578142287686,
      "grad_norm": 0.6123270988464355,
      "learning_rate": 4.658321308246353e-05,
      "loss": 1.2867,
      "step": 65800
    },
    {
      "epoch": 0.20523161408353255,
      "grad_norm": 0.7367225885391235,
      "learning_rate": 4.65806158714526e-05,
      "loss": 1.3309,
      "step": 65850
    },
    {
      "epoch": 0.2053874467441882,
      "grad_norm": 0.5684239268302917,
      "learning_rate": 4.6578018660441675e-05,
      "loss": 1.2368,
      "step": 65900
    },
    {
      "epoch": 0.2055432794048439,
      "grad_norm": 0.6555652022361755,
      "learning_rate": 4.657542144943075e-05,
      "loss": 1.203,
      "step": 65950
    },
    {
      "epoch": 0.2056991120654996,
      "grad_norm": 0.6085348725318909,
      "learning_rate": 4.657282423841981e-05,
      "loss": 1.3143,
      "step": 66000
    },
    {
      "epoch": 0.20585494472615526,
      "grad_norm": 0.7464703321456909,
      "learning_rate": 4.657022702740889e-05,
      "loss": 1.2456,
      "step": 66050
    },
    {
      "epoch": 0.20601077738681095,
      "grad_norm": 0.6690695881843567,
      "learning_rate": 4.6567629816397965e-05,
      "loss": 1.3512,
      "step": 66100
    },
    {
      "epoch": 0.20616661004746664,
      "grad_norm": 0.5802823901176453,
      "learning_rate": 4.656503260538703e-05,
      "loss": 1.3003,
      "step": 66150
    },
    {
      "epoch": 0.2063224427081223,
      "grad_norm": 0.5819591283798218,
      "learning_rate": 4.65624353943761e-05,
      "loss": 1.3067,
      "step": 66200
    },
    {
      "epoch": 0.206478275368778,
      "grad_norm": 0.5117917060852051,
      "learning_rate": 4.6559838183365176e-05,
      "loss": 1.2968,
      "step": 66250
    },
    {
      "epoch": 0.20663410802943366,
      "grad_norm": 0.5727739930152893,
      "learning_rate": 4.655724097235425e-05,
      "loss": 1.3033,
      "step": 66300
    },
    {
      "epoch": 0.20678994069008935,
      "grad_norm": 0.7060168385505676,
      "learning_rate": 4.655464376134332e-05,
      "loss": 1.2167,
      "step": 66350
    },
    {
      "epoch": 0.20694577335074504,
      "grad_norm": 0.5490187406539917,
      "learning_rate": 4.655204655033239e-05,
      "loss": 1.2873,
      "step": 66400
    },
    {
      "epoch": 0.2071016060114007,
      "grad_norm": 0.5274326801300049,
      "learning_rate": 4.6549501283541685e-05,
      "loss": 1.3695,
      "step": 66450
    },
    {
      "epoch": 0.2072574386720564,
      "grad_norm": 0.5958650708198547,
      "learning_rate": 4.654690407253076e-05,
      "loss": 1.2324,
      "step": 66500
    },
    {
      "epoch": 0.2074132713327121,
      "grad_norm": 0.4372328221797943,
      "learning_rate": 4.6544306861519824e-05,
      "loss": 1.2887,
      "step": 66550
    },
    {
      "epoch": 0.20756910399336775,
      "grad_norm": 0.6431561708450317,
      "learning_rate": 4.65417096505089e-05,
      "loss": 1.2875,
      "step": 66600
    },
    {
      "epoch": 0.20772493665402345,
      "grad_norm": 0.7644646763801575,
      "learning_rate": 4.6539112439497975e-05,
      "loss": 1.2811,
      "step": 66650
    },
    {
      "epoch": 0.20788076931467914,
      "grad_norm": 0.5666146874427795,
      "learning_rate": 4.653651522848704e-05,
      "loss": 1.2754,
      "step": 66700
    },
    {
      "epoch": 0.2080366019753348,
      "grad_norm": 0.5595414042472839,
      "learning_rate": 4.6533918017476114e-05,
      "loss": 1.251,
      "step": 66750
    },
    {
      "epoch": 0.2081924346359905,
      "grad_norm": 0.7823895215988159,
      "learning_rate": 4.653132080646519e-05,
      "loss": 1.3021,
      "step": 66800
    },
    {
      "epoch": 0.20834826729664616,
      "grad_norm": 0.4601406157016754,
      "learning_rate": 4.652872359545426e-05,
      "loss": 1.2543,
      "step": 66850
    },
    {
      "epoch": 0.20850409995730185,
      "grad_norm": 0.5399758815765381,
      "learning_rate": 4.652612638444333e-05,
      "loss": 1.241,
      "step": 66900
    },
    {
      "epoch": 0.20865993261795754,
      "grad_norm": 0.5484157800674438,
      "learning_rate": 4.6523529173432404e-05,
      "loss": 1.251,
      "step": 66950
    },
    {
      "epoch": 0.2088157652786132,
      "grad_norm": 0.612287700176239,
      "learning_rate": 4.6520931962421476e-05,
      "loss": 1.2407,
      "step": 67000
    },
    {
      "epoch": 0.2089715979392689,
      "grad_norm": 0.657568633556366,
      "learning_rate": 4.651833475141055e-05,
      "loss": 1.2534,
      "step": 67050
    },
    {
      "epoch": 0.20912743059992459,
      "grad_norm": 0.7256750464439392,
      "learning_rate": 4.6515737540399614e-05,
      "loss": 1.2852,
      "step": 67100
    },
    {
      "epoch": 0.20928326326058025,
      "grad_norm": 0.569068193435669,
      "learning_rate": 4.6513140329388694e-05,
      "loss": 1.2825,
      "step": 67150
    },
    {
      "epoch": 0.20943909592123594,
      "grad_norm": 0.5841131210327148,
      "learning_rate": 4.6510543118377766e-05,
      "loss": 1.2867,
      "step": 67200
    },
    {
      "epoch": 0.20959492858189163,
      "grad_norm": 0.5607755780220032,
      "learning_rate": 4.650794590736683e-05,
      "loss": 1.2401,
      "step": 67250
    },
    {
      "epoch": 0.2097507612425473,
      "grad_norm": 0.6084451675415039,
      "learning_rate": 4.6505348696355904e-05,
      "loss": 1.3017,
      "step": 67300
    },
    {
      "epoch": 0.209906593903203,
      "grad_norm": 0.5152478218078613,
      "learning_rate": 4.6502751485344984e-05,
      "loss": 1.2703,
      "step": 67350
    },
    {
      "epoch": 0.21006242656385868,
      "grad_norm": 0.6660149097442627,
      "learning_rate": 4.650015427433405e-05,
      "loss": 1.2934,
      "step": 67400
    },
    {
      "epoch": 0.21021825922451434,
      "grad_norm": 0.5849317312240601,
      "learning_rate": 4.649755706332312e-05,
      "loss": 1.3076,
      "step": 67450
    },
    {
      "epoch": 0.21037409188517003,
      "grad_norm": 0.5215211510658264,
      "learning_rate": 4.6494959852312194e-05,
      "loss": 1.3181,
      "step": 67500
    },
    {
      "epoch": 0.2105299245458257,
      "grad_norm": 0.6967973709106445,
      "learning_rate": 4.649236264130127e-05,
      "loss": 1.3053,
      "step": 67550
    },
    {
      "epoch": 0.2106857572064814,
      "grad_norm": 0.6250441074371338,
      "learning_rate": 4.648976543029034e-05,
      "loss": 1.2743,
      "step": 67600
    },
    {
      "epoch": 0.21084158986713708,
      "grad_norm": 0.7048274874687195,
      "learning_rate": 4.648716821927941e-05,
      "loss": 1.2834,
      "step": 67650
    },
    {
      "epoch": 0.21099742252779274,
      "grad_norm": 0.5840187668800354,
      "learning_rate": 4.6484571008268484e-05,
      "loss": 1.283,
      "step": 67700
    },
    {
      "epoch": 0.21115325518844844,
      "grad_norm": 0.4861902892589569,
      "learning_rate": 4.648197379725756e-05,
      "loss": 1.2971,
      "step": 67750
    },
    {
      "epoch": 0.21130908784910413,
      "grad_norm": 0.6368274092674255,
      "learning_rate": 4.647937658624662e-05,
      "loss": 1.2534,
      "step": 67800
    },
    {
      "epoch": 0.2114649205097598,
      "grad_norm": 0.6637482643127441,
      "learning_rate": 4.64767793752357e-05,
      "loss": 1.2902,
      "step": 67850
    },
    {
      "epoch": 0.21162075317041548,
      "grad_norm": 0.5981953740119934,
      "learning_rate": 4.6474182164224774e-05,
      "loss": 1.2384,
      "step": 67900
    },
    {
      "epoch": 0.21177658583107117,
      "grad_norm": 0.6270100474357605,
      "learning_rate": 4.647158495321384e-05,
      "loss": 1.31,
      "step": 67950
    },
    {
      "epoch": 0.21193241849172684,
      "grad_norm": 0.7264842987060547,
      "learning_rate": 4.646898774220291e-05,
      "loss": 1.3427,
      "step": 68000
    },
    {
      "epoch": 0.21208825115238253,
      "grad_norm": 0.5019625425338745,
      "learning_rate": 4.646639053119199e-05,
      "loss": 1.3089,
      "step": 68050
    },
    {
      "epoch": 0.21224408381303822,
      "grad_norm": 0.7746782302856445,
      "learning_rate": 4.646379332018106e-05,
      "loss": 1.3176,
      "step": 68100
    },
    {
      "epoch": 0.21239991647369388,
      "grad_norm": 0.49563097953796387,
      "learning_rate": 4.646119610917013e-05,
      "loss": 1.2728,
      "step": 68150
    },
    {
      "epoch": 0.21255574913434958,
      "grad_norm": 0.49140456318855286,
      "learning_rate": 4.64585988981592e-05,
      "loss": 1.3201,
      "step": 68200
    },
    {
      "epoch": 0.21271158179500524,
      "grad_norm": 0.639885663986206,
      "learning_rate": 4.6456001687148275e-05,
      "loss": 1.308,
      "step": 68250
    },
    {
      "epoch": 0.21286741445566093,
      "grad_norm": 0.6340157389640808,
      "learning_rate": 4.645340447613735e-05,
      "loss": 1.2965,
      "step": 68300
    },
    {
      "epoch": 0.21302324711631662,
      "grad_norm": 0.5175265073776245,
      "learning_rate": 4.645080726512642e-05,
      "loss": 1.337,
      "step": 68350
    },
    {
      "epoch": 0.21317907977697229,
      "grad_norm": 0.6173223853111267,
      "learning_rate": 4.644821005411549e-05,
      "loss": 1.256,
      "step": 68400
    },
    {
      "epoch": 0.21333491243762798,
      "grad_norm": 0.7605581879615784,
      "learning_rate": 4.6445612843104565e-05,
      "loss": 1.2634,
      "step": 68450
    },
    {
      "epoch": 0.21349074509828367,
      "grad_norm": 0.47486287355422974,
      "learning_rate": 4.644301563209363e-05,
      "loss": 1.2333,
      "step": 68500
    },
    {
      "epoch": 0.21364657775893933,
      "grad_norm": 0.6538503170013428,
      "learning_rate": 4.64404184210827e-05,
      "loss": 1.2919,
      "step": 68550
    },
    {
      "epoch": 0.21380241041959502,
      "grad_norm": 0.6573764681816101,
      "learning_rate": 4.643782121007178e-05,
      "loss": 1.3305,
      "step": 68600
    },
    {
      "epoch": 0.21395824308025072,
      "grad_norm": 0.7415863871574402,
      "learning_rate": 4.643522399906085e-05,
      "loss": 1.2966,
      "step": 68650
    },
    {
      "epoch": 0.21411407574090638,
      "grad_norm": 0.49995601177215576,
      "learning_rate": 4.643262678804992e-05,
      "loss": 1.223,
      "step": 68700
    },
    {
      "epoch": 0.21426990840156207,
      "grad_norm": 0.5276639461517334,
      "learning_rate": 4.6430029577039e-05,
      "loss": 1.2962,
      "step": 68750
    },
    {
      "epoch": 0.21442574106221776,
      "grad_norm": 0.623737633228302,
      "learning_rate": 4.6427432366028066e-05,
      "loss": 1.2518,
      "step": 68800
    },
    {
      "epoch": 0.21458157372287343,
      "grad_norm": 0.587867796421051,
      "learning_rate": 4.642483515501714e-05,
      "loss": 1.2683,
      "step": 68850
    },
    {
      "epoch": 0.21473740638352912,
      "grad_norm": 0.5999011993408203,
      "learning_rate": 4.642223794400621e-05,
      "loss": 1.2849,
      "step": 68900
    },
    {
      "epoch": 0.21489323904418478,
      "grad_norm": 0.4813564717769623,
      "learning_rate": 4.641964073299528e-05,
      "loss": 1.3464,
      "step": 68950
    },
    {
      "epoch": 0.21504907170484047,
      "grad_norm": 0.5506970882415771,
      "learning_rate": 4.6417043521984356e-05,
      "loss": 1.2882,
      "step": 69000
    },
    {
      "epoch": 0.21520490436549616,
      "grad_norm": 0.5990175008773804,
      "learning_rate": 4.641444631097343e-05,
      "loss": 1.29,
      "step": 69050
    },
    {
      "epoch": 0.21536073702615183,
      "grad_norm": 0.7059299349784851,
      "learning_rate": 4.64118490999625e-05,
      "loss": 1.3017,
      "step": 69100
    },
    {
      "epoch": 0.21551656968680752,
      "grad_norm": 0.5940800309181213,
      "learning_rate": 4.640925188895157e-05,
      "loss": 1.2732,
      "step": 69150
    },
    {
      "epoch": 0.2156724023474632,
      "grad_norm": 0.8108808994293213,
      "learning_rate": 4.640665467794064e-05,
      "loss": 1.3124,
      "step": 69200
    },
    {
      "epoch": 0.21582823500811887,
      "grad_norm": 0.7018696665763855,
      "learning_rate": 4.640405746692971e-05,
      "loss": 1.2857,
      "step": 69250
    },
    {
      "epoch": 0.21598406766877457,
      "grad_norm": 0.4780503213405609,
      "learning_rate": 4.640146025591879e-05,
      "loss": 1.242,
      "step": 69300
    },
    {
      "epoch": 0.21613990032943026,
      "grad_norm": 0.4494949281215668,
      "learning_rate": 4.6398863044907857e-05,
      "loss": 1.2937,
      "step": 69350
    },
    {
      "epoch": 0.21629573299008592,
      "grad_norm": 0.7227841019630432,
      "learning_rate": 4.639626583389693e-05,
      "loss": 1.2746,
      "step": 69400
    },
    {
      "epoch": 0.2164515656507416,
      "grad_norm": 0.5682891607284546,
      "learning_rate": 4.6393668622886e-05,
      "loss": 1.3361,
      "step": 69450
    },
    {
      "epoch": 0.21660739831139728,
      "grad_norm": 0.6672849655151367,
      "learning_rate": 4.6391071411875074e-05,
      "loss": 1.2853,
      "step": 69500
    },
    {
      "epoch": 0.21676323097205297,
      "grad_norm": 0.6178155541419983,
      "learning_rate": 4.638847420086415e-05,
      "loss": 1.3622,
      "step": 69550
    },
    {
      "epoch": 0.21691906363270866,
      "grad_norm": 0.6324227452278137,
      "learning_rate": 4.638587698985322e-05,
      "loss": 1.2766,
      "step": 69600
    },
    {
      "epoch": 0.21707489629336432,
      "grad_norm": 0.5885457396507263,
      "learning_rate": 4.638327977884229e-05,
      "loss": 1.2663,
      "step": 69650
    },
    {
      "epoch": 0.21723072895402,
      "grad_norm": 0.640190839767456,
      "learning_rate": 4.6380682567831364e-05,
      "loss": 1.2757,
      "step": 69700
    },
    {
      "epoch": 0.2173865616146757,
      "grad_norm": 0.6598213911056519,
      "learning_rate": 4.637808535682043e-05,
      "loss": 1.2824,
      "step": 69750
    },
    {
      "epoch": 0.21754239427533137,
      "grad_norm": 0.6065354347229004,
      "learning_rate": 4.63754881458095e-05,
      "loss": 1.3174,
      "step": 69800
    },
    {
      "epoch": 0.21769822693598706,
      "grad_norm": 0.7042612433433533,
      "learning_rate": 4.637289093479858e-05,
      "loss": 1.2876,
      "step": 69850
    },
    {
      "epoch": 0.21785405959664275,
      "grad_norm": 0.5661587119102478,
      "learning_rate": 4.637029372378765e-05,
      "loss": 1.2802,
      "step": 69900
    },
    {
      "epoch": 0.21800989225729842,
      "grad_norm": 0.7826735377311707,
      "learning_rate": 4.636769651277672e-05,
      "loss": 1.2866,
      "step": 69950
    },
    {
      "epoch": 0.2181657249179541,
      "grad_norm": 0.6057630777359009,
      "learning_rate": 4.63650993017658e-05,
      "loss": 1.3286,
      "step": 70000
    },
    {
      "epoch": 0.2183215575786098,
      "grad_norm": 0.5641568303108215,
      "learning_rate": 4.6362502090754865e-05,
      "loss": 1.2846,
      "step": 70050
    },
    {
      "epoch": 0.21847739023926546,
      "grad_norm": 0.6100899577140808,
      "learning_rate": 4.635990487974394e-05,
      "loss": 1.3144,
      "step": 70100
    },
    {
      "epoch": 0.21863322289992115,
      "grad_norm": 0.5492218136787415,
      "learning_rate": 4.635730766873301e-05,
      "loss": 1.2967,
      "step": 70150
    },
    {
      "epoch": 0.21878905556057682,
      "grad_norm": 0.449149489402771,
      "learning_rate": 4.635471045772208e-05,
      "loss": 1.2593,
      "step": 70200
    },
    {
      "epoch": 0.2189448882212325,
      "grad_norm": 0.6660369038581848,
      "learning_rate": 4.6352113246711155e-05,
      "loss": 1.2836,
      "step": 70250
    },
    {
      "epoch": 0.2191007208818882,
      "grad_norm": 0.540694534778595,
      "learning_rate": 4.634951603570023e-05,
      "loss": 1.2342,
      "step": 70300
    },
    {
      "epoch": 0.21925655354254386,
      "grad_norm": 0.5647057294845581,
      "learning_rate": 4.63469188246893e-05,
      "loss": 1.2427,
      "step": 70350
    },
    {
      "epoch": 0.21941238620319956,
      "grad_norm": 0.7145959734916687,
      "learning_rate": 4.634432161367837e-05,
      "loss": 1.3418,
      "step": 70400
    },
    {
      "epoch": 0.21956821886385525,
      "grad_norm": 0.4907006025314331,
      "learning_rate": 4.634172440266744e-05,
      "loss": 1.332,
      "step": 70450
    },
    {
      "epoch": 0.2197240515245109,
      "grad_norm": 0.5644859075546265,
      "learning_rate": 4.633912719165651e-05,
      "loss": 1.3088,
      "step": 70500
    },
    {
      "epoch": 0.2198798841851666,
      "grad_norm": 0.7533993721008301,
      "learning_rate": 4.63365819248658e-05,
      "loss": 1.2883,
      "step": 70550
    },
    {
      "epoch": 0.2200357168458223,
      "grad_norm": 0.6795079112052917,
      "learning_rate": 4.6333984713854875e-05,
      "loss": 1.2248,
      "step": 70600
    },
    {
      "epoch": 0.22019154950647796,
      "grad_norm": 0.6312673091888428,
      "learning_rate": 4.633138750284395e-05,
      "loss": 1.2871,
      "step": 70650
    },
    {
      "epoch": 0.22034738216713365,
      "grad_norm": 0.5526705384254456,
      "learning_rate": 4.632879029183302e-05,
      "loss": 1.3203,
      "step": 70700
    },
    {
      "epoch": 0.22050321482778934,
      "grad_norm": 0.6753116846084595,
      "learning_rate": 4.632619308082209e-05,
      "loss": 1.3083,
      "step": 70750
    },
    {
      "epoch": 0.220659047488445,
      "grad_norm": 0.5424126982688904,
      "learning_rate": 4.6323595869811165e-05,
      "loss": 1.2855,
      "step": 70800
    },
    {
      "epoch": 0.2208148801491007,
      "grad_norm": 0.6450543403625488,
      "learning_rate": 4.632105060302046e-05,
      "loss": 1.2697,
      "step": 70850
    },
    {
      "epoch": 0.22097071280975636,
      "grad_norm": 0.6385448575019836,
      "learning_rate": 4.631845339200952e-05,
      "loss": 1.3153,
      "step": 70900
    },
    {
      "epoch": 0.22112654547041205,
      "grad_norm": 0.6255855560302734,
      "learning_rate": 4.63158561809986e-05,
      "loss": 1.2947,
      "step": 70950
    },
    {
      "epoch": 0.22128237813106774,
      "grad_norm": 0.6269358396530151,
      "learning_rate": 4.6313258969987675e-05,
      "loss": 1.2769,
      "step": 71000
    },
    {
      "epoch": 0.2214382107917234,
      "grad_norm": 0.7384969592094421,
      "learning_rate": 4.631066175897674e-05,
      "loss": 1.2648,
      "step": 71050
    },
    {
      "epoch": 0.2215940434523791,
      "grad_norm": 0.48208484053611755,
      "learning_rate": 4.630806454796581e-05,
      "loss": 1.3073,
      "step": 71100
    },
    {
      "epoch": 0.2217498761130348,
      "grad_norm": 0.7402802109718323,
      "learning_rate": 4.6305467336954886e-05,
      "loss": 1.3035,
      "step": 71150
    },
    {
      "epoch": 0.22190570877369045,
      "grad_norm": 0.7635326385498047,
      "learning_rate": 4.630287012594396e-05,
      "loss": 1.3093,
      "step": 71200
    },
    {
      "epoch": 0.22206154143434614,
      "grad_norm": 0.4652780592441559,
      "learning_rate": 4.630027291493303e-05,
      "loss": 1.2578,
      "step": 71250
    },
    {
      "epoch": 0.22221737409500184,
      "grad_norm": 0.5958724617958069,
      "learning_rate": 4.62976757039221e-05,
      "loss": 1.2637,
      "step": 71300
    },
    {
      "epoch": 0.2223732067556575,
      "grad_norm": 0.54151850938797,
      "learning_rate": 4.6295078492911176e-05,
      "loss": 1.2283,
      "step": 71350
    },
    {
      "epoch": 0.2225290394163132,
      "grad_norm": 0.7490941882133484,
      "learning_rate": 4.629248128190025e-05,
      "loss": 1.3136,
      "step": 71400
    },
    {
      "epoch": 0.22268487207696885,
      "grad_norm": 0.46164342761039734,
      "learning_rate": 4.6289884070889314e-05,
      "loss": 1.2614,
      "step": 71450
    },
    {
      "epoch": 0.22284070473762455,
      "grad_norm": 0.528932511806488,
      "learning_rate": 4.628728685987839e-05,
      "loss": 1.3151,
      "step": 71500
    },
    {
      "epoch": 0.22299653739828024,
      "grad_norm": 0.6686425805091858,
      "learning_rate": 4.6284689648867466e-05,
      "loss": 1.2825,
      "step": 71550
    },
    {
      "epoch": 0.2231523700589359,
      "grad_norm": 0.6674734950065613,
      "learning_rate": 4.628209243785653e-05,
      "loss": 1.3012,
      "step": 71600
    },
    {
      "epoch": 0.2233082027195916,
      "grad_norm": 0.6535020470619202,
      "learning_rate": 4.6279495226845604e-05,
      "loss": 1.2742,
      "step": 71650
    },
    {
      "epoch": 0.22346403538024728,
      "grad_norm": 0.6152780652046204,
      "learning_rate": 4.627689801583468e-05,
      "loss": 1.335,
      "step": 71700
    },
    {
      "epoch": 0.22361986804090295,
      "grad_norm": 0.6633535623550415,
      "learning_rate": 4.627430080482375e-05,
      "loss": 1.3124,
      "step": 71750
    },
    {
      "epoch": 0.22377570070155864,
      "grad_norm": 0.5165836215019226,
      "learning_rate": 4.627170359381282e-05,
      "loss": 1.2765,
      "step": 71800
    },
    {
      "epoch": 0.22393153336221433,
      "grad_norm": 0.5673712491989136,
      "learning_rate": 4.6269106382801894e-05,
      "loss": 1.3092,
      "step": 71850
    },
    {
      "epoch": 0.22408736602287,
      "grad_norm": 0.5639752149581909,
      "learning_rate": 4.6266509171790966e-05,
      "loss": 1.2742,
      "step": 71900
    },
    {
      "epoch": 0.22424319868352569,
      "grad_norm": 0.5311650633811951,
      "learning_rate": 4.626391196078004e-05,
      "loss": 1.2554,
      "step": 71950
    },
    {
      "epoch": 0.22439903134418138,
      "grad_norm": 0.562175452709198,
      "learning_rate": 4.6261314749769105e-05,
      "loss": 1.296,
      "step": 72000
    },
    {
      "epoch": 0.22455486400483704,
      "grad_norm": 0.5725643038749695,
      "learning_rate": 4.6258717538758184e-05,
      "loss": 1.2973,
      "step": 72050
    },
    {
      "epoch": 0.22471069666549273,
      "grad_norm": 0.6529726982116699,
      "learning_rate": 4.6256120327747256e-05,
      "loss": 1.277,
      "step": 72100
    },
    {
      "epoch": 0.2248665293261484,
      "grad_norm": 0.4923674464225769,
      "learning_rate": 4.625352311673632e-05,
      "loss": 1.3147,
      "step": 72150
    },
    {
      "epoch": 0.2250223619868041,
      "grad_norm": 0.624786376953125,
      "learning_rate": 4.62509259057254e-05,
      "loss": 1.3141,
      "step": 72200
    },
    {
      "epoch": 0.22517819464745978,
      "grad_norm": 0.605603039264679,
      "learning_rate": 4.6248328694714474e-05,
      "loss": 1.2762,
      "step": 72250
    },
    {
      "epoch": 0.22533402730811544,
      "grad_norm": 0.5627497434616089,
      "learning_rate": 4.624573148370354e-05,
      "loss": 1.2986,
      "step": 72300
    },
    {
      "epoch": 0.22548985996877113,
      "grad_norm": 0.5139368772506714,
      "learning_rate": 4.624313427269261e-05,
      "loss": 1.2929,
      "step": 72350
    },
    {
      "epoch": 0.22564569262942683,
      "grad_norm": 0.47183385491371155,
      "learning_rate": 4.6240537061681685e-05,
      "loss": 1.3023,
      "step": 72400
    },
    {
      "epoch": 0.2258015252900825,
      "grad_norm": 0.5910354256629944,
      "learning_rate": 4.623793985067076e-05,
      "loss": 1.2758,
      "step": 72450
    },
    {
      "epoch": 0.22595735795073818,
      "grad_norm": 0.6033272743225098,
      "learning_rate": 4.623534263965983e-05,
      "loss": 1.3106,
      "step": 72500
    },
    {
      "epoch": 0.22611319061139387,
      "grad_norm": 0.5516175627708435,
      "learning_rate": 4.62327454286489e-05,
      "loss": 1.3273,
      "step": 72550
    },
    {
      "epoch": 0.22626902327204954,
      "grad_norm": 0.6564483046531677,
      "learning_rate": 4.6230148217637975e-05,
      "loss": 1.3392,
      "step": 72600
    },
    {
      "epoch": 0.22642485593270523,
      "grad_norm": 0.45165371894836426,
      "learning_rate": 4.622755100662705e-05,
      "loss": 1.2653,
      "step": 72650
    },
    {
      "epoch": 0.22658068859336092,
      "grad_norm": 0.5875057578086853,
      "learning_rate": 4.622495379561611e-05,
      "loss": 1.2457,
      "step": 72700
    },
    {
      "epoch": 0.22673652125401658,
      "grad_norm": 0.6385176181793213,
      "learning_rate": 4.622235658460519e-05,
      "loss": 1.3232,
      "step": 72750
    },
    {
      "epoch": 0.22689235391467227,
      "grad_norm": 0.548184871673584,
      "learning_rate": 4.6219759373594265e-05,
      "loss": 1.2806,
      "step": 72800
    },
    {
      "epoch": 0.22704818657532794,
      "grad_norm": 0.6916170120239258,
      "learning_rate": 4.621716216258333e-05,
      "loss": 1.3147,
      "step": 72850
    },
    {
      "epoch": 0.22720401923598363,
      "grad_norm": 0.6753619313240051,
      "learning_rate": 4.62145649515724e-05,
      "loss": 1.2873,
      "step": 72900
    },
    {
      "epoch": 0.22735985189663932,
      "grad_norm": 0.675290584564209,
      "learning_rate": 4.621196774056148e-05,
      "loss": 1.2789,
      "step": 72950
    },
    {
      "epoch": 0.22751568455729498,
      "grad_norm": 0.8153882622718811,
      "learning_rate": 4.620937052955055e-05,
      "loss": 1.2639,
      "step": 73000
    },
    {
      "epoch": 0.22767151721795068,
      "grad_norm": 0.6233119964599609,
      "learning_rate": 4.620677331853962e-05,
      "loss": 1.2469,
      "step": 73050
    },
    {
      "epoch": 0.22782734987860637,
      "grad_norm": 0.5969549417495728,
      "learning_rate": 4.620417610752869e-05,
      "loss": 1.2321,
      "step": 73100
    },
    {
      "epoch": 0.22798318253926203,
      "grad_norm": 0.5448806285858154,
      "learning_rate": 4.6201578896517765e-05,
      "loss": 1.2528,
      "step": 73150
    },
    {
      "epoch": 0.22813901519991772,
      "grad_norm": 0.676522970199585,
      "learning_rate": 4.619898168550684e-05,
      "loss": 1.3166,
      "step": 73200
    },
    {
      "epoch": 0.2282948478605734,
      "grad_norm": 0.5689308643341064,
      "learning_rate": 4.619638447449591e-05,
      "loss": 1.3051,
      "step": 73250
    },
    {
      "epoch": 0.22845068052122908,
      "grad_norm": 0.46251481771469116,
      "learning_rate": 4.619378726348498e-05,
      "loss": 1.2859,
      "step": 73300
    },
    {
      "epoch": 0.22860651318188477,
      "grad_norm": 0.7993087768554688,
      "learning_rate": 4.6191190052474055e-05,
      "loss": 1.2494,
      "step": 73350
    },
    {
      "epoch": 0.22876234584254046,
      "grad_norm": 0.5669376850128174,
      "learning_rate": 4.618859284146312e-05,
      "loss": 1.2668,
      "step": 73400
    },
    {
      "epoch": 0.22891817850319612,
      "grad_norm": 0.6385453343391418,
      "learning_rate": 4.61859956304522e-05,
      "loss": 1.2938,
      "step": 73450
    },
    {
      "epoch": 0.22907401116385182,
      "grad_norm": 0.5411500334739685,
      "learning_rate": 4.618339841944127e-05,
      "loss": 1.3361,
      "step": 73500
    },
    {
      "epoch": 0.22922984382450748,
      "grad_norm": 0.5956188440322876,
      "learning_rate": 4.618080120843034e-05,
      "loss": 1.3177,
      "step": 73550
    },
    {
      "epoch": 0.22938567648516317,
      "grad_norm": 0.613857626914978,
      "learning_rate": 4.617820399741941e-05,
      "loss": 1.3219,
      "step": 73600
    },
    {
      "epoch": 0.22954150914581886,
      "grad_norm": 0.5228366255760193,
      "learning_rate": 4.617560678640849e-05,
      "loss": 1.2672,
      "step": 73650
    },
    {
      "epoch": 0.22969734180647453,
      "grad_norm": 0.7445792555809021,
      "learning_rate": 4.6173009575397556e-05,
      "loss": 1.3079,
      "step": 73700
    },
    {
      "epoch": 0.22985317446713022,
      "grad_norm": 0.6832590699195862,
      "learning_rate": 4.617041236438663e-05,
      "loss": 1.3308,
      "step": 73750
    },
    {
      "epoch": 0.2300090071277859,
      "grad_norm": 0.6676473021507263,
      "learning_rate": 4.61678151533757e-05,
      "loss": 1.302,
      "step": 73800
    },
    {
      "epoch": 0.23016483978844157,
      "grad_norm": 0.5063507556915283,
      "learning_rate": 4.6165217942364774e-05,
      "loss": 1.3165,
      "step": 73850
    },
    {
      "epoch": 0.23032067244909726,
      "grad_norm": 0.6791144609451294,
      "learning_rate": 4.6162620731353846e-05,
      "loss": 1.3221,
      "step": 73900
    },
    {
      "epoch": 0.23047650510975295,
      "grad_norm": 0.6085173487663269,
      "learning_rate": 4.616002352034292e-05,
      "loss": 1.2532,
      "step": 73950
    },
    {
      "epoch": 0.23063233777040862,
      "grad_norm": 0.4614095389842987,
      "learning_rate": 4.615742630933199e-05,
      "loss": 1.2951,
      "step": 74000
    },
    {
      "epoch": 0.2307881704310643,
      "grad_norm": 0.6210775971412659,
      "learning_rate": 4.6154829098321064e-05,
      "loss": 1.2307,
      "step": 74050
    },
    {
      "epoch": 0.23094400309171997,
      "grad_norm": 0.48948371410369873,
      "learning_rate": 4.615223188731013e-05,
      "loss": 1.222,
      "step": 74100
    },
    {
      "epoch": 0.23109983575237567,
      "grad_norm": 0.5536307692527771,
      "learning_rate": 4.61496346762992e-05,
      "loss": 1.3688,
      "step": 74150
    },
    {
      "epoch": 0.23125566841303136,
      "grad_norm": 0.582335889339447,
      "learning_rate": 4.614703746528828e-05,
      "loss": 1.28,
      "step": 74200
    },
    {
      "epoch": 0.23141150107368702,
      "grad_norm": 0.7649626135826111,
      "learning_rate": 4.614444025427735e-05,
      "loss": 1.2661,
      "step": 74250
    },
    {
      "epoch": 0.2315673337343427,
      "grad_norm": 0.4314311742782593,
      "learning_rate": 4.614184304326642e-05,
      "loss": 1.2803,
      "step": 74300
    },
    {
      "epoch": 0.2317231663949984,
      "grad_norm": 0.709029495716095,
      "learning_rate": 4.61392458322555e-05,
      "loss": 1.2899,
      "step": 74350
    },
    {
      "epoch": 0.23187899905565407,
      "grad_norm": 0.5977361798286438,
      "learning_rate": 4.6136648621244564e-05,
      "loss": 1.2833,
      "step": 74400
    },
    {
      "epoch": 0.23203483171630976,
      "grad_norm": 0.7451409697532654,
      "learning_rate": 4.613405141023364e-05,
      "loss": 1.3304,
      "step": 74450
    },
    {
      "epoch": 0.23219066437696545,
      "grad_norm": 0.5732812285423279,
      "learning_rate": 4.613145419922271e-05,
      "loss": 1.281,
      "step": 74500
    },
    {
      "epoch": 0.2323464970376211,
      "grad_norm": 0.5096186995506287,
      "learning_rate": 4.612885698821178e-05,
      "loss": 1.3106,
      "step": 74550
    },
    {
      "epoch": 0.2325023296982768,
      "grad_norm": 0.6808014512062073,
      "learning_rate": 4.6126311721421074e-05,
      "loss": 1.3898,
      "step": 74600
    },
    {
      "epoch": 0.2326581623589325,
      "grad_norm": 0.6214515566825867,
      "learning_rate": 4.612371451041014e-05,
      "loss": 1.3078,
      "step": 74650
    },
    {
      "epoch": 0.23281399501958816,
      "grad_norm": 0.5926674604415894,
      "learning_rate": 4.612111729939921e-05,
      "loss": 1.2158,
      "step": 74700
    },
    {
      "epoch": 0.23296982768024385,
      "grad_norm": 0.5435240268707275,
      "learning_rate": 4.611852008838829e-05,
      "loss": 1.2607,
      "step": 74750
    },
    {
      "epoch": 0.23312566034089952,
      "grad_norm": 0.6387382745742798,
      "learning_rate": 4.611592287737736e-05,
      "loss": 1.2726,
      "step": 74800
    },
    {
      "epoch": 0.2332814930015552,
      "grad_norm": 0.6734755039215088,
      "learning_rate": 4.611332566636643e-05,
      "loss": 1.2656,
      "step": 74850
    },
    {
      "epoch": 0.2334373256622109,
      "grad_norm": 0.6931257247924805,
      "learning_rate": 4.61107284553555e-05,
      "loss": 1.2902,
      "step": 74900
    },
    {
      "epoch": 0.23359315832286656,
      "grad_norm": 0.6530857682228088,
      "learning_rate": 4.6108131244344575e-05,
      "loss": 1.3127,
      "step": 74950
    },
    {
      "epoch": 0.23374899098352225,
      "grad_norm": 0.6114856004714966,
      "learning_rate": 4.610553403333365e-05,
      "loss": 1.2923,
      "step": 75000
    },
    {
      "epoch": 0.23390482364417794,
      "grad_norm": 0.5969224572181702,
      "learning_rate": 4.610293682232272e-05,
      "loss": 1.3287,
      "step": 75050
    },
    {
      "epoch": 0.2340606563048336,
      "grad_norm": 0.6447217464447021,
      "learning_rate": 4.610033961131179e-05,
      "loss": 1.278,
      "step": 75100
    },
    {
      "epoch": 0.2342164889654893,
      "grad_norm": 0.6092022061347961,
      "learning_rate": 4.6097742400300865e-05,
      "loss": 1.297,
      "step": 75150
    },
    {
      "epoch": 0.234372321626145,
      "grad_norm": 0.5810270309448242,
      "learning_rate": 4.609514518928994e-05,
      "loss": 1.2922,
      "step": 75200
    },
    {
      "epoch": 0.23452815428680066,
      "grad_norm": 0.7039566040039062,
      "learning_rate": 4.6092547978279e-05,
      "loss": 1.274,
      "step": 75250
    },
    {
      "epoch": 0.23468398694745635,
      "grad_norm": 0.6150133013725281,
      "learning_rate": 4.608995076726808e-05,
      "loss": 1.2687,
      "step": 75300
    },
    {
      "epoch": 0.23483981960811204,
      "grad_norm": 0.629364013671875,
      "learning_rate": 4.608735355625715e-05,
      "loss": 1.329,
      "step": 75350
    },
    {
      "epoch": 0.2349956522687677,
      "grad_norm": 0.47224631905555725,
      "learning_rate": 4.608475634524622e-05,
      "loss": 1.3268,
      "step": 75400
    },
    {
      "epoch": 0.2351514849294234,
      "grad_norm": 0.535197377204895,
      "learning_rate": 4.60821591342353e-05,
      "loss": 1.2754,
      "step": 75450
    },
    {
      "epoch": 0.23530731759007906,
      "grad_norm": 0.5683671236038208,
      "learning_rate": 4.6079561923224366e-05,
      "loss": 1.3026,
      "step": 75500
    },
    {
      "epoch": 0.23546315025073475,
      "grad_norm": 0.5500588417053223,
      "learning_rate": 4.607696471221344e-05,
      "loss": 1.3208,
      "step": 75550
    },
    {
      "epoch": 0.23561898291139044,
      "grad_norm": 0.5358873009681702,
      "learning_rate": 4.607436750120251e-05,
      "loss": 1.3118,
      "step": 75600
    },
    {
      "epoch": 0.2357748155720461,
      "grad_norm": 0.6396069526672363,
      "learning_rate": 4.607177029019158e-05,
      "loss": 1.2985,
      "step": 75650
    },
    {
      "epoch": 0.2359306482327018,
      "grad_norm": 0.6249249577522278,
      "learning_rate": 4.6069173079180656e-05,
      "loss": 1.2933,
      "step": 75700
    },
    {
      "epoch": 0.2360864808933575,
      "grad_norm": 0.6301833987236023,
      "learning_rate": 4.606657586816973e-05,
      "loss": 1.3344,
      "step": 75750
    },
    {
      "epoch": 0.23624231355401315,
      "grad_norm": 0.6244150996208191,
      "learning_rate": 4.6063978657158794e-05,
      "loss": 1.2265,
      "step": 75800
    },
    {
      "epoch": 0.23639814621466884,
      "grad_norm": 0.7013332843780518,
      "learning_rate": 4.606138144614787e-05,
      "loss": 1.2557,
      "step": 75850
    },
    {
      "epoch": 0.23655397887532453,
      "grad_norm": 0.5582396388053894,
      "learning_rate": 4.6058784235136946e-05,
      "loss": 1.3168,
      "step": 75900
    },
    {
      "epoch": 0.2367098115359802,
      "grad_norm": 0.757702112197876,
      "learning_rate": 4.605618702412601e-05,
      "loss": 1.2864,
      "step": 75950
    },
    {
      "epoch": 0.2368656441966359,
      "grad_norm": 0.5092594027519226,
      "learning_rate": 4.605358981311509e-05,
      "loss": 1.3026,
      "step": 76000
    },
    {
      "epoch": 0.23702147685729158,
      "grad_norm": 0.5936386585235596,
      "learning_rate": 4.6050992602104156e-05,
      "loss": 1.2361,
      "step": 76050
    },
    {
      "epoch": 0.23717730951794724,
      "grad_norm": 0.7020496129989624,
      "learning_rate": 4.604839539109323e-05,
      "loss": 1.2419,
      "step": 76100
    },
    {
      "epoch": 0.23733314217860293,
      "grad_norm": 0.5075629353523254,
      "learning_rate": 4.60457981800823e-05,
      "loss": 1.2543,
      "step": 76150
    },
    {
      "epoch": 0.2374889748392586,
      "grad_norm": 0.6495800614356995,
      "learning_rate": 4.6043200969071374e-05,
      "loss": 1.3196,
      "step": 76200
    },
    {
      "epoch": 0.2376448074999143,
      "grad_norm": 0.5867325067520142,
      "learning_rate": 4.6040603758060446e-05,
      "loss": 1.2724,
      "step": 76250
    },
    {
      "epoch": 0.23780064016056998,
      "grad_norm": 0.4871244430541992,
      "learning_rate": 4.603800654704952e-05,
      "loss": 1.2874,
      "step": 76300
    },
    {
      "epoch": 0.23795647282122565,
      "grad_norm": 0.622615396976471,
      "learning_rate": 4.603540933603859e-05,
      "loss": 1.2952,
      "step": 76350
    },
    {
      "epoch": 0.23811230548188134,
      "grad_norm": 0.5532708168029785,
      "learning_rate": 4.6032812125027664e-05,
      "loss": 1.2795,
      "step": 76400
    },
    {
      "epoch": 0.23826813814253703,
      "grad_norm": 0.5001797676086426,
      "learning_rate": 4.6030214914016736e-05,
      "loss": 1.288,
      "step": 76450
    },
    {
      "epoch": 0.2384239708031927,
      "grad_norm": 0.5992991328239441,
      "learning_rate": 4.60276177030058e-05,
      "loss": 1.2567,
      "step": 76500
    },
    {
      "epoch": 0.23857980346384838,
      "grad_norm": 0.6636461019515991,
      "learning_rate": 4.602502049199488e-05,
      "loss": 1.2843,
      "step": 76550
    },
    {
      "epoch": 0.23873563612450407,
      "grad_norm": 0.5596132278442383,
      "learning_rate": 4.6022423280983954e-05,
      "loss": 1.3314,
      "step": 76600
    },
    {
      "epoch": 0.23889146878515974,
      "grad_norm": 0.7188637256622314,
      "learning_rate": 4.601982606997302e-05,
      "loss": 1.3308,
      "step": 76650
    },
    {
      "epoch": 0.23904730144581543,
      "grad_norm": 0.6429800987243652,
      "learning_rate": 4.60172288589621e-05,
      "loss": 1.2938,
      "step": 76700
    },
    {
      "epoch": 0.2392031341064711,
      "grad_norm": 0.6637905836105347,
      "learning_rate": 4.6014631647951165e-05,
      "loss": 1.3301,
      "step": 76750
    },
    {
      "epoch": 0.23935896676712679,
      "grad_norm": 0.5810577273368835,
      "learning_rate": 4.601203443694024e-05,
      "loss": 1.2666,
      "step": 76800
    },
    {
      "epoch": 0.23951479942778248,
      "grad_norm": 0.5039186477661133,
      "learning_rate": 4.600943722592931e-05,
      "loss": 1.2808,
      "step": 76850
    },
    {
      "epoch": 0.23967063208843814,
      "grad_norm": 0.6417985558509827,
      "learning_rate": 4.600684001491838e-05,
      "loss": 1.2673,
      "step": 76900
    },
    {
      "epoch": 0.23982646474909383,
      "grad_norm": 0.6685624718666077,
      "learning_rate": 4.6004242803907455e-05,
      "loss": 1.2239,
      "step": 76950
    },
    {
      "epoch": 0.23998229740974952,
      "grad_norm": 0.667351484298706,
      "learning_rate": 4.600164559289653e-05,
      "loss": 1.1985,
      "step": 77000
    },
    {
      "epoch": 0.2401381300704052,
      "grad_norm": 0.5203225016593933,
      "learning_rate": 4.599904838188559e-05,
      "loss": 1.2769,
      "step": 77050
    },
    {
      "epoch": 0.24029396273106088,
      "grad_norm": 0.4758932888507843,
      "learning_rate": 4.599645117087467e-05,
      "loss": 1.3245,
      "step": 77100
    },
    {
      "epoch": 0.24044979539171657,
      "grad_norm": 0.6451480388641357,
      "learning_rate": 4.5993853959863745e-05,
      "loss": 1.2858,
      "step": 77150
    },
    {
      "epoch": 0.24060562805237223,
      "grad_norm": 0.5684905648231506,
      "learning_rate": 4.599125674885281e-05,
      "loss": 1.289,
      "step": 77200
    },
    {
      "epoch": 0.24076146071302792,
      "grad_norm": 0.6385517716407776,
      "learning_rate": 4.598865953784189e-05,
      "loss": 1.2507,
      "step": 77250
    },
    {
      "epoch": 0.24091729337368362,
      "grad_norm": 0.5189354419708252,
      "learning_rate": 4.5986062326830955e-05,
      "loss": 1.2781,
      "step": 77300
    },
    {
      "epoch": 0.24107312603433928,
      "grad_norm": 0.5467830300331116,
      "learning_rate": 4.598346511582003e-05,
      "loss": 1.291,
      "step": 77350
    },
    {
      "epoch": 0.24122895869499497,
      "grad_norm": 0.5707492232322693,
      "learning_rate": 4.59808679048091e-05,
      "loss": 1.3421,
      "step": 77400
    },
    {
      "epoch": 0.24138479135565064,
      "grad_norm": 0.6309916973114014,
      "learning_rate": 4.597827069379817e-05,
      "loss": 1.2908,
      "step": 77450
    },
    {
      "epoch": 0.24154062401630633,
      "grad_norm": 0.5995920300483704,
      "learning_rate": 4.5975673482787245e-05,
      "loss": 1.317,
      "step": 77500
    },
    {
      "epoch": 0.24169645667696202,
      "grad_norm": 0.5592604875564575,
      "learning_rate": 4.597307627177632e-05,
      "loss": 1.3039,
      "step": 77550
    },
    {
      "epoch": 0.24185228933761768,
      "grad_norm": 0.6691204309463501,
      "learning_rate": 4.597047906076539e-05,
      "loss": 1.3144,
      "step": 77600
    },
    {
      "epoch": 0.24200812199827337,
      "grad_norm": 0.688119649887085,
      "learning_rate": 4.596788184975446e-05,
      "loss": 1.2719,
      "step": 77650
    },
    {
      "epoch": 0.24216395465892906,
      "grad_norm": 0.516312301158905,
      "learning_rate": 4.5965284638743535e-05,
      "loss": 1.2799,
      "step": 77700
    },
    {
      "epoch": 0.24231978731958473,
      "grad_norm": 0.5335947871208191,
      "learning_rate": 4.59626874277326e-05,
      "loss": 1.2919,
      "step": 77750
    },
    {
      "epoch": 0.24247561998024042,
      "grad_norm": 0.6216434836387634,
      "learning_rate": 4.596009021672168e-05,
      "loss": 1.3169,
      "step": 77800
    },
    {
      "epoch": 0.2426314526408961,
      "grad_norm": 0.4849635660648346,
      "learning_rate": 4.595749300571075e-05,
      "loss": 1.3097,
      "step": 77850
    },
    {
      "epoch": 0.24278728530155178,
      "grad_norm": 0.6191376447677612,
      "learning_rate": 4.595489579469982e-05,
      "loss": 1.2668,
      "step": 77900
    },
    {
      "epoch": 0.24294311796220747,
      "grad_norm": 0.5080739259719849,
      "learning_rate": 4.59522985836889e-05,
      "loss": 1.2906,
      "step": 77950
    },
    {
      "epoch": 0.24309895062286316,
      "grad_norm": 0.6187155246734619,
      "learning_rate": 4.5949701372677964e-05,
      "loss": 1.2395,
      "step": 78000
    },
    {
      "epoch": 0.24325478328351882,
      "grad_norm": 0.5383922457695007,
      "learning_rate": 4.5947104161667036e-05,
      "loss": 1.2656,
      "step": 78050
    },
    {
      "epoch": 0.2434106159441745,
      "grad_norm": 0.5913935303688049,
      "learning_rate": 4.594450695065611e-05,
      "loss": 1.2429,
      "step": 78100
    },
    {
      "epoch": 0.24356644860483018,
      "grad_norm": 0.5097468495368958,
      "learning_rate": 4.594190973964518e-05,
      "loss": 1.3542,
      "step": 78150
    },
    {
      "epoch": 0.24372228126548587,
      "grad_norm": 0.5657509565353394,
      "learning_rate": 4.5939312528634254e-05,
      "loss": 1.2617,
      "step": 78200
    },
    {
      "epoch": 0.24387811392614156,
      "grad_norm": 0.6034310460090637,
      "learning_rate": 4.5936715317623326e-05,
      "loss": 1.2603,
      "step": 78250
    },
    {
      "epoch": 0.24403394658679722,
      "grad_norm": 0.6543072462081909,
      "learning_rate": 4.593411810661239e-05,
      "loss": 1.2793,
      "step": 78300
    },
    {
      "epoch": 0.24418977924745291,
      "grad_norm": 0.6545208692550659,
      "learning_rate": 4.593152089560147e-05,
      "loss": 1.23,
      "step": 78350
    },
    {
      "epoch": 0.2443456119081086,
      "grad_norm": 0.5291104912757874,
      "learning_rate": 4.5928923684590544e-05,
      "loss": 1.3234,
      "step": 78400
    },
    {
      "epoch": 0.24450144456876427,
      "grad_norm": 0.5951617360115051,
      "learning_rate": 4.592632647357961e-05,
      "loss": 1.3096,
      "step": 78450
    },
    {
      "epoch": 0.24465727722941996,
      "grad_norm": 0.6951316595077515,
      "learning_rate": 4.592372926256869e-05,
      "loss": 1.3088,
      "step": 78500
    },
    {
      "epoch": 0.24481310989007565,
      "grad_norm": 0.5203333497047424,
      "learning_rate": 4.592113205155776e-05,
      "loss": 1.2912,
      "step": 78550
    },
    {
      "epoch": 0.24496894255073132,
      "grad_norm": 0.7288040518760681,
      "learning_rate": 4.591853484054683e-05,
      "loss": 1.286,
      "step": 78600
    },
    {
      "epoch": 0.245124775211387,
      "grad_norm": 0.5122436285018921,
      "learning_rate": 4.59159376295359e-05,
      "loss": 1.3429,
      "step": 78650
    },
    {
      "epoch": 0.24528060787204267,
      "grad_norm": 0.5654112696647644,
      "learning_rate": 4.591334041852497e-05,
      "loss": 1.2697,
      "step": 78700
    },
    {
      "epoch": 0.24543644053269836,
      "grad_norm": 0.7556186318397522,
      "learning_rate": 4.5910743207514044e-05,
      "loss": 1.3129,
      "step": 78750
    },
    {
      "epoch": 0.24559227319335405,
      "grad_norm": 0.4935595691204071,
      "learning_rate": 4.5908197940723337e-05,
      "loss": 1.315,
      "step": 78800
    },
    {
      "epoch": 0.24574810585400972,
      "grad_norm": 0.6311109066009521,
      "learning_rate": 4.590560072971241e-05,
      "loss": 1.2777,
      "step": 78850
    },
    {
      "epoch": 0.2459039385146654,
      "grad_norm": 0.6602144241333008,
      "learning_rate": 4.590300351870148e-05,
      "loss": 1.2048,
      "step": 78900
    },
    {
      "epoch": 0.2460597711753211,
      "grad_norm": 0.6622995138168335,
      "learning_rate": 4.5900406307690554e-05,
      "loss": 1.2901,
      "step": 78950
    },
    {
      "epoch": 0.24621560383597677,
      "grad_norm": 0.6864298582077026,
      "learning_rate": 4.589780909667962e-05,
      "loss": 1.3042,
      "step": 79000
    },
    {
      "epoch": 0.24637143649663246,
      "grad_norm": 0.7986307144165039,
      "learning_rate": 4.589521188566869e-05,
      "loss": 1.3055,
      "step": 79050
    },
    {
      "epoch": 0.24652726915728815,
      "grad_norm": 0.5294826626777649,
      "learning_rate": 4.589261467465777e-05,
      "loss": 1.2883,
      "step": 79100
    },
    {
      "epoch": 0.2466831018179438,
      "grad_norm": 0.7019808888435364,
      "learning_rate": 4.589001746364684e-05,
      "loss": 1.3177,
      "step": 79150
    },
    {
      "epoch": 0.2468389344785995,
      "grad_norm": 0.5301319360733032,
      "learning_rate": 4.588742025263591e-05,
      "loss": 1.2697,
      "step": 79200
    },
    {
      "epoch": 0.2469947671392552,
      "grad_norm": 0.6391617655754089,
      "learning_rate": 4.588482304162499e-05,
      "loss": 1.2749,
      "step": 79250
    },
    {
      "epoch": 0.24715059979991086,
      "grad_norm": 0.5405246615409851,
      "learning_rate": 4.5882225830614055e-05,
      "loss": 1.3291,
      "step": 79300
    },
    {
      "epoch": 0.24730643246056655,
      "grad_norm": 0.5661253929138184,
      "learning_rate": 4.587962861960313e-05,
      "loss": 1.3648,
      "step": 79350
    },
    {
      "epoch": 0.2474622651212222,
      "grad_norm": 0.5902490615844727,
      "learning_rate": 4.58770314085922e-05,
      "loss": 1.3006,
      "step": 79400
    },
    {
      "epoch": 0.2476180977818779,
      "grad_norm": 0.6764198541641235,
      "learning_rate": 4.587443419758127e-05,
      "loss": 1.277,
      "step": 79450
    },
    {
      "epoch": 0.2477739304425336,
      "grad_norm": 0.5784648656845093,
      "learning_rate": 4.5871836986570345e-05,
      "loss": 1.3407,
      "step": 79500
    },
    {
      "epoch": 0.24792976310318926,
      "grad_norm": 0.5986754298210144,
      "learning_rate": 4.586923977555941e-05,
      "loss": 1.2893,
      "step": 79550
    },
    {
      "epoch": 0.24808559576384495,
      "grad_norm": 0.6897023916244507,
      "learning_rate": 4.586664256454849e-05,
      "loss": 1.2677,
      "step": 79600
    },
    {
      "epoch": 0.24824142842450064,
      "grad_norm": 0.6593602299690247,
      "learning_rate": 4.586404535353756e-05,
      "loss": 1.3311,
      "step": 79650
    },
    {
      "epoch": 0.2483972610851563,
      "grad_norm": 0.6085437536239624,
      "learning_rate": 4.586144814252663e-05,
      "loss": 1.2918,
      "step": 79700
    },
    {
      "epoch": 0.248553093745812,
      "grad_norm": 0.5363956689834595,
      "learning_rate": 4.58588509315157e-05,
      "loss": 1.2475,
      "step": 79750
    },
    {
      "epoch": 0.2487089264064677,
      "grad_norm": 0.7886404395103455,
      "learning_rate": 4.585625372050478e-05,
      "loss": 1.2556,
      "step": 79800
    },
    {
      "epoch": 0.24886475906712335,
      "grad_norm": 0.5461465120315552,
      "learning_rate": 4.5853656509493846e-05,
      "loss": 1.307,
      "step": 79850
    },
    {
      "epoch": 0.24902059172777904,
      "grad_norm": 0.4164615869522095,
      "learning_rate": 4.585105929848292e-05,
      "loss": 1.3048,
      "step": 79900
    },
    {
      "epoch": 0.24917642438843474,
      "grad_norm": 0.7136761546134949,
      "learning_rate": 4.584846208747199e-05,
      "loss": 1.2971,
      "step": 79950
    },
    {
      "epoch": 0.2493322570490904,
      "grad_norm": 0.7377680540084839,
      "learning_rate": 4.584586487646106e-05,
      "loss": 1.2582,
      "step": 80000
    },
    {
      "epoch": 0.2494880897097461,
      "grad_norm": 0.5250226259231567,
      "learning_rate": 4.5843267665450136e-05,
      "loss": 1.2539,
      "step": 80050
    },
    {
      "epoch": 0.24964392237040176,
      "grad_norm": 0.5304407477378845,
      "learning_rate": 4.584067045443921e-05,
      "loss": 1.2648,
      "step": 80100
    },
    {
      "epoch": 0.24979975503105745,
      "grad_norm": 0.6282007098197937,
      "learning_rate": 4.583807324342828e-05,
      "loss": 1.2791,
      "step": 80150
    },
    {
      "epoch": 0.24995558769171314,
      "grad_norm": 0.547230064868927,
      "learning_rate": 4.583547603241735e-05,
      "loss": 1.3136,
      "step": 80200
    },
    {
      "epoch": 0.25011142035236883,
      "grad_norm": 0.5064212679862976,
      "learning_rate": 4.583287882140642e-05,
      "loss": 1.2824,
      "step": 80250
    },
    {
      "epoch": 0.2502672530130245,
      "grad_norm": 0.7569674849510193,
      "learning_rate": 4.583028161039549e-05,
      "loss": 1.241,
      "step": 80300
    },
    {
      "epoch": 0.25042308567368016,
      "grad_norm": 0.583357572555542,
      "learning_rate": 4.582768439938457e-05,
      "loss": 1.3504,
      "step": 80350
    },
    {
      "epoch": 0.2505789183343359,
      "grad_norm": 0.5195572972297668,
      "learning_rate": 4.5825087188373636e-05,
      "loss": 1.3046,
      "step": 80400
    },
    {
      "epoch": 0.25073475099499154,
      "grad_norm": 0.4808945953845978,
      "learning_rate": 4.582248997736271e-05,
      "loss": 1.2959,
      "step": 80450
    },
    {
      "epoch": 0.2508905836556472,
      "grad_norm": 0.47794362902641296,
      "learning_rate": 4.581989276635179e-05,
      "loss": 1.2789,
      "step": 80500
    },
    {
      "epoch": 0.2510464163163029,
      "grad_norm": 0.5360226035118103,
      "learning_rate": 4.5817295555340854e-05,
      "loss": 1.2395,
      "step": 80550
    },
    {
      "epoch": 0.2512022489769586,
      "grad_norm": 0.6140544414520264,
      "learning_rate": 4.5814698344329926e-05,
      "loss": 1.2613,
      "step": 80600
    },
    {
      "epoch": 0.25135808163761425,
      "grad_norm": 0.5352336168289185,
      "learning_rate": 4.5812101133319e-05,
      "loss": 1.2774,
      "step": 80650
    },
    {
      "epoch": 0.25151391429826997,
      "grad_norm": 0.6317477226257324,
      "learning_rate": 4.580950392230807e-05,
      "loss": 1.2928,
      "step": 80700
    },
    {
      "epoch": 0.25166974695892563,
      "grad_norm": 0.5884549617767334,
      "learning_rate": 4.5806906711297144e-05,
      "loss": 1.2475,
      "step": 80750
    },
    {
      "epoch": 0.2518255796195813,
      "grad_norm": 0.5632742047309875,
      "learning_rate": 4.5804309500286216e-05,
      "loss": 1.307,
      "step": 80800
    },
    {
      "epoch": 0.251981412280237,
      "grad_norm": 0.7146185636520386,
      "learning_rate": 4.580171228927529e-05,
      "loss": 1.313,
      "step": 80850
    },
    {
      "epoch": 0.2521372449408927,
      "grad_norm": 0.4831809103488922,
      "learning_rate": 4.579911507826436e-05,
      "loss": 1.314,
      "step": 80900
    },
    {
      "epoch": 0.25229307760154834,
      "grad_norm": 0.6504733562469482,
      "learning_rate": 4.579651786725343e-05,
      "loss": 1.2716,
      "step": 80950
    },
    {
      "epoch": 0.252448910262204,
      "grad_norm": 0.5283527374267578,
      "learning_rate": 4.57939206562425e-05,
      "loss": 1.2766,
      "step": 81000
    },
    {
      "epoch": 0.2526047429228597,
      "grad_norm": 0.5459429025650024,
      "learning_rate": 4.579132344523158e-05,
      "loss": 1.3062,
      "step": 81050
    },
    {
      "epoch": 0.2527605755835154,
      "grad_norm": 0.7217633128166199,
      "learning_rate": 4.5788726234220645e-05,
      "loss": 1.2777,
      "step": 81100
    },
    {
      "epoch": 0.25291640824417105,
      "grad_norm": 0.623176634311676,
      "learning_rate": 4.578612902320972e-05,
      "loss": 1.3402,
      "step": 81150
    },
    {
      "epoch": 0.2530722409048268,
      "grad_norm": 0.5629448294639587,
      "learning_rate": 4.5783531812198796e-05,
      "loss": 1.2865,
      "step": 81200
    },
    {
      "epoch": 0.25322807356548244,
      "grad_norm": 0.5487751960754395,
      "learning_rate": 4.578093460118786e-05,
      "loss": 1.2697,
      "step": 81250
    },
    {
      "epoch": 0.2533839062261381,
      "grad_norm": 0.7594932317733765,
      "learning_rate": 4.5778337390176935e-05,
      "loss": 1.3241,
      "step": 81300
    },
    {
      "epoch": 0.2535397388867938,
      "grad_norm": 0.6269635558128357,
      "learning_rate": 4.577574017916601e-05,
      "loss": 1.2756,
      "step": 81350
    },
    {
      "epoch": 0.2536955715474495,
      "grad_norm": 0.5294318795204163,
      "learning_rate": 4.577319491237529e-05,
      "loss": 1.2756,
      "step": 81400
    },
    {
      "epoch": 0.25385140420810515,
      "grad_norm": 0.583104133605957,
      "learning_rate": 4.577059770136437e-05,
      "loss": 1.2803,
      "step": 81450
    },
    {
      "epoch": 0.25400723686876087,
      "grad_norm": 0.6475543975830078,
      "learning_rate": 4.5768000490353444e-05,
      "loss": 1.279,
      "step": 81500
    },
    {
      "epoch": 0.25416306952941653,
      "grad_norm": 0.5847091674804688,
      "learning_rate": 4.576540327934251e-05,
      "loss": 1.2619,
      "step": 81550
    },
    {
      "epoch": 0.2543189021900722,
      "grad_norm": 0.49943676590919495,
      "learning_rate": 4.576280606833159e-05,
      "loss": 1.252,
      "step": 81600
    },
    {
      "epoch": 0.2544747348507279,
      "grad_norm": 0.7525278329849243,
      "learning_rate": 4.5760208857320655e-05,
      "loss": 1.2604,
      "step": 81650
    },
    {
      "epoch": 0.2546305675113836,
      "grad_norm": 0.603931188583374,
      "learning_rate": 4.575761164630973e-05,
      "loss": 1.2275,
      "step": 81700
    },
    {
      "epoch": 0.25478640017203924,
      "grad_norm": 0.5545457005500793,
      "learning_rate": 4.57550144352988e-05,
      "loss": 1.2531,
      "step": 81750
    },
    {
      "epoch": 0.25494223283269496,
      "grad_norm": 0.5500612854957581,
      "learning_rate": 4.575241722428787e-05,
      "loss": 1.2754,
      "step": 81800
    },
    {
      "epoch": 0.2550980654933506,
      "grad_norm": 0.4567192494869232,
      "learning_rate": 4.5749820013276945e-05,
      "loss": 1.2978,
      "step": 81850
    },
    {
      "epoch": 0.2552538981540063,
      "grad_norm": 0.6230711936950684,
      "learning_rate": 4.574722280226602e-05,
      "loss": 1.3388,
      "step": 81900
    },
    {
      "epoch": 0.255409730814662,
      "grad_norm": 0.6170932054519653,
      "learning_rate": 4.574462559125509e-05,
      "loss": 1.3183,
      "step": 81950
    },
    {
      "epoch": 0.25556556347531767,
      "grad_norm": 0.5693236589431763,
      "learning_rate": 4.574202838024416e-05,
      "loss": 1.3112,
      "step": 82000
    },
    {
      "epoch": 0.25572139613597333,
      "grad_norm": 0.5062043070793152,
      "learning_rate": 4.5739431169233235e-05,
      "loss": 1.2639,
      "step": 82050
    },
    {
      "epoch": 0.25587722879662905,
      "grad_norm": 0.4948459565639496,
      "learning_rate": 4.57368339582223e-05,
      "loss": 1.2675,
      "step": 82100
    },
    {
      "epoch": 0.2560330614572847,
      "grad_norm": 0.5767163038253784,
      "learning_rate": 4.573423674721138e-05,
      "loss": 1.2493,
      "step": 82150
    },
    {
      "epoch": 0.2561888941179404,
      "grad_norm": 0.5225650668144226,
      "learning_rate": 4.5731639536200446e-05,
      "loss": 1.3188,
      "step": 82200
    },
    {
      "epoch": 0.2563447267785961,
      "grad_norm": 0.47443124651908875,
      "learning_rate": 4.572904232518952e-05,
      "loss": 1.2612,
      "step": 82250
    },
    {
      "epoch": 0.25650055943925176,
      "grad_norm": 0.49568450450897217,
      "learning_rate": 4.572644511417859e-05,
      "loss": 1.3361,
      "step": 82300
    },
    {
      "epoch": 0.2566563920999074,
      "grad_norm": 0.5769704580307007,
      "learning_rate": 4.572384790316766e-05,
      "loss": 1.2962,
      "step": 82350
    },
    {
      "epoch": 0.2568122247605631,
      "grad_norm": 0.6007116436958313,
      "learning_rate": 4.5721250692156736e-05,
      "loss": 1.3261,
      "step": 82400
    },
    {
      "epoch": 0.2569680574212188,
      "grad_norm": 0.5866349935531616,
      "learning_rate": 4.571865348114581e-05,
      "loss": 1.2787,
      "step": 82450
    },
    {
      "epoch": 0.2571238900818745,
      "grad_norm": 0.5911731123924255,
      "learning_rate": 4.571605627013488e-05,
      "loss": 1.2997,
      "step": 82500
    },
    {
      "epoch": 0.25727972274253014,
      "grad_norm": 0.5986641049385071,
      "learning_rate": 4.571345905912395e-05,
      "loss": 1.2849,
      "step": 82550
    },
    {
      "epoch": 0.25743555540318586,
      "grad_norm": 0.6680855751037598,
      "learning_rate": 4.5710861848113026e-05,
      "loss": 1.2923,
      "step": 82600
    },
    {
      "epoch": 0.2575913880638415,
      "grad_norm": 0.6190816760063171,
      "learning_rate": 4.570826463710209e-05,
      "loss": 1.3252,
      "step": 82650
    },
    {
      "epoch": 0.2577472207244972,
      "grad_norm": 0.6487060785293579,
      "learning_rate": 4.570566742609117e-05,
      "loss": 1.2871,
      "step": 82700
    },
    {
      "epoch": 0.2579030533851529,
      "grad_norm": 0.6265318989753723,
      "learning_rate": 4.570307021508024e-05,
      "loss": 1.2531,
      "step": 82750
    },
    {
      "epoch": 0.25805888604580857,
      "grad_norm": 0.430514931678772,
      "learning_rate": 4.570047300406931e-05,
      "loss": 1.3373,
      "step": 82800
    },
    {
      "epoch": 0.25821471870646423,
      "grad_norm": 0.5861346125602722,
      "learning_rate": 4.569787579305839e-05,
      "loss": 1.3209,
      "step": 82850
    },
    {
      "epoch": 0.25837055136711995,
      "grad_norm": 0.565089762210846,
      "learning_rate": 4.5695278582047454e-05,
      "loss": 1.2529,
      "step": 82900
    },
    {
      "epoch": 0.2585263840277756,
      "grad_norm": 0.5519312620162964,
      "learning_rate": 4.5692681371036527e-05,
      "loss": 1.291,
      "step": 82950
    },
    {
      "epoch": 0.2586822166884313,
      "grad_norm": 0.669353187084198,
      "learning_rate": 4.56900841600256e-05,
      "loss": 1.2884,
      "step": 83000
    },
    {
      "epoch": 0.258838049349087,
      "grad_norm": 0.6625820994377136,
      "learning_rate": 4.568748694901467e-05,
      "loss": 1.2104,
      "step": 83050
    },
    {
      "epoch": 0.25899388200974266,
      "grad_norm": 0.6424161791801453,
      "learning_rate": 4.5684889738003744e-05,
      "loss": 1.2812,
      "step": 83100
    },
    {
      "epoch": 0.2591497146703983,
      "grad_norm": 0.5512340664863586,
      "learning_rate": 4.5682344471213036e-05,
      "loss": 1.308,
      "step": 83150
    },
    {
      "epoch": 0.25930554733105404,
      "grad_norm": 0.6497212052345276,
      "learning_rate": 4.56797472602021e-05,
      "loss": 1.2975,
      "step": 83200
    },
    {
      "epoch": 0.2594613799917097,
      "grad_norm": 0.45659691095352173,
      "learning_rate": 4.567715004919118e-05,
      "loss": 1.2867,
      "step": 83250
    },
    {
      "epoch": 0.25961721265236537,
      "grad_norm": 0.5545891523361206,
      "learning_rate": 4.5674552838180254e-05,
      "loss": 1.3126,
      "step": 83300
    },
    {
      "epoch": 0.2597730453130211,
      "grad_norm": 0.5637006163597107,
      "learning_rate": 4.567195562716932e-05,
      "loss": 1.284,
      "step": 83350
    },
    {
      "epoch": 0.25992887797367675,
      "grad_norm": 0.6540057063102722,
      "learning_rate": 4.566935841615839e-05,
      "loss": 1.2766,
      "step": 83400
    },
    {
      "epoch": 0.2600847106343324,
      "grad_norm": 0.494385689496994,
      "learning_rate": 4.566676120514747e-05,
      "loss": 1.2443,
      "step": 83450
    },
    {
      "epoch": 0.26024054329498814,
      "grad_norm": 0.5104668140411377,
      "learning_rate": 4.566416399413654e-05,
      "loss": 1.3449,
      "step": 83500
    },
    {
      "epoch": 0.2603963759556438,
      "grad_norm": 0.5445752143859863,
      "learning_rate": 4.566156678312561e-05,
      "loss": 1.2894,
      "step": 83550
    },
    {
      "epoch": 0.26055220861629946,
      "grad_norm": 0.49044206738471985,
      "learning_rate": 4.565896957211468e-05,
      "loss": 1.2392,
      "step": 83600
    },
    {
      "epoch": 0.2607080412769551,
      "grad_norm": 0.8458634614944458,
      "learning_rate": 4.5656372361103754e-05,
      "loss": 1.2883,
      "step": 83650
    },
    {
      "epoch": 0.26086387393761085,
      "grad_norm": 0.4892648458480835,
      "learning_rate": 4.565377515009283e-05,
      "loss": 1.279,
      "step": 83700
    },
    {
      "epoch": 0.2610197065982665,
      "grad_norm": 0.5816143155097961,
      "learning_rate": 4.56511779390819e-05,
      "loss": 1.3127,
      "step": 83750
    },
    {
      "epoch": 0.2611755392589222,
      "grad_norm": 0.5470326542854309,
      "learning_rate": 4.564858072807097e-05,
      "loss": 1.2608,
      "step": 83800
    },
    {
      "epoch": 0.2613313719195779,
      "grad_norm": 0.6689218282699585,
      "learning_rate": 4.5645983517060044e-05,
      "loss": 1.3129,
      "step": 83850
    },
    {
      "epoch": 0.26148720458023356,
      "grad_norm": 0.5353822708129883,
      "learning_rate": 4.564338630604911e-05,
      "loss": 1.2631,
      "step": 83900
    },
    {
      "epoch": 0.2616430372408892,
      "grad_norm": 0.46757256984710693,
      "learning_rate": 4.564078909503819e-05,
      "loss": 1.2937,
      "step": 83950
    },
    {
      "epoch": 0.26179886990154494,
      "grad_norm": 0.6434746980667114,
      "learning_rate": 4.563819188402726e-05,
      "loss": 1.3304,
      "step": 84000
    },
    {
      "epoch": 0.2619547025622006,
      "grad_norm": 0.6625784039497375,
      "learning_rate": 4.563559467301633e-05,
      "loss": 1.2732,
      "step": 84050
    },
    {
      "epoch": 0.26211053522285627,
      "grad_norm": 0.5427460074424744,
      "learning_rate": 4.56329974620054e-05,
      "loss": 1.2766,
      "step": 84100
    },
    {
      "epoch": 0.262266367883512,
      "grad_norm": 0.7200298309326172,
      "learning_rate": 4.563040025099448e-05,
      "loss": 1.2792,
      "step": 84150
    },
    {
      "epoch": 0.26242220054416765,
      "grad_norm": 0.6540895700454712,
      "learning_rate": 4.5627803039983545e-05,
      "loss": 1.2695,
      "step": 84200
    },
    {
      "epoch": 0.2625780332048233,
      "grad_norm": 0.6394543051719666,
      "learning_rate": 4.562520582897262e-05,
      "loss": 1.246,
      "step": 84250
    },
    {
      "epoch": 0.26273386586547903,
      "grad_norm": 0.5494763255119324,
      "learning_rate": 4.562260861796169e-05,
      "loss": 1.2869,
      "step": 84300
    },
    {
      "epoch": 0.2628896985261347,
      "grad_norm": 0.715196967124939,
      "learning_rate": 4.562001140695076e-05,
      "loss": 1.3124,
      "step": 84350
    },
    {
      "epoch": 0.26304553118679036,
      "grad_norm": 0.5641338229179382,
      "learning_rate": 4.5617414195939835e-05,
      "loss": 1.2437,
      "step": 84400
    },
    {
      "epoch": 0.2632013638474461,
      "grad_norm": 0.7060596346855164,
      "learning_rate": 4.56148169849289e-05,
      "loss": 1.3044,
      "step": 84450
    },
    {
      "epoch": 0.26335719650810174,
      "grad_norm": 0.6842904090881348,
      "learning_rate": 4.561221977391798e-05,
      "loss": 1.2462,
      "step": 84500
    },
    {
      "epoch": 0.2635130291687574,
      "grad_norm": 0.7185524106025696,
      "learning_rate": 4.560962256290705e-05,
      "loss": 1.3179,
      "step": 84550
    },
    {
      "epoch": 0.2636688618294131,
      "grad_norm": 0.5901867747306824,
      "learning_rate": 4.560702535189612e-05,
      "loss": 1.3145,
      "step": 84600
    },
    {
      "epoch": 0.2638246944900688,
      "grad_norm": 0.5830810070037842,
      "learning_rate": 4.560442814088519e-05,
      "loss": 1.2568,
      "step": 84650
    },
    {
      "epoch": 0.26398052715072445,
      "grad_norm": 0.5661691427230835,
      "learning_rate": 4.560183092987427e-05,
      "loss": 1.2716,
      "step": 84700
    },
    {
      "epoch": 0.2641363598113802,
      "grad_norm": 0.6351439356803894,
      "learning_rate": 4.5599233718863336e-05,
      "loss": 1.3316,
      "step": 84750
    },
    {
      "epoch": 0.26429219247203584,
      "grad_norm": 0.661577582359314,
      "learning_rate": 4.559663650785241e-05,
      "loss": 1.3035,
      "step": 84800
    },
    {
      "epoch": 0.2644480251326915,
      "grad_norm": 0.6967508792877197,
      "learning_rate": 4.559403929684148e-05,
      "loss": 1.3075,
      "step": 84850
    },
    {
      "epoch": 0.2646038577933472,
      "grad_norm": 0.5806007385253906,
      "learning_rate": 4.5591442085830553e-05,
      "loss": 1.286,
      "step": 84900
    },
    {
      "epoch": 0.2647596904540029,
      "grad_norm": 0.6083700656890869,
      "learning_rate": 4.5588844874819626e-05,
      "loss": 1.29,
      "step": 84950
    },
    {
      "epoch": 0.26491552311465855,
      "grad_norm": 0.5436133146286011,
      "learning_rate": 4.55862476638087e-05,
      "loss": 1.3154,
      "step": 85000
    },
    {
      "epoch": 0.2650713557753142,
      "grad_norm": 0.5759167075157166,
      "learning_rate": 4.558365045279777e-05,
      "loss": 1.341,
      "step": 85050
    },
    {
      "epoch": 0.26522718843596993,
      "grad_norm": 0.5384543538093567,
      "learning_rate": 4.5581053241786843e-05,
      "loss": 1.3103,
      "step": 85100
    },
    {
      "epoch": 0.2653830210966256,
      "grad_norm": 0.6477901935577393,
      "learning_rate": 4.557845603077591e-05,
      "loss": 1.262,
      "step": 85150
    },
    {
      "epoch": 0.26553885375728126,
      "grad_norm": 0.5784161686897278,
      "learning_rate": 4.557585881976499e-05,
      "loss": 1.2156,
      "step": 85200
    },
    {
      "epoch": 0.265694686417937,
      "grad_norm": 0.651058554649353,
      "learning_rate": 4.557326160875406e-05,
      "loss": 1.2931,
      "step": 85250
    },
    {
      "epoch": 0.26585051907859264,
      "grad_norm": 0.5347767472267151,
      "learning_rate": 4.557066439774313e-05,
      "loss": 1.2404,
      "step": 85300
    },
    {
      "epoch": 0.2660063517392483,
      "grad_norm": 0.5931082963943481,
      "learning_rate": 4.55680671867322e-05,
      "loss": 1.2691,
      "step": 85350
    },
    {
      "epoch": 0.266162184399904,
      "grad_norm": 0.7155861854553223,
      "learning_rate": 4.556546997572128e-05,
      "loss": 1.2653,
      "step": 85400
    },
    {
      "epoch": 0.2663180170605597,
      "grad_norm": 0.6382118463516235,
      "learning_rate": 4.5562872764710344e-05,
      "loss": 1.2407,
      "step": 85450
    },
    {
      "epoch": 0.26647384972121535,
      "grad_norm": 0.7435847520828247,
      "learning_rate": 4.556027555369942e-05,
      "loss": 1.2917,
      "step": 85500
    },
    {
      "epoch": 0.26662968238187107,
      "grad_norm": 0.48578453063964844,
      "learning_rate": 4.555767834268849e-05,
      "loss": 1.2972,
      "step": 85550
    },
    {
      "epoch": 0.26678551504252673,
      "grad_norm": 0.8409299254417419,
      "learning_rate": 4.555508113167756e-05,
      "loss": 1.3074,
      "step": 85600
    },
    {
      "epoch": 0.2669413477031824,
      "grad_norm": 0.4607927203178406,
      "learning_rate": 4.5552483920666634e-05,
      "loss": 1.24,
      "step": 85650
    },
    {
      "epoch": 0.2670971803638381,
      "grad_norm": 0.5138390064239502,
      "learning_rate": 4.554988670965571e-05,
      "loss": 1.3042,
      "step": 85700
    },
    {
      "epoch": 0.2672530130244938,
      "grad_norm": 0.5464673638343811,
      "learning_rate": 4.554728949864478e-05,
      "loss": 1.317,
      "step": 85750
    },
    {
      "epoch": 0.26740884568514944,
      "grad_norm": 0.5347546935081482,
      "learning_rate": 4.554469228763385e-05,
      "loss": 1.305,
      "step": 85800
    },
    {
      "epoch": 0.26756467834580516,
      "grad_norm": 0.6451399326324463,
      "learning_rate": 4.554209507662292e-05,
      "loss": 1.2723,
      "step": 85850
    },
    {
      "epoch": 0.2677205110064608,
      "grad_norm": 0.5333268046379089,
      "learning_rate": 4.553949786561199e-05,
      "loss": 1.2767,
      "step": 85900
    },
    {
      "epoch": 0.2678763436671165,
      "grad_norm": 0.6594564914703369,
      "learning_rate": 4.553690065460107e-05,
      "loss": 1.2895,
      "step": 85950
    },
    {
      "epoch": 0.2680321763277722,
      "grad_norm": 0.5379328727722168,
      "learning_rate": 4.5534303443590135e-05,
      "loss": 1.2431,
      "step": 86000
    },
    {
      "epoch": 0.2681880089884279,
      "grad_norm": 0.6412636637687683,
      "learning_rate": 4.553170623257921e-05,
      "loss": 1.2808,
      "step": 86050
    },
    {
      "epoch": 0.26834384164908354,
      "grad_norm": 0.5537891387939453,
      "learning_rate": 4.552910902156829e-05,
      "loss": 1.2861,
      "step": 86100
    },
    {
      "epoch": 0.26849967430973926,
      "grad_norm": 0.6035051345825195,
      "learning_rate": 4.552651181055735e-05,
      "loss": 1.328,
      "step": 86150
    },
    {
      "epoch": 0.2686555069703949,
      "grad_norm": 0.6944226622581482,
      "learning_rate": 4.5523914599546425e-05,
      "loss": 1.26,
      "step": 86200
    },
    {
      "epoch": 0.2688113396310506,
      "grad_norm": 0.6961657404899597,
      "learning_rate": 4.55213173885355e-05,
      "loss": 1.3308,
      "step": 86250
    },
    {
      "epoch": 0.26896717229170625,
      "grad_norm": 0.605569064617157,
      "learning_rate": 4.551872017752457e-05,
      "loss": 1.2581,
      "step": 86300
    },
    {
      "epoch": 0.26912300495236197,
      "grad_norm": 0.580876350402832,
      "learning_rate": 4.551612296651364e-05,
      "loss": 1.2967,
      "step": 86350
    },
    {
      "epoch": 0.26927883761301763,
      "grad_norm": 0.5834437608718872,
      "learning_rate": 4.5513525755502715e-05,
      "loss": 1.3805,
      "step": 86400
    },
    {
      "epoch": 0.2694346702736733,
      "grad_norm": 0.8851470351219177,
      "learning_rate": 4.551092854449179e-05,
      "loss": 1.2503,
      "step": 86450
    },
    {
      "epoch": 0.269590502934329,
      "grad_norm": 0.5412924885749817,
      "learning_rate": 4.550833133348086e-05,
      "loss": 1.3416,
      "step": 86500
    },
    {
      "epoch": 0.2697463355949847,
      "grad_norm": 0.5711610913276672,
      "learning_rate": 4.5505734122469926e-05,
      "loss": 1.2258,
      "step": 86550
    },
    {
      "epoch": 0.26990216825564034,
      "grad_norm": 0.6921356320381165,
      "learning_rate": 4.5503136911459e-05,
      "loss": 1.3367,
      "step": 86600
    },
    {
      "epoch": 0.27005800091629606,
      "grad_norm": 0.5513818264007568,
      "learning_rate": 4.550053970044808e-05,
      "loss": 1.2639,
      "step": 86650
    },
    {
      "epoch": 0.2702138335769517,
      "grad_norm": 0.5155803561210632,
      "learning_rate": 4.549794248943714e-05,
      "loss": 1.2101,
      "step": 86700
    },
    {
      "epoch": 0.2703696662376074,
      "grad_norm": 0.7908751368522644,
      "learning_rate": 4.5495397222646435e-05,
      "loss": 1.299,
      "step": 86750
    },
    {
      "epoch": 0.2705254988982631,
      "grad_norm": 0.5515145063400269,
      "learning_rate": 4.549280001163551e-05,
      "loss": 1.3332,
      "step": 86800
    },
    {
      "epoch": 0.27068133155891877,
      "grad_norm": 0.37788644433021545,
      "learning_rate": 4.549020280062458e-05,
      "loss": 1.2563,
      "step": 86850
    },
    {
      "epoch": 0.27083716421957443,
      "grad_norm": 0.5835317969322205,
      "learning_rate": 4.548760558961365e-05,
      "loss": 1.252,
      "step": 86900
    },
    {
      "epoch": 0.27099299688023015,
      "grad_norm": 0.804672122001648,
      "learning_rate": 4.5485008378602725e-05,
      "loss": 1.2363,
      "step": 86950
    },
    {
      "epoch": 0.2711488295408858,
      "grad_norm": 0.6022077202796936,
      "learning_rate": 4.548241116759179e-05,
      "loss": 1.2835,
      "step": 87000
    },
    {
      "epoch": 0.2713046622015415,
      "grad_norm": 0.5252754092216492,
      "learning_rate": 4.547981395658087e-05,
      "loss": 1.2698,
      "step": 87050
    },
    {
      "epoch": 0.2714604948621972,
      "grad_norm": 0.5144023895263672,
      "learning_rate": 4.5477216745569936e-05,
      "loss": 1.303,
      "step": 87100
    },
    {
      "epoch": 0.27161632752285286,
      "grad_norm": 0.5714381337165833,
      "learning_rate": 4.547461953455901e-05,
      "loss": 1.2788,
      "step": 87150
    },
    {
      "epoch": 0.2717721601835085,
      "grad_norm": 0.7098354697227478,
      "learning_rate": 4.547202232354809e-05,
      "loss": 1.264,
      "step": 87200
    },
    {
      "epoch": 0.27192799284416425,
      "grad_norm": 0.6366502642631531,
      "learning_rate": 4.5469425112537154e-05,
      "loss": 1.2885,
      "step": 87250
    },
    {
      "epoch": 0.2720838255048199,
      "grad_norm": 0.5165389776229858,
      "learning_rate": 4.5466827901526226e-05,
      "loss": 1.2934,
      "step": 87300
    },
    {
      "epoch": 0.2722396581654756,
      "grad_norm": 0.6315550208091736,
      "learning_rate": 4.54642306905153e-05,
      "loss": 1.2783,
      "step": 87350
    },
    {
      "epoch": 0.2723954908261313,
      "grad_norm": 0.7138054966926575,
      "learning_rate": 4.546163347950437e-05,
      "loss": 1.3093,
      "step": 87400
    },
    {
      "epoch": 0.27255132348678696,
      "grad_norm": 0.5123640298843384,
      "learning_rate": 4.5459036268493444e-05,
      "loss": 1.2721,
      "step": 87450
    },
    {
      "epoch": 0.2727071561474426,
      "grad_norm": 0.5342106223106384,
      "learning_rate": 4.5456439057482516e-05,
      "loss": 1.3476,
      "step": 87500
    },
    {
      "epoch": 0.27286298880809834,
      "grad_norm": 0.648418128490448,
      "learning_rate": 4.545384184647159e-05,
      "loss": 1.3011,
      "step": 87550
    },
    {
      "epoch": 0.273018821468754,
      "grad_norm": 0.41025906801223755,
      "learning_rate": 4.545124463546066e-05,
      "loss": 1.2909,
      "step": 87600
    },
    {
      "epoch": 0.27317465412940967,
      "grad_norm": 0.6340295672416687,
      "learning_rate": 4.5448647424449734e-05,
      "loss": 1.3022,
      "step": 87650
    },
    {
      "epoch": 0.27333048679006533,
      "grad_norm": 0.5079700350761414,
      "learning_rate": 4.54460502134388e-05,
      "loss": 1.2935,
      "step": 87700
    },
    {
      "epoch": 0.27348631945072105,
      "grad_norm": 0.780960738658905,
      "learning_rate": 4.544345300242788e-05,
      "loss": 1.2891,
      "step": 87750
    },
    {
      "epoch": 0.2736421521113767,
      "grad_norm": 0.5761730670928955,
      "learning_rate": 4.5440855791416944e-05,
      "loss": 1.2715,
      "step": 87800
    },
    {
      "epoch": 0.2737979847720324,
      "grad_norm": 0.48911619186401367,
      "learning_rate": 4.543825858040602e-05,
      "loss": 1.3001,
      "step": 87850
    },
    {
      "epoch": 0.2739538174326881,
      "grad_norm": 0.5055565237998962,
      "learning_rate": 4.543566136939509e-05,
      "loss": 1.2518,
      "step": 87900
    },
    {
      "epoch": 0.27410965009334376,
      "grad_norm": 0.5436840057373047,
      "learning_rate": 4.543306415838416e-05,
      "loss": 1.2718,
      "step": 87950
    },
    {
      "epoch": 0.2742654827539994,
      "grad_norm": 0.631568193435669,
      "learning_rate": 4.5430466947373234e-05,
      "loss": 1.2828,
      "step": 88000
    },
    {
      "epoch": 0.27442131541465514,
      "grad_norm": 0.5374544262886047,
      "learning_rate": 4.542786973636231e-05,
      "loss": 1.3039,
      "step": 88050
    },
    {
      "epoch": 0.2745771480753108,
      "grad_norm": 0.5560745596885681,
      "learning_rate": 4.542527252535138e-05,
      "loss": 1.2849,
      "step": 88100
    },
    {
      "epoch": 0.27473298073596647,
      "grad_norm": 0.5626310706138611,
      "learning_rate": 4.542267531434045e-05,
      "loss": 1.293,
      "step": 88150
    },
    {
      "epoch": 0.2748888133966222,
      "grad_norm": 0.6296508312225342,
      "learning_rate": 4.5420078103329524e-05,
      "loss": 1.3308,
      "step": 88200
    },
    {
      "epoch": 0.27504464605727785,
      "grad_norm": 0.614829421043396,
      "learning_rate": 4.541748089231859e-05,
      "loss": 1.28,
      "step": 88250
    },
    {
      "epoch": 0.2752004787179335,
      "grad_norm": 0.5984428524971008,
      "learning_rate": 4.541488368130767e-05,
      "loss": 1.27,
      "step": 88300
    },
    {
      "epoch": 0.27535631137858924,
      "grad_norm": 0.5038381218910217,
      "learning_rate": 4.541228647029674e-05,
      "loss": 1.2684,
      "step": 88350
    },
    {
      "epoch": 0.2755121440392449,
      "grad_norm": 0.5347458124160767,
      "learning_rate": 4.540968925928581e-05,
      "loss": 1.2767,
      "step": 88400
    },
    {
      "epoch": 0.27566797669990056,
      "grad_norm": 0.6481102705001831,
      "learning_rate": 4.540709204827489e-05,
      "loss": 1.2759,
      "step": 88450
    },
    {
      "epoch": 0.2758238093605563,
      "grad_norm": 0.495800256729126,
      "learning_rate": 4.540449483726395e-05,
      "loss": 1.3149,
      "step": 88500
    },
    {
      "epoch": 0.27597964202121195,
      "grad_norm": 0.556863009929657,
      "learning_rate": 4.5401897626253025e-05,
      "loss": 1.2925,
      "step": 88550
    },
    {
      "epoch": 0.2761354746818676,
      "grad_norm": 0.46117550134658813,
      "learning_rate": 4.53993004152421e-05,
      "loss": 1.2165,
      "step": 88600
    },
    {
      "epoch": 0.27629130734252333,
      "grad_norm": 0.4846457839012146,
      "learning_rate": 4.539670320423117e-05,
      "loss": 1.2845,
      "step": 88650
    },
    {
      "epoch": 0.276447140003179,
      "grad_norm": 0.5145203471183777,
      "learning_rate": 4.539410599322024e-05,
      "loss": 1.297,
      "step": 88700
    },
    {
      "epoch": 0.27660297266383466,
      "grad_norm": 0.4761006832122803,
      "learning_rate": 4.5391508782209315e-05,
      "loss": 1.2566,
      "step": 88750
    },
    {
      "epoch": 0.2767588053244904,
      "grad_norm": 0.5004494190216064,
      "learning_rate": 4.538891157119839e-05,
      "loss": 1.3023,
      "step": 88800
    },
    {
      "epoch": 0.27691463798514604,
      "grad_norm": 0.6486847996711731,
      "learning_rate": 4.538631436018746e-05,
      "loss": 1.3485,
      "step": 88850
    },
    {
      "epoch": 0.2770704706458017,
      "grad_norm": 0.5594698786735535,
      "learning_rate": 4.538371714917653e-05,
      "loss": 1.2646,
      "step": 88900
    },
    {
      "epoch": 0.27722630330645737,
      "grad_norm": 0.5719640254974365,
      "learning_rate": 4.53811199381656e-05,
      "loss": 1.3121,
      "step": 88950
    },
    {
      "epoch": 0.2773821359671131,
      "grad_norm": 0.7008188366889954,
      "learning_rate": 4.537852272715468e-05,
      "loss": 1.2738,
      "step": 89000
    },
    {
      "epoch": 0.27753796862776875,
      "grad_norm": 0.5592032670974731,
      "learning_rate": 4.537592551614375e-05,
      "loss": 1.3896,
      "step": 89050
    },
    {
      "epoch": 0.2776938012884244,
      "grad_norm": 0.7832217216491699,
      "learning_rate": 4.5373328305132816e-05,
      "loss": 1.3021,
      "step": 89100
    },
    {
      "epoch": 0.27784963394908013,
      "grad_norm": 0.678544282913208,
      "learning_rate": 4.537073109412189e-05,
      "loss": 1.2772,
      "step": 89150
    },
    {
      "epoch": 0.2780054666097358,
      "grad_norm": 0.5557640194892883,
      "learning_rate": 4.536813388311096e-05,
      "loss": 1.2761,
      "step": 89200
    },
    {
      "epoch": 0.27816129927039146,
      "grad_norm": 0.5160665512084961,
      "learning_rate": 4.536553667210003e-05,
      "loss": 1.3024,
      "step": 89250
    },
    {
      "epoch": 0.2783171319310472,
      "grad_norm": 0.6265513896942139,
      "learning_rate": 4.5362939461089106e-05,
      "loss": 1.2664,
      "step": 89300
    },
    {
      "epoch": 0.27847296459170284,
      "grad_norm": 0.6683763265609741,
      "learning_rate": 4.536034225007818e-05,
      "loss": 1.2745,
      "step": 89350
    },
    {
      "epoch": 0.2786287972523585,
      "grad_norm": 0.5529876351356506,
      "learning_rate": 4.535774503906725e-05,
      "loss": 1.2476,
      "step": 89400
    },
    {
      "epoch": 0.2787846299130142,
      "grad_norm": 0.6883140206336975,
      "learning_rate": 4.535514782805632e-05,
      "loss": 1.3028,
      "step": 89450
    },
    {
      "epoch": 0.2789404625736699,
      "grad_norm": 0.5445650815963745,
      "learning_rate": 4.535255061704539e-05,
      "loss": 1.2935,
      "step": 89500
    },
    {
      "epoch": 0.27909629523432555,
      "grad_norm": 0.6189934611320496,
      "learning_rate": 4.534995340603447e-05,
      "loss": 1.3264,
      "step": 89550
    },
    {
      "epoch": 0.2792521278949813,
      "grad_norm": 0.654151439666748,
      "learning_rate": 4.534735619502354e-05,
      "loss": 1.2761,
      "step": 89600
    },
    {
      "epoch": 0.27940796055563694,
      "grad_norm": 0.6435223817825317,
      "learning_rate": 4.534475898401261e-05,
      "loss": 1.2645,
      "step": 89650
    },
    {
      "epoch": 0.2795637932162926,
      "grad_norm": 0.6134451627731323,
      "learning_rate": 4.5342161773001686e-05,
      "loss": 1.2999,
      "step": 89700
    },
    {
      "epoch": 0.2797196258769483,
      "grad_norm": 0.6317881345748901,
      "learning_rate": 4.533956456199075e-05,
      "loss": 1.3003,
      "step": 89750
    },
    {
      "epoch": 0.279875458537604,
      "grad_norm": 0.6792392730712891,
      "learning_rate": 4.5336967350979824e-05,
      "loss": 1.3022,
      "step": 89800
    },
    {
      "epoch": 0.28003129119825965,
      "grad_norm": 0.5408518314361572,
      "learning_rate": 4.53343701399689e-05,
      "loss": 1.2693,
      "step": 89850
    },
    {
      "epoch": 0.28018712385891537,
      "grad_norm": 0.5584591627120972,
      "learning_rate": 4.533177292895797e-05,
      "loss": 1.2819,
      "step": 89900
    },
    {
      "epoch": 0.28034295651957103,
      "grad_norm": 0.5847387909889221,
      "learning_rate": 4.532917571794704e-05,
      "loss": 1.2752,
      "step": 89950
    },
    {
      "epoch": 0.2804987891802267,
      "grad_norm": 0.5477379560470581,
      "learning_rate": 4.5326578506936114e-05,
      "loss": 1.275,
      "step": 90000
    },
    {
      "epoch": 0.2806546218408824,
      "grad_norm": 0.5948686599731445,
      "learning_rate": 4.532398129592519e-05,
      "loss": 1.3136,
      "step": 90050
    },
    {
      "epoch": 0.2808104545015381,
      "grad_norm": 0.6228139400482178,
      "learning_rate": 4.532138408491426e-05,
      "loss": 1.2633,
      "step": 90100
    },
    {
      "epoch": 0.28096628716219374,
      "grad_norm": 0.6050494313240051,
      "learning_rate": 4.531878687390333e-05,
      "loss": 1.2395,
      "step": 90150
    },
    {
      "epoch": 0.28112211982284946,
      "grad_norm": 0.5032868981361389,
      "learning_rate": 4.53161896628924e-05,
      "loss": 1.3154,
      "step": 90200
    },
    {
      "epoch": 0.2812779524835051,
      "grad_norm": 0.637382447719574,
      "learning_rate": 4.531359245188148e-05,
      "loss": 1.2525,
      "step": 90250
    },
    {
      "epoch": 0.2814337851441608,
      "grad_norm": 0.5989871025085449,
      "learning_rate": 4.531099524087055e-05,
      "loss": 1.2406,
      "step": 90300
    },
    {
      "epoch": 0.28158961780481645,
      "grad_norm": 0.630257785320282,
      "learning_rate": 4.5308398029859615e-05,
      "loss": 1.3089,
      "step": 90350
    },
    {
      "epoch": 0.28174545046547217,
      "grad_norm": 0.5364797711372375,
      "learning_rate": 4.530580081884869e-05,
      "loss": 1.269,
      "step": 90400
    },
    {
      "epoch": 0.28190128312612783,
      "grad_norm": 0.5750184655189514,
      "learning_rate": 4.530320360783776e-05,
      "loss": 1.2894,
      "step": 90450
    },
    {
      "epoch": 0.2820571157867835,
      "grad_norm": 0.6172656416893005,
      "learning_rate": 4.530060639682683e-05,
      "loss": 1.3055,
      "step": 90500
    },
    {
      "epoch": 0.2822129484474392,
      "grad_norm": 0.5629779696464539,
      "learning_rate": 4.5298009185815905e-05,
      "loss": 1.2787,
      "step": 90550
    },
    {
      "epoch": 0.2823687811080949,
      "grad_norm": 0.6033136248588562,
      "learning_rate": 4.529541197480498e-05,
      "loss": 1.2695,
      "step": 90600
    },
    {
      "epoch": 0.28252461376875054,
      "grad_norm": 0.5732226371765137,
      "learning_rate": 4.529281476379405e-05,
      "loss": 1.2346,
      "step": 90650
    },
    {
      "epoch": 0.28268044642940626,
      "grad_norm": 0.6024385690689087,
      "learning_rate": 4.529021755278312e-05,
      "loss": 1.3147,
      "step": 90700
    },
    {
      "epoch": 0.2828362790900619,
      "grad_norm": 0.5212110877037048,
      "learning_rate": 4.528762034177219e-05,
      "loss": 1.2465,
      "step": 90750
    },
    {
      "epoch": 0.2829921117507176,
      "grad_norm": 0.6717135310173035,
      "learning_rate": 4.528502313076127e-05,
      "loss": 1.3074,
      "step": 90800
    },
    {
      "epoch": 0.2831479444113733,
      "grad_norm": 0.49579644203186035,
      "learning_rate": 4.528242591975034e-05,
      "loss": 1.2605,
      "step": 90850
    },
    {
      "epoch": 0.283303777072029,
      "grad_norm": 0.5477232336997986,
      "learning_rate": 4.5279880652959625e-05,
      "loss": 1.3298,
      "step": 90900
    },
    {
      "epoch": 0.28345960973268464,
      "grad_norm": 0.6556195616722107,
      "learning_rate": 4.527733538616892e-05,
      "loss": 1.2838,
      "step": 90950
    },
    {
      "epoch": 0.28361544239334036,
      "grad_norm": 0.5452722311019897,
      "learning_rate": 4.527473817515799e-05,
      "loss": 1.2508,
      "step": 91000
    },
    {
      "epoch": 0.283771275053996,
      "grad_norm": 0.5631448030471802,
      "learning_rate": 4.527214096414706e-05,
      "loss": 1.289,
      "step": 91050
    },
    {
      "epoch": 0.2839271077146517,
      "grad_norm": 0.5967779755592346,
      "learning_rate": 4.5269543753136135e-05,
      "loss": 1.3256,
      "step": 91100
    },
    {
      "epoch": 0.2840829403753074,
      "grad_norm": 0.5523192882537842,
      "learning_rate": 4.526694654212521e-05,
      "loss": 1.2856,
      "step": 91150
    },
    {
      "epoch": 0.28423877303596307,
      "grad_norm": 0.7037512063980103,
      "learning_rate": 4.526434933111428e-05,
      "loss": 1.2862,
      "step": 91200
    },
    {
      "epoch": 0.28439460569661873,
      "grad_norm": 0.6109411120414734,
      "learning_rate": 4.526175212010335e-05,
      "loss": 1.2457,
      "step": 91250
    },
    {
      "epoch": 0.28455043835727445,
      "grad_norm": 0.5001532435417175,
      "learning_rate": 4.5259154909092425e-05,
      "loss": 1.3115,
      "step": 91300
    },
    {
      "epoch": 0.2847062710179301,
      "grad_norm": 0.5027416944503784,
      "learning_rate": 4.525655769808149e-05,
      "loss": 1.2354,
      "step": 91350
    },
    {
      "epoch": 0.2848621036785858,
      "grad_norm": 0.5230715870857239,
      "learning_rate": 4.525396048707057e-05,
      "loss": 1.2548,
      "step": 91400
    },
    {
      "epoch": 0.2850179363392415,
      "grad_norm": 0.5539658665657043,
      "learning_rate": 4.5251363276059636e-05,
      "loss": 1.2867,
      "step": 91450
    },
    {
      "epoch": 0.28517376899989716,
      "grad_norm": 0.626167893409729,
      "learning_rate": 4.524876606504871e-05,
      "loss": 1.2671,
      "step": 91500
    },
    {
      "epoch": 0.2853296016605528,
      "grad_norm": 0.4284021854400635,
      "learning_rate": 4.524616885403779e-05,
      "loss": 1.2806,
      "step": 91550
    },
    {
      "epoch": 0.2854854343212085,
      "grad_norm": 0.8241146206855774,
      "learning_rate": 4.524357164302685e-05,
      "loss": 1.2949,
      "step": 91600
    },
    {
      "epoch": 0.2856412669818642,
      "grad_norm": 0.5927508473396301,
      "learning_rate": 4.5240974432015926e-05,
      "loss": 1.272,
      "step": 91650
    },
    {
      "epoch": 0.28579709964251987,
      "grad_norm": 0.47587311267852783,
      "learning_rate": 4.5238377221005e-05,
      "loss": 1.3051,
      "step": 91700
    },
    {
      "epoch": 0.28595293230317553,
      "grad_norm": 0.539222776889801,
      "learning_rate": 4.523578000999407e-05,
      "loss": 1.2095,
      "step": 91750
    },
    {
      "epoch": 0.28610876496383125,
      "grad_norm": 0.6524333953857422,
      "learning_rate": 4.523318279898314e-05,
      "loss": 1.3204,
      "step": 91800
    },
    {
      "epoch": 0.2862645976244869,
      "grad_norm": 0.6271911859512329,
      "learning_rate": 4.5230585587972216e-05,
      "loss": 1.281,
      "step": 91850
    },
    {
      "epoch": 0.2864204302851426,
      "grad_norm": 0.782088577747345,
      "learning_rate": 4.522798837696128e-05,
      "loss": 1.2663,
      "step": 91900
    },
    {
      "epoch": 0.2865762629457983,
      "grad_norm": 0.6578977704048157,
      "learning_rate": 4.522539116595036e-05,
      "loss": 1.2416,
      "step": 91950
    },
    {
      "epoch": 0.28673209560645396,
      "grad_norm": 0.6290979385375977,
      "learning_rate": 4.5222793954939426e-05,
      "loss": 1.2828,
      "step": 92000
    },
    {
      "epoch": 0.2868879282671096,
      "grad_norm": 0.6554409861564636,
      "learning_rate": 4.52201967439285e-05,
      "loss": 1.2942,
      "step": 92050
    },
    {
      "epoch": 0.28704376092776535,
      "grad_norm": 0.6022779941558838,
      "learning_rate": 4.521759953291758e-05,
      "loss": 1.3007,
      "step": 92100
    },
    {
      "epoch": 0.287199593588421,
      "grad_norm": 0.5650609731674194,
      "learning_rate": 4.5215002321906644e-05,
      "loss": 1.3055,
      "step": 92150
    },
    {
      "epoch": 0.2873554262490767,
      "grad_norm": 0.5236136317253113,
      "learning_rate": 4.5212405110895716e-05,
      "loss": 1.2559,
      "step": 92200
    },
    {
      "epoch": 0.2875112589097324,
      "grad_norm": 0.7750171422958374,
      "learning_rate": 4.520980789988479e-05,
      "loss": 1.3044,
      "step": 92250
    },
    {
      "epoch": 0.28766709157038806,
      "grad_norm": 0.6896830201148987,
      "learning_rate": 4.520721068887386e-05,
      "loss": 1.2657,
      "step": 92300
    },
    {
      "epoch": 0.2878229242310437,
      "grad_norm": 0.6202024221420288,
      "learning_rate": 4.5204613477862934e-05,
      "loss": 1.2863,
      "step": 92350
    },
    {
      "epoch": 0.28797875689169944,
      "grad_norm": 0.5514146089553833,
      "learning_rate": 4.5202016266852006e-05,
      "loss": 1.2834,
      "step": 92400
    },
    {
      "epoch": 0.2881345895523551,
      "grad_norm": 0.6119816899299622,
      "learning_rate": 4.519941905584108e-05,
      "loss": 1.2741,
      "step": 92450
    },
    {
      "epoch": 0.28829042221301077,
      "grad_norm": 0.5522558093070984,
      "learning_rate": 4.519682184483015e-05,
      "loss": 1.2707,
      "step": 92500
    },
    {
      "epoch": 0.2884462548736665,
      "grad_norm": 0.556108295917511,
      "learning_rate": 4.5194224633819224e-05,
      "loss": 1.252,
      "step": 92550
    },
    {
      "epoch": 0.28860208753432215,
      "grad_norm": 0.6908206343650818,
      "learning_rate": 4.519162742280829e-05,
      "loss": 1.2591,
      "step": 92600
    },
    {
      "epoch": 0.2887579201949778,
      "grad_norm": 0.7641803622245789,
      "learning_rate": 4.518903021179737e-05,
      "loss": 1.2504,
      "step": 92650
    },
    {
      "epoch": 0.28891375285563353,
      "grad_norm": 0.6841703653335571,
      "learning_rate": 4.5186433000786435e-05,
      "loss": 1.3349,
      "step": 92700
    },
    {
      "epoch": 0.2890695855162892,
      "grad_norm": 0.5560402870178223,
      "learning_rate": 4.518383578977551e-05,
      "loss": 1.3004,
      "step": 92750
    },
    {
      "epoch": 0.28922541817694486,
      "grad_norm": 0.5927939414978027,
      "learning_rate": 4.5181238578764586e-05,
      "loss": 1.2984,
      "step": 92800
    },
    {
      "epoch": 0.2893812508376006,
      "grad_norm": 0.5348142981529236,
      "learning_rate": 4.517864136775365e-05,
      "loss": 1.264,
      "step": 92850
    },
    {
      "epoch": 0.28953708349825624,
      "grad_norm": 0.6487886309623718,
      "learning_rate": 4.5176044156742725e-05,
      "loss": 1.2845,
      "step": 92900
    },
    {
      "epoch": 0.2896929161589119,
      "grad_norm": 0.5970886945724487,
      "learning_rate": 4.51734469457318e-05,
      "loss": 1.2707,
      "step": 92950
    },
    {
      "epoch": 0.28984874881956757,
      "grad_norm": 0.6040114760398865,
      "learning_rate": 4.517084973472087e-05,
      "loss": 1.3347,
      "step": 93000
    },
    {
      "epoch": 0.2900045814802233,
      "grad_norm": 0.6348373889923096,
      "learning_rate": 4.516825252370994e-05,
      "loss": 1.3272,
      "step": 93050
    },
    {
      "epoch": 0.29016041414087895,
      "grad_norm": 0.47357356548309326,
      "learning_rate": 4.5165655312699015e-05,
      "loss": 1.3007,
      "step": 93100
    },
    {
      "epoch": 0.2903162468015346,
      "grad_norm": 0.6825065016746521,
      "learning_rate": 4.516305810168808e-05,
      "loss": 1.3177,
      "step": 93150
    },
    {
      "epoch": 0.29047207946219034,
      "grad_norm": 0.5769498348236084,
      "learning_rate": 4.516046089067716e-05,
      "loss": 1.2776,
      "step": 93200
    },
    {
      "epoch": 0.290627912122846,
      "grad_norm": 0.6178246736526489,
      "learning_rate": 4.515786367966623e-05,
      "loss": 1.3004,
      "step": 93250
    },
    {
      "epoch": 0.29078374478350166,
      "grad_norm": 0.5048084259033203,
      "learning_rate": 4.51552664686553e-05,
      "loss": 1.2676,
      "step": 93300
    },
    {
      "epoch": 0.2909395774441574,
      "grad_norm": 0.5827587246894836,
      "learning_rate": 4.515266925764438e-05,
      "loss": 1.2461,
      "step": 93350
    },
    {
      "epoch": 0.29109541010481305,
      "grad_norm": 0.5935578942298889,
      "learning_rate": 4.515007204663344e-05,
      "loss": 1.3021,
      "step": 93400
    },
    {
      "epoch": 0.2912512427654687,
      "grad_norm": 0.7481949329376221,
      "learning_rate": 4.5147474835622515e-05,
      "loss": 1.3177,
      "step": 93450
    },
    {
      "epoch": 0.29140707542612443,
      "grad_norm": 0.5756901502609253,
      "learning_rate": 4.514487762461159e-05,
      "loss": 1.2701,
      "step": 93500
    },
    {
      "epoch": 0.2915629080867801,
      "grad_norm": 0.5244693160057068,
      "learning_rate": 4.514228041360066e-05,
      "loss": 1.2928,
      "step": 93550
    },
    {
      "epoch": 0.29171874074743576,
      "grad_norm": 0.5839806199073792,
      "learning_rate": 4.513968320258973e-05,
      "loss": 1.2749,
      "step": 93600
    },
    {
      "epoch": 0.2918745734080915,
      "grad_norm": 0.5975198745727539,
      "learning_rate": 4.5137085991578805e-05,
      "loss": 1.3076,
      "step": 93650
    },
    {
      "epoch": 0.29203040606874714,
      "grad_norm": 0.5947805643081665,
      "learning_rate": 4.513448878056788e-05,
      "loss": 1.2745,
      "step": 93700
    },
    {
      "epoch": 0.2921862387294028,
      "grad_norm": 0.5905247926712036,
      "learning_rate": 4.513189156955695e-05,
      "loss": 1.264,
      "step": 93750
    },
    {
      "epoch": 0.2923420713900585,
      "grad_norm": 0.5729665756225586,
      "learning_rate": 4.512934630276624e-05,
      "loss": 1.2557,
      "step": 93800
    },
    {
      "epoch": 0.2924979040507142,
      "grad_norm": 0.735503077507019,
      "learning_rate": 4.512674909175531e-05,
      "loss": 1.3062,
      "step": 93850
    },
    {
      "epoch": 0.29265373671136985,
      "grad_norm": 0.698738694190979,
      "learning_rate": 4.512415188074439e-05,
      "loss": 1.3055,
      "step": 93900
    },
    {
      "epoch": 0.29280956937202557,
      "grad_norm": 0.5992863178253174,
      "learning_rate": 4.512155466973346e-05,
      "loss": 1.2891,
      "step": 93950
    },
    {
      "epoch": 0.29296540203268123,
      "grad_norm": 0.5692920088768005,
      "learning_rate": 4.5118957458722526e-05,
      "loss": 1.316,
      "step": 94000
    },
    {
      "epoch": 0.2931212346933369,
      "grad_norm": 0.6956822276115417,
      "learning_rate": 4.51163602477116e-05,
      "loss": 1.2638,
      "step": 94050
    },
    {
      "epoch": 0.2932770673539926,
      "grad_norm": 0.5353192687034607,
      "learning_rate": 4.511376303670067e-05,
      "loss": 1.2794,
      "step": 94100
    },
    {
      "epoch": 0.2934329000146483,
      "grad_norm": 0.5957822799682617,
      "learning_rate": 4.511116582568974e-05,
      "loss": 1.2969,
      "step": 94150
    },
    {
      "epoch": 0.29358873267530394,
      "grad_norm": 0.6047754883766174,
      "learning_rate": 4.5108568614678816e-05,
      "loss": 1.2947,
      "step": 94200
    },
    {
      "epoch": 0.2937445653359596,
      "grad_norm": 0.6430733799934387,
      "learning_rate": 4.510597140366788e-05,
      "loss": 1.2821,
      "step": 94250
    },
    {
      "epoch": 0.2939003979966153,
      "grad_norm": 0.5783955454826355,
      "learning_rate": 4.510337419265696e-05,
      "loss": 1.3073,
      "step": 94300
    },
    {
      "epoch": 0.294056230657271,
      "grad_norm": 0.48991066217422485,
      "learning_rate": 4.510077698164603e-05,
      "loss": 1.2867,
      "step": 94350
    },
    {
      "epoch": 0.29421206331792665,
      "grad_norm": 0.5384417176246643,
      "learning_rate": 4.50981797706351e-05,
      "loss": 1.2805,
      "step": 94400
    },
    {
      "epoch": 0.2943678959785824,
      "grad_norm": 0.6198071241378784,
      "learning_rate": 4.509558255962418e-05,
      "loss": 1.2674,
      "step": 94450
    },
    {
      "epoch": 0.29452372863923804,
      "grad_norm": 0.7431765794754028,
      "learning_rate": 4.509298534861325e-05,
      "loss": 1.2729,
      "step": 94500
    },
    {
      "epoch": 0.2946795612998937,
      "grad_norm": 0.4990307688713074,
      "learning_rate": 4.5090388137602317e-05,
      "loss": 1.2722,
      "step": 94550
    },
    {
      "epoch": 0.2948353939605494,
      "grad_norm": 0.5612137317657471,
      "learning_rate": 4.508779092659139e-05,
      "loss": 1.2992,
      "step": 94600
    },
    {
      "epoch": 0.2949912266212051,
      "grad_norm": 0.5394738912582397,
      "learning_rate": 4.508519371558046e-05,
      "loss": 1.3391,
      "step": 94650
    },
    {
      "epoch": 0.29514705928186075,
      "grad_norm": 0.6243597865104675,
      "learning_rate": 4.5082596504569534e-05,
      "loss": 1.3518,
      "step": 94700
    },
    {
      "epoch": 0.29530289194251647,
      "grad_norm": 0.6651831865310669,
      "learning_rate": 4.507999929355861e-05,
      "loss": 1.282,
      "step": 94750
    },
    {
      "epoch": 0.29545872460317213,
      "grad_norm": 0.4946800768375397,
      "learning_rate": 4.507740208254768e-05,
      "loss": 1.2671,
      "step": 94800
    },
    {
      "epoch": 0.2956145572638278,
      "grad_norm": 0.6572282910346985,
      "learning_rate": 4.507480487153675e-05,
      "loss": 1.2826,
      "step": 94850
    },
    {
      "epoch": 0.2957703899244835,
      "grad_norm": 0.6565377116203308,
      "learning_rate": 4.5072207660525824e-05,
      "loss": 1.3116,
      "step": 94900
    },
    {
      "epoch": 0.2959262225851392,
      "grad_norm": 0.605445384979248,
      "learning_rate": 4.506961044951489e-05,
      "loss": 1.3142,
      "step": 94950
    },
    {
      "epoch": 0.29608205524579484,
      "grad_norm": 0.6622234582901001,
      "learning_rate": 4.506701323850397e-05,
      "loss": 1.2645,
      "step": 95000
    },
    {
      "epoch": 0.29623788790645056,
      "grad_norm": 0.6220176219940186,
      "learning_rate": 4.506441602749304e-05,
      "loss": 1.2616,
      "step": 95050
    },
    {
      "epoch": 0.2963937205671062,
      "grad_norm": 0.53985196352005,
      "learning_rate": 4.506181881648211e-05,
      "loss": 1.2297,
      "step": 95100
    },
    {
      "epoch": 0.2965495532277619,
      "grad_norm": 0.5711265802383423,
      "learning_rate": 4.505922160547119e-05,
      "loss": 1.2914,
      "step": 95150
    },
    {
      "epoch": 0.2967053858884176,
      "grad_norm": 0.6377158761024475,
      "learning_rate": 4.505662439446026e-05,
      "loss": 1.2751,
      "step": 95200
    },
    {
      "epoch": 0.29686121854907327,
      "grad_norm": 0.6815695762634277,
      "learning_rate": 4.5054027183449325e-05,
      "loss": 1.313,
      "step": 95250
    },
    {
      "epoch": 0.29701705120972893,
      "grad_norm": 0.6014872789382935,
      "learning_rate": 4.50514299724384e-05,
      "loss": 1.3527,
      "step": 95300
    },
    {
      "epoch": 0.29717288387038465,
      "grad_norm": 0.5888962745666504,
      "learning_rate": 4.504883276142747e-05,
      "loss": 1.2604,
      "step": 95350
    },
    {
      "epoch": 0.2973287165310403,
      "grad_norm": 0.45924296975135803,
      "learning_rate": 4.504623555041654e-05,
      "loss": 1.2927,
      "step": 95400
    },
    {
      "epoch": 0.297484549191696,
      "grad_norm": 0.6108421087265015,
      "learning_rate": 4.5043638339405615e-05,
      "loss": 1.2718,
      "step": 95450
    },
    {
      "epoch": 0.29764038185235164,
      "grad_norm": 0.6607656478881836,
      "learning_rate": 4.504104112839469e-05,
      "loss": 1.3065,
      "step": 95500
    },
    {
      "epoch": 0.29779621451300736,
      "grad_norm": 0.6234967112541199,
      "learning_rate": 4.503844391738376e-05,
      "loss": 1.3065,
      "step": 95550
    },
    {
      "epoch": 0.297952047173663,
      "grad_norm": 0.574096143245697,
      "learning_rate": 4.503584670637283e-05,
      "loss": 1.3167,
      "step": 95600
    },
    {
      "epoch": 0.2981078798343187,
      "grad_norm": 0.6495015621185303,
      "learning_rate": 4.50332494953619e-05,
      "loss": 1.2842,
      "step": 95650
    },
    {
      "epoch": 0.2982637124949744,
      "grad_norm": 0.7182378768920898,
      "learning_rate": 4.503065228435098e-05,
      "loss": 1.3215,
      "step": 95700
    },
    {
      "epoch": 0.2984195451556301,
      "grad_norm": 0.6117981672286987,
      "learning_rate": 4.502805507334005e-05,
      "loss": 1.309,
      "step": 95750
    },
    {
      "epoch": 0.29857537781628574,
      "grad_norm": 0.6057953834533691,
      "learning_rate": 4.5025457862329116e-05,
      "loss": 1.296,
      "step": 95800
    },
    {
      "epoch": 0.29873121047694146,
      "grad_norm": 0.5881677865982056,
      "learning_rate": 4.502286065131819e-05,
      "loss": 1.2827,
      "step": 95850
    },
    {
      "epoch": 0.2988870431375971,
      "grad_norm": 0.49803686141967773,
      "learning_rate": 4.502026344030727e-05,
      "loss": 1.3097,
      "step": 95900
    },
    {
      "epoch": 0.2990428757982528,
      "grad_norm": 0.7468553185462952,
      "learning_rate": 4.501766622929633e-05,
      "loss": 1.3474,
      "step": 95950
    },
    {
      "epoch": 0.2991987084589085,
      "grad_norm": 0.5677075386047363,
      "learning_rate": 4.5015069018285406e-05,
      "loss": 1.3155,
      "step": 96000
    },
    {
      "epoch": 0.29935454111956417,
      "grad_norm": 0.6365548968315125,
      "learning_rate": 4.501247180727448e-05,
      "loss": 1.2747,
      "step": 96050
    },
    {
      "epoch": 0.29951037378021983,
      "grad_norm": 0.5126882791519165,
      "learning_rate": 4.500987459626355e-05,
      "loss": 1.2608,
      "step": 96100
    },
    {
      "epoch": 0.29966620644087555,
      "grad_norm": 0.47791025042533875,
      "learning_rate": 4.500727738525262e-05,
      "loss": 1.2649,
      "step": 96150
    },
    {
      "epoch": 0.2998220391015312,
      "grad_norm": 0.6058936715126038,
      "learning_rate": 4.5004680174241696e-05,
      "loss": 1.2835,
      "step": 96200
    },
    {
      "epoch": 0.2999778717621869,
      "grad_norm": 0.5299693942070007,
      "learning_rate": 4.500208296323077e-05,
      "loss": 1.3183,
      "step": 96250
    },
    {
      "epoch": 0.3001337044228426,
      "grad_norm": 0.5560826063156128,
      "learning_rate": 4.499948575221984e-05,
      "loss": 1.258,
      "step": 96300
    },
    {
      "epoch": 0.30028953708349826,
      "grad_norm": 0.9025711417198181,
      "learning_rate": 4.4996888541208906e-05,
      "loss": 1.2292,
      "step": 96350
    },
    {
      "epoch": 0.3004453697441539,
      "grad_norm": 0.5649691224098206,
      "learning_rate": 4.499429133019798e-05,
      "loss": 1.313,
      "step": 96400
    },
    {
      "epoch": 0.30060120240480964,
      "grad_norm": 0.5626839995384216,
      "learning_rate": 4.499169411918706e-05,
      "loss": 1.2383,
      "step": 96450
    },
    {
      "epoch": 0.3007570350654653,
      "grad_norm": 0.6383341550827026,
      "learning_rate": 4.4989096908176124e-05,
      "loss": 1.2807,
      "step": 96500
    },
    {
      "epoch": 0.30091286772612097,
      "grad_norm": 0.6585282683372498,
      "learning_rate": 4.4986499697165196e-05,
      "loss": 1.2376,
      "step": 96550
    },
    {
      "epoch": 0.3010687003867767,
      "grad_norm": 0.4823295772075653,
      "learning_rate": 4.4983902486154276e-05,
      "loss": 1.3124,
      "step": 96600
    },
    {
      "epoch": 0.30122453304743235,
      "grad_norm": 0.6041480898857117,
      "learning_rate": 4.498130527514334e-05,
      "loss": 1.2722,
      "step": 96650
    },
    {
      "epoch": 0.301380365708088,
      "grad_norm": 0.7538700699806213,
      "learning_rate": 4.4978708064132414e-05,
      "loss": 1.274,
      "step": 96700
    },
    {
      "epoch": 0.30153619836874374,
      "grad_norm": 0.5654622316360474,
      "learning_rate": 4.4976110853121486e-05,
      "loss": 1.2673,
      "step": 96750
    },
    {
      "epoch": 0.3016920310293994,
      "grad_norm": 0.6189660429954529,
      "learning_rate": 4.497351364211056e-05,
      "loss": 1.255,
      "step": 96800
    },
    {
      "epoch": 0.30184786369005506,
      "grad_norm": 0.5337457060813904,
      "learning_rate": 4.497091643109963e-05,
      "loss": 1.2989,
      "step": 96850
    },
    {
      "epoch": 0.3020036963507107,
      "grad_norm": 0.7162205576896667,
      "learning_rate": 4.49683192200887e-05,
      "loss": 1.2629,
      "step": 96900
    },
    {
      "epoch": 0.30215952901136645,
      "grad_norm": 0.5486875176429749,
      "learning_rate": 4.4965722009077776e-05,
      "loss": 1.2851,
      "step": 96950
    },
    {
      "epoch": 0.3023153616720221,
      "grad_norm": 0.4924072325229645,
      "learning_rate": 4.496312479806685e-05,
      "loss": 1.2548,
      "step": 97000
    },
    {
      "epoch": 0.3024711943326778,
      "grad_norm": 0.5724454522132874,
      "learning_rate": 4.4960527587055915e-05,
      "loss": 1.3549,
      "step": 97050
    },
    {
      "epoch": 0.3026270269933335,
      "grad_norm": 0.5485723614692688,
      "learning_rate": 4.495793037604499e-05,
      "loss": 1.2893,
      "step": 97100
    },
    {
      "epoch": 0.30278285965398916,
      "grad_norm": 0.5160033702850342,
      "learning_rate": 4.4955333165034066e-05,
      "loss": 1.2484,
      "step": 97150
    },
    {
      "epoch": 0.3029386923146448,
      "grad_norm": 0.509704053401947,
      "learning_rate": 4.495273595402313e-05,
      "loss": 1.2536,
      "step": 97200
    },
    {
      "epoch": 0.30309452497530054,
      "grad_norm": 0.633838415145874,
      "learning_rate": 4.4950138743012205e-05,
      "loss": 1.3011,
      "step": 97250
    },
    {
      "epoch": 0.3032503576359562,
      "grad_norm": 0.5229220986366272,
      "learning_rate": 4.494754153200128e-05,
      "loss": 1.289,
      "step": 97300
    },
    {
      "epoch": 0.30340619029661187,
      "grad_norm": 0.6062067747116089,
      "learning_rate": 4.494494432099035e-05,
      "loss": 1.2986,
      "step": 97350
    },
    {
      "epoch": 0.3035620229572676,
      "grad_norm": 0.6893708109855652,
      "learning_rate": 4.494234710997942e-05,
      "loss": 1.2495,
      "step": 97400
    },
    {
      "epoch": 0.30371785561792325,
      "grad_norm": 0.5300588011741638,
      "learning_rate": 4.4939749898968495e-05,
      "loss": 1.2547,
      "step": 97450
    },
    {
      "epoch": 0.3038736882785789,
      "grad_norm": 0.6074531078338623,
      "learning_rate": 4.493715268795757e-05,
      "loss": 1.2912,
      "step": 97500
    },
    {
      "epoch": 0.30402952093923463,
      "grad_norm": 0.6824127435684204,
      "learning_rate": 4.493455547694664e-05,
      "loss": 1.3302,
      "step": 97550
    },
    {
      "epoch": 0.3041853535998903,
      "grad_norm": 0.6009795069694519,
      "learning_rate": 4.4931958265935705e-05,
      "loss": 1.2812,
      "step": 97600
    },
    {
      "epoch": 0.30434118626054596,
      "grad_norm": 0.7307578921318054,
      "learning_rate": 4.492936105492478e-05,
      "loss": 1.2753,
      "step": 97650
    },
    {
      "epoch": 0.3044970189212017,
      "grad_norm": 0.49199846386909485,
      "learning_rate": 4.492676384391386e-05,
      "loss": 1.2796,
      "step": 97700
    },
    {
      "epoch": 0.30465285158185734,
      "grad_norm": 0.7051723599433899,
      "learning_rate": 4.492416663290292e-05,
      "loss": 1.2831,
      "step": 97750
    },
    {
      "epoch": 0.304808684242513,
      "grad_norm": 0.6075356006622314,
      "learning_rate": 4.4921569421891995e-05,
      "loss": 1.2936,
      "step": 97800
    },
    {
      "epoch": 0.3049645169031687,
      "grad_norm": 0.6023327112197876,
      "learning_rate": 4.4918972210881075e-05,
      "loss": 1.2462,
      "step": 97850
    },
    {
      "epoch": 0.3051203495638244,
      "grad_norm": 0.7100374102592468,
      "learning_rate": 4.491637499987014e-05,
      "loss": 1.2405,
      "step": 97900
    },
    {
      "epoch": 0.30527618222448005,
      "grad_norm": 0.6376941204071045,
      "learning_rate": 4.491377778885921e-05,
      "loss": 1.278,
      "step": 97950
    },
    {
      "epoch": 0.30543201488513577,
      "grad_norm": 0.5308479070663452,
      "learning_rate": 4.4911180577848285e-05,
      "loss": 1.2849,
      "step": 98000
    },
    {
      "epoch": 0.30558784754579144,
      "grad_norm": 0.624951958656311,
      "learning_rate": 4.490858336683736e-05,
      "loss": 1.2949,
      "step": 98050
    },
    {
      "epoch": 0.3057436802064471,
      "grad_norm": 0.9100723266601562,
      "learning_rate": 4.490598615582643e-05,
      "loss": 1.2367,
      "step": 98100
    },
    {
      "epoch": 0.30589951286710276,
      "grad_norm": 0.6926414966583252,
      "learning_rate": 4.49033889448155e-05,
      "loss": 1.2849,
      "step": 98150
    },
    {
      "epoch": 0.3060553455277585,
      "grad_norm": 0.5667209625244141,
      "learning_rate": 4.4900791733804575e-05,
      "loss": 1.2844,
      "step": 98200
    },
    {
      "epoch": 0.30621117818841415,
      "grad_norm": 0.6951920390129089,
      "learning_rate": 4.489819452279365e-05,
      "loss": 1.3321,
      "step": 98250
    },
    {
      "epoch": 0.3063670108490698,
      "grad_norm": 0.6612361669540405,
      "learning_rate": 4.4895597311782714e-05,
      "loss": 1.2483,
      "step": 98300
    },
    {
      "epoch": 0.30652284350972553,
      "grad_norm": 0.5551003217697144,
      "learning_rate": 4.4893000100771786e-05,
      "loss": 1.2255,
      "step": 98350
    },
    {
      "epoch": 0.3066786761703812,
      "grad_norm": 0.5530357360839844,
      "learning_rate": 4.4890402889760865e-05,
      "loss": 1.288,
      "step": 98400
    },
    {
      "epoch": 0.30683450883103686,
      "grad_norm": 0.65899258852005,
      "learning_rate": 4.488780567874993e-05,
      "loss": 1.3239,
      "step": 98450
    },
    {
      "epoch": 0.3069903414916926,
      "grad_norm": 0.6518936157226562,
      "learning_rate": 4.4885208467739004e-05,
      "loss": 1.3192,
      "step": 98500
    },
    {
      "epoch": 0.30714617415234824,
      "grad_norm": 0.528838038444519,
      "learning_rate": 4.488261125672808e-05,
      "loss": 1.3199,
      "step": 98550
    },
    {
      "epoch": 0.3073020068130039,
      "grad_norm": 0.6013385653495789,
      "learning_rate": 4.488001404571715e-05,
      "loss": 1.2971,
      "step": 98600
    },
    {
      "epoch": 0.3074578394736596,
      "grad_norm": 0.45610323548316956,
      "learning_rate": 4.487746877892644e-05,
      "loss": 1.2643,
      "step": 98650
    },
    {
      "epoch": 0.3076136721343153,
      "grad_norm": 0.4942629635334015,
      "learning_rate": 4.487487156791551e-05,
      "loss": 1.3048,
      "step": 98700
    },
    {
      "epoch": 0.30776950479497095,
      "grad_norm": 0.5889999270439148,
      "learning_rate": 4.487227435690458e-05,
      "loss": 1.3364,
      "step": 98750
    },
    {
      "epoch": 0.30792533745562667,
      "grad_norm": 0.6268710494041443,
      "learning_rate": 4.486967714589366e-05,
      "loss": 1.288,
      "step": 98800
    },
    {
      "epoch": 0.30808117011628233,
      "grad_norm": 0.564455509185791,
      "learning_rate": 4.486707993488273e-05,
      "loss": 1.2548,
      "step": 98850
    },
    {
      "epoch": 0.308237002776938,
      "grad_norm": 0.570297360420227,
      "learning_rate": 4.4864482723871797e-05,
      "loss": 1.3104,
      "step": 98900
    },
    {
      "epoch": 0.3083928354375937,
      "grad_norm": 0.6798434853553772,
      "learning_rate": 4.4861885512860876e-05,
      "loss": 1.2626,
      "step": 98950
    },
    {
      "epoch": 0.3085486680982494,
      "grad_norm": 0.5482914447784424,
      "learning_rate": 4.485928830184994e-05,
      "loss": 1.3163,
      "step": 99000
    },
    {
      "epoch": 0.30870450075890504,
      "grad_norm": 0.6615256071090698,
      "learning_rate": 4.4856691090839014e-05,
      "loss": 1.2559,
      "step": 99050
    },
    {
      "epoch": 0.30886033341956076,
      "grad_norm": 0.7171490788459778,
      "learning_rate": 4.4854093879828087e-05,
      "loss": 1.2513,
      "step": 99100
    },
    {
      "epoch": 0.3090161660802164,
      "grad_norm": 0.5548343062400818,
      "learning_rate": 4.485149666881716e-05,
      "loss": 1.2715,
      "step": 99150
    },
    {
      "epoch": 0.3091719987408721,
      "grad_norm": 0.5043261051177979,
      "learning_rate": 4.484889945780623e-05,
      "loss": 1.2413,
      "step": 99200
    },
    {
      "epoch": 0.3093278314015278,
      "grad_norm": 0.6550219058990479,
      "learning_rate": 4.4846302246795304e-05,
      "loss": 1.2889,
      "step": 99250
    },
    {
      "epoch": 0.30948366406218347,
      "grad_norm": 0.7366300821304321,
      "learning_rate": 4.4843705035784377e-05,
      "loss": 1.2724,
      "step": 99300
    },
    {
      "epoch": 0.30963949672283914,
      "grad_norm": 0.7543854713439941,
      "learning_rate": 4.484110782477345e-05,
      "loss": 1.3197,
      "step": 99350
    },
    {
      "epoch": 0.30979532938349486,
      "grad_norm": 0.4079675078392029,
      "learning_rate": 4.483851061376252e-05,
      "loss": 1.2546,
      "step": 99400
    },
    {
      "epoch": 0.3099511620441505,
      "grad_norm": 0.7120348811149597,
      "learning_rate": 4.483591340275159e-05,
      "loss": 1.2863,
      "step": 99450
    },
    {
      "epoch": 0.3101069947048062,
      "grad_norm": 0.5074723958969116,
      "learning_rate": 4.4833316191740667e-05,
      "loss": 1.2877,
      "step": 99500
    },
    {
      "epoch": 0.31026282736546185,
      "grad_norm": 0.6494568586349487,
      "learning_rate": 4.483071898072973e-05,
      "loss": 1.2677,
      "step": 99550
    },
    {
      "epoch": 0.31041866002611757,
      "grad_norm": 0.5199990272521973,
      "learning_rate": 4.4828121769718805e-05,
      "loss": 1.3727,
      "step": 99600
    },
    {
      "epoch": 0.31057449268677323,
      "grad_norm": 0.6094517111778259,
      "learning_rate": 4.4825524558707884e-05,
      "loss": 1.3065,
      "step": 99650
    },
    {
      "epoch": 0.3107303253474289,
      "grad_norm": Infinity,
      "learning_rate": 4.482292734769695e-05,
      "loss": 1.2812,
      "step": 99700
    },
    {
      "epoch": 0.3108861580080846,
      "grad_norm": 0.4463479816913605,
      "learning_rate": 4.482038208090624e-05,
      "loss": 1.2625,
      "step": 99750
    },
    {
      "epoch": 0.3110419906687403,
      "grad_norm": 0.6140783429145813,
      "learning_rate": 4.4817784869895314e-05,
      "loss": 1.2778,
      "step": 99800
    },
    {
      "epoch": 0.31119782332939594,
      "grad_norm": 0.4912654459476471,
      "learning_rate": 4.481518765888438e-05,
      "loss": 1.2994,
      "step": 99850
    },
    {
      "epoch": 0.31135365599005166,
      "grad_norm": 1.3080778121948242,
      "learning_rate": 4.481259044787346e-05,
      "loss": 1.2766,
      "step": 99900
    },
    {
      "epoch": 0.3115094886507073,
      "grad_norm": 0.6295023560523987,
      "learning_rate": 4.480999323686253e-05,
      "loss": 1.3305,
      "step": 99950
    },
    {
      "epoch": 0.311665321311363,
      "grad_norm": 0.506354808807373,
      "learning_rate": 4.48073960258516e-05,
      "loss": 1.2667,
      "step": 100000
    },
    {
      "epoch": 0.3118211539720187,
      "grad_norm": 0.61216801404953,
      "learning_rate": 4.480479881484068e-05,
      "loss": 1.2932,
      "step": 100050
    },
    {
      "epoch": 0.31197698663267437,
      "grad_norm": 0.6324853301048279,
      "learning_rate": 4.480220160382975e-05,
      "loss": 1.335,
      "step": 100100
    },
    {
      "epoch": 0.31213281929333003,
      "grad_norm": 0.7565706968307495,
      "learning_rate": 4.4799604392818815e-05,
      "loss": 1.235,
      "step": 100150
    },
    {
      "epoch": 0.31228865195398575,
      "grad_norm": 0.7823220491409302,
      "learning_rate": 4.479700718180789e-05,
      "loss": 1.2535,
      "step": 100200
    },
    {
      "epoch": 0.3124444846146414,
      "grad_norm": 0.6159356832504272,
      "learning_rate": 4.479440997079696e-05,
      "loss": 1.2325,
      "step": 100250
    },
    {
      "epoch": 0.3126003172752971,
      "grad_norm": 0.6588977575302124,
      "learning_rate": 4.479181275978603e-05,
      "loss": 1.299,
      "step": 100300
    },
    {
      "epoch": 0.3127561499359528,
      "grad_norm": 0.5287116765975952,
      "learning_rate": 4.4789215548775105e-05,
      "loss": 1.2391,
      "step": 100350
    },
    {
      "epoch": 0.31291198259660846,
      "grad_norm": 0.6185861825942993,
      "learning_rate": 4.478661833776418e-05,
      "loss": 1.2714,
      "step": 100400
    },
    {
      "epoch": 0.3130678152572641,
      "grad_norm": 0.6512271165847778,
      "learning_rate": 4.478402112675325e-05,
      "loss": 1.3094,
      "step": 100450
    },
    {
      "epoch": 0.31322364791791985,
      "grad_norm": 0.5899868011474609,
      "learning_rate": 4.478142391574232e-05,
      "loss": 1.2594,
      "step": 100500
    },
    {
      "epoch": 0.3133794805785755,
      "grad_norm": 0.4314413070678711,
      "learning_rate": 4.477882670473139e-05,
      "loss": 1.321,
      "step": 100550
    },
    {
      "epoch": 0.3135353132392312,
      "grad_norm": 0.6853488087654114,
      "learning_rate": 4.477622949372047e-05,
      "loss": 1.2605,
      "step": 100600
    },
    {
      "epoch": 0.3136911458998869,
      "grad_norm": 0.6693912744522095,
      "learning_rate": 4.477363228270954e-05,
      "loss": 1.3061,
      "step": 100650
    },
    {
      "epoch": 0.31384697856054256,
      "grad_norm": 0.5892251133918762,
      "learning_rate": 4.4771035071698606e-05,
      "loss": 1.2265,
      "step": 100700
    },
    {
      "epoch": 0.3140028112211982,
      "grad_norm": 0.4895721673965454,
      "learning_rate": 4.476843786068768e-05,
      "loss": 1.2921,
      "step": 100750
    },
    {
      "epoch": 0.3141586438818539,
      "grad_norm": 0.6140424013137817,
      "learning_rate": 4.476584064967676e-05,
      "loss": 1.2322,
      "step": 100800
    },
    {
      "epoch": 0.3143144765425096,
      "grad_norm": 0.6539985537528992,
      "learning_rate": 4.4763243438665823e-05,
      "loss": 1.2871,
      "step": 100850
    },
    {
      "epoch": 0.31447030920316527,
      "grad_norm": 0.676758885383606,
      "learning_rate": 4.4760646227654896e-05,
      "loss": 1.2275,
      "step": 100900
    },
    {
      "epoch": 0.31462614186382093,
      "grad_norm": 0.5691264867782593,
      "learning_rate": 4.475804901664397e-05,
      "loss": 1.2549,
      "step": 100950
    },
    {
      "epoch": 0.31478197452447665,
      "grad_norm": 0.6524783968925476,
      "learning_rate": 4.475545180563304e-05,
      "loss": 1.2897,
      "step": 101000
    },
    {
      "epoch": 0.3149378071851323,
      "grad_norm": 0.6372010111808777,
      "learning_rate": 4.4752854594622113e-05,
      "loss": 1.275,
      "step": 101050
    },
    {
      "epoch": 0.315093639845788,
      "grad_norm": 0.6690057516098022,
      "learning_rate": 4.4750257383611186e-05,
      "loss": 1.2623,
      "step": 101100
    },
    {
      "epoch": 0.3152494725064437,
      "grad_norm": 0.7525289058685303,
      "learning_rate": 4.474766017260026e-05,
      "loss": 1.2886,
      "step": 101150
    },
    {
      "epoch": 0.31540530516709936,
      "grad_norm": 0.6467063426971436,
      "learning_rate": 4.474506296158933e-05,
      "loss": 1.3446,
      "step": 101200
    },
    {
      "epoch": 0.315561137827755,
      "grad_norm": 0.6268717050552368,
      "learning_rate": 4.47424657505784e-05,
      "loss": 1.2774,
      "step": 101250
    },
    {
      "epoch": 0.31571697048841074,
      "grad_norm": 0.5794708728790283,
      "learning_rate": 4.4739868539567476e-05,
      "loss": 1.2249,
      "step": 101300
    },
    {
      "epoch": 0.3158728031490664,
      "grad_norm": 0.6445797681808472,
      "learning_rate": 4.473727132855655e-05,
      "loss": 1.3303,
      "step": 101350
    },
    {
      "epoch": 0.31602863580972207,
      "grad_norm": 0.5410595536231995,
      "learning_rate": 4.4734674117545614e-05,
      "loss": 1.2741,
      "step": 101400
    },
    {
      "epoch": 0.3161844684703778,
      "grad_norm": 0.6442211270332336,
      "learning_rate": 4.473207690653469e-05,
      "loss": 1.2743,
      "step": 101450
    },
    {
      "epoch": 0.31634030113103345,
      "grad_norm": 0.671593964099884,
      "learning_rate": 4.4729479695523766e-05,
      "loss": 1.3106,
      "step": 101500
    },
    {
      "epoch": 0.3164961337916891,
      "grad_norm": 0.43838775157928467,
      "learning_rate": 4.472688248451283e-05,
      "loss": 1.3062,
      "step": 101550
    },
    {
      "epoch": 0.31665196645234484,
      "grad_norm": 0.5876198410987854,
      "learning_rate": 4.4724285273501904e-05,
      "loss": 1.2978,
      "step": 101600
    },
    {
      "epoch": 0.3168077991130005,
      "grad_norm": 0.6842820048332214,
      "learning_rate": 4.472168806249098e-05,
      "loss": 1.2916,
      "step": 101650
    },
    {
      "epoch": 0.31696363177365616,
      "grad_norm": 0.7091017365455627,
      "learning_rate": 4.471909085148005e-05,
      "loss": 1.2879,
      "step": 101700
    },
    {
      "epoch": 0.3171194644343119,
      "grad_norm": 0.6854441165924072,
      "learning_rate": 4.471649364046912e-05,
      "loss": 1.2622,
      "step": 101750
    },
    {
      "epoch": 0.31727529709496755,
      "grad_norm": 0.6092779040336609,
      "learning_rate": 4.471389642945819e-05,
      "loss": 1.3012,
      "step": 101800
    },
    {
      "epoch": 0.3174311297556232,
      "grad_norm": 0.693956732749939,
      "learning_rate": 4.471129921844727e-05,
      "loss": 1.2829,
      "step": 101850
    },
    {
      "epoch": 0.31758696241627893,
      "grad_norm": 0.606889545917511,
      "learning_rate": 4.470870200743634e-05,
      "loss": 1.2541,
      "step": 101900
    },
    {
      "epoch": 0.3177427950769346,
      "grad_norm": 0.7352508902549744,
      "learning_rate": 4.4706104796425405e-05,
      "loss": 1.2632,
      "step": 101950
    },
    {
      "epoch": 0.31789862773759026,
      "grad_norm": 0.6845191121101379,
      "learning_rate": 4.470350758541448e-05,
      "loss": 1.2175,
      "step": 102000
    },
    {
      "epoch": 0.318054460398246,
      "grad_norm": 0.540234386920929,
      "learning_rate": 4.470091037440356e-05,
      "loss": 1.3506,
      "step": 102050
    },
    {
      "epoch": 0.31821029305890164,
      "grad_norm": 0.6023526787757874,
      "learning_rate": 4.469831316339262e-05,
      "loss": 1.2446,
      "step": 102100
    },
    {
      "epoch": 0.3183661257195573,
      "grad_norm": 0.6849383115768433,
      "learning_rate": 4.4695715952381695e-05,
      "loss": 1.2755,
      "step": 102150
    },
    {
      "epoch": 0.31852195838021297,
      "grad_norm": 0.5520586371421814,
      "learning_rate": 4.469311874137077e-05,
      "loss": 1.2623,
      "step": 102200
    },
    {
      "epoch": 0.3186777910408687,
      "grad_norm": 0.6601771712303162,
      "learning_rate": 4.469052153035984e-05,
      "loss": 1.2498,
      "step": 102250
    },
    {
      "epoch": 0.31883362370152435,
      "grad_norm": 0.6500405073165894,
      "learning_rate": 4.468792431934891e-05,
      "loss": 1.293,
      "step": 102300
    },
    {
      "epoch": 0.31898945636218,
      "grad_norm": 0.5784962177276611,
      "learning_rate": 4.4685327108337985e-05,
      "loss": 1.2753,
      "step": 102350
    },
    {
      "epoch": 0.31914528902283573,
      "grad_norm": 0.5964756608009338,
      "learning_rate": 4.468272989732706e-05,
      "loss": 1.2794,
      "step": 102400
    },
    {
      "epoch": 0.3193011216834914,
      "grad_norm": 0.6511868238449097,
      "learning_rate": 4.468013268631613e-05,
      "loss": 1.3064,
      "step": 102450
    },
    {
      "epoch": 0.31945695434414706,
      "grad_norm": 0.5977102518081665,
      "learning_rate": 4.4677535475305196e-05,
      "loss": 1.321,
      "step": 102500
    },
    {
      "epoch": 0.3196127870048028,
      "grad_norm": 0.6623042821884155,
      "learning_rate": 4.4674938264294275e-05,
      "loss": 1.2813,
      "step": 102550
    },
    {
      "epoch": 0.31976861966545844,
      "grad_norm": 0.6557614803314209,
      "learning_rate": 4.467234105328335e-05,
      "loss": 1.2828,
      "step": 102600
    },
    {
      "epoch": 0.3199244523261141,
      "grad_norm": 0.5823073983192444,
      "learning_rate": 4.466974384227241e-05,
      "loss": 1.2617,
      "step": 102650
    },
    {
      "epoch": 0.3200802849867698,
      "grad_norm": 0.5394653677940369,
      "learning_rate": 4.4667146631261486e-05,
      "loss": 1.2351,
      "step": 102700
    },
    {
      "epoch": 0.3202361176474255,
      "grad_norm": 0.6467834711074829,
      "learning_rate": 4.4664549420250565e-05,
      "loss": 1.2953,
      "step": 102750
    },
    {
      "epoch": 0.32039195030808115,
      "grad_norm": 0.5045218467712402,
      "learning_rate": 4.466195220923963e-05,
      "loss": 1.3215,
      "step": 102800
    },
    {
      "epoch": 0.32054778296873687,
      "grad_norm": 0.6819407939910889,
      "learning_rate": 4.46593549982287e-05,
      "loss": 1.3086,
      "step": 102850
    },
    {
      "epoch": 0.32070361562939254,
      "grad_norm": 0.6775474548339844,
      "learning_rate": 4.4656757787217776e-05,
      "loss": 1.294,
      "step": 102900
    },
    {
      "epoch": 0.3208594482900482,
      "grad_norm": 0.5385020971298218,
      "learning_rate": 4.465416057620685e-05,
      "loss": 1.2354,
      "step": 102950
    },
    {
      "epoch": 0.3210152809507039,
      "grad_norm": 0.5733563899993896,
      "learning_rate": 4.465156336519592e-05,
      "loss": 1.276,
      "step": 103000
    },
    {
      "epoch": 0.3211711136113596,
      "grad_norm": 0.5564776659011841,
      "learning_rate": 4.464896615418499e-05,
      "loss": 1.2656,
      "step": 103050
    },
    {
      "epoch": 0.32132694627201525,
      "grad_norm": 0.5678614377975464,
      "learning_rate": 4.4646368943174066e-05,
      "loss": 1.2106,
      "step": 103100
    },
    {
      "epoch": 0.32148277893267097,
      "grad_norm": 0.6711746454238892,
      "learning_rate": 4.464377173216314e-05,
      "loss": 1.3245,
      "step": 103150
    },
    {
      "epoch": 0.32163861159332663,
      "grad_norm": 0.5655507445335388,
      "learning_rate": 4.4641174521152204e-05,
      "loss": 1.2702,
      "step": 103200
    },
    {
      "epoch": 0.3217944442539823,
      "grad_norm": 0.48463863134384155,
      "learning_rate": 4.4638577310141277e-05,
      "loss": 1.2562,
      "step": 103250
    },
    {
      "epoch": 0.321950276914638,
      "grad_norm": 0.6455929279327393,
      "learning_rate": 4.4635980099130356e-05,
      "loss": 1.2393,
      "step": 103300
    },
    {
      "epoch": 0.3221061095752937,
      "grad_norm": 0.6185289621353149,
      "learning_rate": 4.463338288811942e-05,
      "loss": 1.2708,
      "step": 103350
    },
    {
      "epoch": 0.32226194223594934,
      "grad_norm": 0.7384850978851318,
      "learning_rate": 4.4630785677108494e-05,
      "loss": 1.2548,
      "step": 103400
    },
    {
      "epoch": 0.322417774896605,
      "grad_norm": 0.6032554507255554,
      "learning_rate": 4.462818846609757e-05,
      "loss": 1.2781,
      "step": 103450
    },
    {
      "epoch": 0.3225736075572607,
      "grad_norm": 0.576097846031189,
      "learning_rate": 4.462559125508664e-05,
      "loss": 1.2734,
      "step": 103500
    },
    {
      "epoch": 0.3227294402179164,
      "grad_norm": 0.590056300163269,
      "learning_rate": 4.462299404407571e-05,
      "loss": 1.308,
      "step": 103550
    },
    {
      "epoch": 0.32288527287857205,
      "grad_norm": 0.5991587042808533,
      "learning_rate": 4.4620396833064784e-05,
      "loss": 1.2633,
      "step": 103600
    },
    {
      "epoch": 0.32304110553922777,
      "grad_norm": 0.582976758480072,
      "learning_rate": 4.4617799622053857e-05,
      "loss": 1.2648,
      "step": 103650
    },
    {
      "epoch": 0.32319693819988343,
      "grad_norm": 0.6358640789985657,
      "learning_rate": 4.461520241104293e-05,
      "loss": 1.2985,
      "step": 103700
    },
    {
      "epoch": 0.3233527708605391,
      "grad_norm": 0.45737212896347046,
      "learning_rate": 4.4612605200032e-05,
      "loss": 1.2741,
      "step": 103750
    },
    {
      "epoch": 0.3235086035211948,
      "grad_norm": 0.5955425500869751,
      "learning_rate": 4.4610007989021074e-05,
      "loss": 1.2996,
      "step": 103800
    },
    {
      "epoch": 0.3236644361818505,
      "grad_norm": 0.7190332412719727,
      "learning_rate": 4.4607410778010147e-05,
      "loss": 1.2954,
      "step": 103850
    },
    {
      "epoch": 0.32382026884250614,
      "grad_norm": 0.6888150572776794,
      "learning_rate": 4.460486551121943e-05,
      "loss": 1.2949,
      "step": 103900
    },
    {
      "epoch": 0.32397610150316186,
      "grad_norm": 0.7388259172439575,
      "learning_rate": 4.4602268300208504e-05,
      "loss": 1.3047,
      "step": 103950
    },
    {
      "epoch": 0.3241319341638175,
      "grad_norm": 0.6361538767814636,
      "learning_rate": 4.459967108919758e-05,
      "loss": 1.2491,
      "step": 104000
    },
    {
      "epoch": 0.3242877668244732,
      "grad_norm": 0.4826432168483734,
      "learning_rate": 4.459707387818665e-05,
      "loss": 1.2624,
      "step": 104050
    },
    {
      "epoch": 0.3244435994851289,
      "grad_norm": 0.52118980884552,
      "learning_rate": 4.459447666717572e-05,
      "loss": 1.2382,
      "step": 104100
    },
    {
      "epoch": 0.32459943214578457,
      "grad_norm": 0.6537537574768066,
      "learning_rate": 4.4591879456164794e-05,
      "loss": 1.2843,
      "step": 104150
    },
    {
      "epoch": 0.32475526480644024,
      "grad_norm": 0.6419819593429565,
      "learning_rate": 4.458928224515387e-05,
      "loss": 1.2881,
      "step": 104200
    },
    {
      "epoch": 0.32491109746709596,
      "grad_norm": 0.6644263863563538,
      "learning_rate": 4.458668503414294e-05,
      "loss": 1.3026,
      "step": 104250
    },
    {
      "epoch": 0.3250669301277516,
      "grad_norm": 0.5143871307373047,
      "learning_rate": 4.458408782313201e-05,
      "loss": 1.3003,
      "step": 104300
    },
    {
      "epoch": 0.3252227627884073,
      "grad_norm": 0.6009998917579651,
      "learning_rate": 4.458149061212108e-05,
      "loss": 1.2931,
      "step": 104350
    },
    {
      "epoch": 0.325378595449063,
      "grad_norm": 0.6847239136695862,
      "learning_rate": 4.457889340111016e-05,
      "loss": 1.2345,
      "step": 104400
    },
    {
      "epoch": 0.32553442810971867,
      "grad_norm": 0.7290115356445312,
      "learning_rate": 4.457629619009922e-05,
      "loss": 1.2354,
      "step": 104450
    },
    {
      "epoch": 0.32569026077037433,
      "grad_norm": 0.5662927627563477,
      "learning_rate": 4.4573698979088295e-05,
      "loss": 1.2459,
      "step": 104500
    },
    {
      "epoch": 0.32584609343103005,
      "grad_norm": 0.7260034680366516,
      "learning_rate": 4.4571101768077374e-05,
      "loss": 1.2724,
      "step": 104550
    },
    {
      "epoch": 0.3260019260916857,
      "grad_norm": 0.6172962784767151,
      "learning_rate": 4.456850455706644e-05,
      "loss": 1.2257,
      "step": 104600
    },
    {
      "epoch": 0.3261577587523414,
      "grad_norm": 0.6030847430229187,
      "learning_rate": 4.456590734605551e-05,
      "loss": 1.3001,
      "step": 104650
    },
    {
      "epoch": 0.3263135914129971,
      "grad_norm": 0.6361303329467773,
      "learning_rate": 4.4563310135044585e-05,
      "loss": 1.243,
      "step": 104700
    },
    {
      "epoch": 0.32646942407365276,
      "grad_norm": 0.7363646030426025,
      "learning_rate": 4.456071292403366e-05,
      "loss": 1.2797,
      "step": 104750
    },
    {
      "epoch": 0.3266252567343084,
      "grad_norm": 0.6007058024406433,
      "learning_rate": 4.455811571302273e-05,
      "loss": 1.2664,
      "step": 104800
    },
    {
      "epoch": 0.3267810893949641,
      "grad_norm": 0.4273405969142914,
      "learning_rate": 4.45555185020118e-05,
      "loss": 1.2426,
      "step": 104850
    },
    {
      "epoch": 0.3269369220556198,
      "grad_norm": 0.6700474619865417,
      "learning_rate": 4.4552921291000875e-05,
      "loss": 1.2711,
      "step": 104900
    },
    {
      "epoch": 0.32709275471627547,
      "grad_norm": 0.6443252563476562,
      "learning_rate": 4.455032407998995e-05,
      "loss": 1.3054,
      "step": 104950
    },
    {
      "epoch": 0.32724858737693113,
      "grad_norm": 0.7288479804992676,
      "learning_rate": 4.454772686897902e-05,
      "loss": 1.2921,
      "step": 105000
    },
    {
      "epoch": 0.32740442003758685,
      "grad_norm": 0.6198641061782837,
      "learning_rate": 4.4545129657968086e-05,
      "loss": 1.2452,
      "step": 105050
    },
    {
      "epoch": 0.3275602526982425,
      "grad_norm": 0.5289993286132812,
      "learning_rate": 4.4542532446957165e-05,
      "loss": 1.2957,
      "step": 105100
    },
    {
      "epoch": 0.3277160853588982,
      "grad_norm": 0.5066801905632019,
      "learning_rate": 4.453993523594623e-05,
      "loss": 1.2427,
      "step": 105150
    },
    {
      "epoch": 0.3278719180195539,
      "grad_norm": 0.5546957850456238,
      "learning_rate": 4.4537338024935303e-05,
      "loss": 1.2686,
      "step": 105200
    },
    {
      "epoch": 0.32802775068020956,
      "grad_norm": 0.7022430896759033,
      "learning_rate": 4.4534740813924376e-05,
      "loss": 1.2704,
      "step": 105250
    },
    {
      "epoch": 0.3281835833408652,
      "grad_norm": 0.5387314558029175,
      "learning_rate": 4.453214360291345e-05,
      "loss": 1.2655,
      "step": 105300
    },
    {
      "epoch": 0.32833941600152095,
      "grad_norm": 0.5360340476036072,
      "learning_rate": 4.452954639190252e-05,
      "loss": 1.2852,
      "step": 105350
    },
    {
      "epoch": 0.3284952486621766,
      "grad_norm": 0.7305423617362976,
      "learning_rate": 4.4526949180891593e-05,
      "loss": 1.2919,
      "step": 105400
    },
    {
      "epoch": 0.3286510813228323,
      "grad_norm": 0.5766274333000183,
      "learning_rate": 4.4524351969880666e-05,
      "loss": 1.3095,
      "step": 105450
    },
    {
      "epoch": 0.328806913983488,
      "grad_norm": 0.5716481804847717,
      "learning_rate": 4.452175475886974e-05,
      "loss": 1.3326,
      "step": 105500
    },
    {
      "epoch": 0.32896274664414366,
      "grad_norm": 0.5349916219711304,
      "learning_rate": 4.451915754785881e-05,
      "loss": 1.3473,
      "step": 105550
    },
    {
      "epoch": 0.3291185793047993,
      "grad_norm": 0.6268051862716675,
      "learning_rate": 4.451656033684788e-05,
      "loss": 1.2698,
      "step": 105600
    },
    {
      "epoch": 0.32927441196545504,
      "grad_norm": 0.5047663450241089,
      "learning_rate": 4.4513963125836956e-05,
      "loss": 1.3416,
      "step": 105650
    },
    {
      "epoch": 0.3294302446261107,
      "grad_norm": 0.859217643737793,
      "learning_rate": 4.451136591482603e-05,
      "loss": 1.2052,
      "step": 105700
    },
    {
      "epoch": 0.32958607728676637,
      "grad_norm": 0.6013326048851013,
      "learning_rate": 4.4508768703815094e-05,
      "loss": 1.2662,
      "step": 105750
    },
    {
      "epoch": 0.3297419099474221,
      "grad_norm": 0.6152858138084412,
      "learning_rate": 4.4506171492804173e-05,
      "loss": 1.2856,
      "step": 105800
    },
    {
      "epoch": 0.32989774260807775,
      "grad_norm": 0.6372882127761841,
      "learning_rate": 4.450357428179324e-05,
      "loss": 1.3086,
      "step": 105850
    },
    {
      "epoch": 0.3300535752687334,
      "grad_norm": 0.6351324319839478,
      "learning_rate": 4.450097707078231e-05,
      "loss": 1.2725,
      "step": 105900
    },
    {
      "epoch": 0.33020940792938913,
      "grad_norm": 0.611475944519043,
      "learning_rate": 4.4498379859771384e-05,
      "loss": 1.2605,
      "step": 105950
    },
    {
      "epoch": 0.3303652405900448,
      "grad_norm": 0.6757643818855286,
      "learning_rate": 4.449578264876046e-05,
      "loss": 1.3327,
      "step": 106000
    },
    {
      "epoch": 0.33052107325070046,
      "grad_norm": 0.7760528326034546,
      "learning_rate": 4.449318543774953e-05,
      "loss": 1.2705,
      "step": 106050
    },
    {
      "epoch": 0.3306769059113561,
      "grad_norm": 0.500226616859436,
      "learning_rate": 4.44905882267386e-05,
      "loss": 1.2981,
      "step": 106100
    },
    {
      "epoch": 0.33083273857201184,
      "grad_norm": 0.5212945342063904,
      "learning_rate": 4.4487991015727674e-05,
      "loss": 1.3052,
      "step": 106150
    },
    {
      "epoch": 0.3309885712326675,
      "grad_norm": 0.6872866749763489,
      "learning_rate": 4.448539380471675e-05,
      "loss": 1.2903,
      "step": 106200
    },
    {
      "epoch": 0.33114440389332317,
      "grad_norm": 0.6403462290763855,
      "learning_rate": 4.448279659370582e-05,
      "loss": 1.3017,
      "step": 106250
    },
    {
      "epoch": 0.3313002365539789,
      "grad_norm": 0.6442083120346069,
      "learning_rate": 4.4480199382694885e-05,
      "loss": 1.2589,
      "step": 106300
    },
    {
      "epoch": 0.33145606921463455,
      "grad_norm": 0.5425065159797668,
      "learning_rate": 4.4477602171683964e-05,
      "loss": 1.247,
      "step": 106350
    },
    {
      "epoch": 0.3316119018752902,
      "grad_norm": 0.5714404582977295,
      "learning_rate": 4.447500496067304e-05,
      "loss": 1.215,
      "step": 106400
    },
    {
      "epoch": 0.33176773453594594,
      "grad_norm": 0.47033190727233887,
      "learning_rate": 4.44724077496621e-05,
      "loss": 1.2679,
      "step": 106450
    },
    {
      "epoch": 0.3319235671966016,
      "grad_norm": 0.7768062949180603,
      "learning_rate": 4.4469810538651175e-05,
      "loss": 1.2735,
      "step": 106500
    },
    {
      "epoch": 0.33207939985725726,
      "grad_norm": 0.5927753448486328,
      "learning_rate": 4.446721332764025e-05,
      "loss": 1.2838,
      "step": 106550
    },
    {
      "epoch": 0.332235232517913,
      "grad_norm": 0.49736157059669495,
      "learning_rate": 4.446461611662932e-05,
      "loss": 1.2853,
      "step": 106600
    },
    {
      "epoch": 0.33239106517856865,
      "grad_norm": 0.43462467193603516,
      "learning_rate": 4.446201890561839e-05,
      "loss": 1.2605,
      "step": 106650
    },
    {
      "epoch": 0.3325468978392243,
      "grad_norm": 0.6487023234367371,
      "learning_rate": 4.4459421694607465e-05,
      "loss": 1.2473,
      "step": 106700
    },
    {
      "epoch": 0.33270273049988003,
      "grad_norm": 0.583764374256134,
      "learning_rate": 4.445682448359654e-05,
      "loss": 1.2963,
      "step": 106750
    },
    {
      "epoch": 0.3328585631605357,
      "grad_norm": 0.6755040287971497,
      "learning_rate": 4.445422727258561e-05,
      "loss": 1.2577,
      "step": 106800
    },
    {
      "epoch": 0.33301439582119136,
      "grad_norm": 0.48181986808776855,
      "learning_rate": 4.4451630061574676e-05,
      "loss": 1.321,
      "step": 106850
    },
    {
      "epoch": 0.3331702284818471,
      "grad_norm": 0.5579139590263367,
      "learning_rate": 4.4449032850563755e-05,
      "loss": 1.3087,
      "step": 106900
    },
    {
      "epoch": 0.33332606114250274,
      "grad_norm": 0.6655833721160889,
      "learning_rate": 4.444643563955283e-05,
      "loss": 1.2943,
      "step": 106950
    },
    {
      "epoch": 0.3334818938031584,
      "grad_norm": 0.6189484596252441,
      "learning_rate": 4.444389037276211e-05,
      "loss": 1.2708,
      "step": 107000
    },
    {
      "epoch": 0.3336377264638141,
      "grad_norm": 0.5205338001251221,
      "learning_rate": 4.4441293161751185e-05,
      "loss": 1.2583,
      "step": 107050
    },
    {
      "epoch": 0.3337935591244698,
      "grad_norm": 0.5519961714744568,
      "learning_rate": 4.443869595074026e-05,
      "loss": 1.3064,
      "step": 107100
    },
    {
      "epoch": 0.33394939178512545,
      "grad_norm": 0.5291520953178406,
      "learning_rate": 4.443609873972933e-05,
      "loss": 1.2822,
      "step": 107150
    },
    {
      "epoch": 0.33410522444578117,
      "grad_norm": 0.7634178996086121,
      "learning_rate": 4.44335015287184e-05,
      "loss": 1.2618,
      "step": 107200
    },
    {
      "epoch": 0.33426105710643683,
      "grad_norm": 0.6161315441131592,
      "learning_rate": 4.4430904317707475e-05,
      "loss": 1.2519,
      "step": 107250
    },
    {
      "epoch": 0.3344168897670925,
      "grad_norm": 0.6298354268074036,
      "learning_rate": 4.442830710669655e-05,
      "loss": 1.2684,
      "step": 107300
    },
    {
      "epoch": 0.33457272242774816,
      "grad_norm": 0.5324782133102417,
      "learning_rate": 4.442570989568562e-05,
      "loss": 1.2878,
      "step": 107350
    },
    {
      "epoch": 0.3347285550884039,
      "grad_norm": 0.5226105451583862,
      "learning_rate": 4.4423112684674686e-05,
      "loss": 1.2395,
      "step": 107400
    },
    {
      "epoch": 0.33488438774905954,
      "grad_norm": 0.4177846312522888,
      "learning_rate": 4.4420515473663765e-05,
      "loss": 1.2621,
      "step": 107450
    },
    {
      "epoch": 0.3350402204097152,
      "grad_norm": 0.5460736751556396,
      "learning_rate": 4.441791826265284e-05,
      "loss": 1.2955,
      "step": 107500
    },
    {
      "epoch": 0.3351960530703709,
      "grad_norm": 0.5842162370681763,
      "learning_rate": 4.4415321051641904e-05,
      "loss": 1.2474,
      "step": 107550
    },
    {
      "epoch": 0.3353518857310266,
      "grad_norm": 0.6069103479385376,
      "learning_rate": 4.4412723840630976e-05,
      "loss": 1.282,
      "step": 107600
    },
    {
      "epoch": 0.33550771839168225,
      "grad_norm": 0.5901483297348022,
      "learning_rate": 4.4410126629620055e-05,
      "loss": 1.2881,
      "step": 107650
    },
    {
      "epoch": 0.33566355105233797,
      "grad_norm": 0.6028290390968323,
      "learning_rate": 4.440752941860912e-05,
      "loss": 1.3052,
      "step": 107700
    },
    {
      "epoch": 0.33581938371299364,
      "grad_norm": 0.5455816984176636,
      "learning_rate": 4.4404932207598194e-05,
      "loss": 1.3022,
      "step": 107750
    },
    {
      "epoch": 0.3359752163736493,
      "grad_norm": 0.8271147012710571,
      "learning_rate": 4.4402334996587266e-05,
      "loss": 1.2684,
      "step": 107800
    },
    {
      "epoch": 0.336131049034305,
      "grad_norm": 0.6111274361610413,
      "learning_rate": 4.439973778557634e-05,
      "loss": 1.3189,
      "step": 107850
    },
    {
      "epoch": 0.3362868816949607,
      "grad_norm": 0.5184516310691833,
      "learning_rate": 4.439714057456541e-05,
      "loss": 1.3036,
      "step": 107900
    },
    {
      "epoch": 0.33644271435561635,
      "grad_norm": 0.5219009518623352,
      "learning_rate": 4.4394543363554484e-05,
      "loss": 1.3238,
      "step": 107950
    },
    {
      "epoch": 0.33659854701627206,
      "grad_norm": 0.5387771725654602,
      "learning_rate": 4.4391946152543556e-05,
      "loss": 1.2459,
      "step": 108000
    },
    {
      "epoch": 0.33675437967692773,
      "grad_norm": 0.5909059047698975,
      "learning_rate": 4.438934894153263e-05,
      "loss": 1.2897,
      "step": 108050
    },
    {
      "epoch": 0.3369102123375834,
      "grad_norm": 0.7278880476951599,
      "learning_rate": 4.4386751730521694e-05,
      "loss": 1.3013,
      "step": 108100
    },
    {
      "epoch": 0.3370660449982391,
      "grad_norm": 0.7954710125923157,
      "learning_rate": 4.4384154519510774e-05,
      "loss": 1.2765,
      "step": 108150
    },
    {
      "epoch": 0.3372218776588948,
      "grad_norm": 0.6399828791618347,
      "learning_rate": 4.4381557308499846e-05,
      "loss": 1.3417,
      "step": 108200
    },
    {
      "epoch": 0.33737771031955044,
      "grad_norm": 0.7876444458961487,
      "learning_rate": 4.437896009748891e-05,
      "loss": 1.2091,
      "step": 108250
    },
    {
      "epoch": 0.33753354298020616,
      "grad_norm": 0.5800461173057556,
      "learning_rate": 4.4376362886477984e-05,
      "loss": 1.3083,
      "step": 108300
    },
    {
      "epoch": 0.3376893756408618,
      "grad_norm": 0.5127671957015991,
      "learning_rate": 4.4373765675467064e-05,
      "loss": 1.2969,
      "step": 108350
    },
    {
      "epoch": 0.3378452083015175,
      "grad_norm": 0.6025092005729675,
      "learning_rate": 4.437116846445613e-05,
      "loss": 1.3245,
      "step": 108400
    },
    {
      "epoch": 0.3380010409621732,
      "grad_norm": 0.7275633811950684,
      "learning_rate": 4.43685712534452e-05,
      "loss": 1.3014,
      "step": 108450
    },
    {
      "epoch": 0.33815687362282887,
      "grad_norm": 0.5386354327201843,
      "learning_rate": 4.4365974042434274e-05,
      "loss": 1.3041,
      "step": 108500
    },
    {
      "epoch": 0.33831270628348453,
      "grad_norm": 0.685849130153656,
      "learning_rate": 4.4363428775643567e-05,
      "loss": 1.2821,
      "step": 108550
    },
    {
      "epoch": 0.33846853894414025,
      "grad_norm": 0.4916180372238159,
      "learning_rate": 4.436083156463264e-05,
      "loss": 1.3095,
      "step": 108600
    },
    {
      "epoch": 0.3386243716047959,
      "grad_norm": 0.6783302426338196,
      "learning_rate": 4.435823435362171e-05,
      "loss": 1.2911,
      "step": 108650
    },
    {
      "epoch": 0.3387802042654516,
      "grad_norm": 0.5979049801826477,
      "learning_rate": 4.435563714261078e-05,
      "loss": 1.3134,
      "step": 108700
    },
    {
      "epoch": 0.33893603692610724,
      "grad_norm": 0.5313169360160828,
      "learning_rate": 4.4353039931599857e-05,
      "loss": 1.3164,
      "step": 108750
    },
    {
      "epoch": 0.33909186958676296,
      "grad_norm": 0.45856305956840515,
      "learning_rate": 4.435044272058892e-05,
      "loss": 1.2733,
      "step": 108800
    },
    {
      "epoch": 0.3392477022474186,
      "grad_norm": 0.5142455697059631,
      "learning_rate": 4.4347845509577995e-05,
      "loss": 1.2846,
      "step": 108850
    },
    {
      "epoch": 0.3394035349080743,
      "grad_norm": 0.6858980059623718,
      "learning_rate": 4.4345248298567074e-05,
      "loss": 1.2073,
      "step": 108900
    },
    {
      "epoch": 0.33955936756873,
      "grad_norm": 0.6039164066314697,
      "learning_rate": 4.434265108755614e-05,
      "loss": 1.2883,
      "step": 108950
    },
    {
      "epoch": 0.33971520022938567,
      "grad_norm": 0.666096031665802,
      "learning_rate": 4.434005387654521e-05,
      "loss": 1.2528,
      "step": 109000
    },
    {
      "epoch": 0.33987103289004134,
      "grad_norm": 0.6132872700691223,
      "learning_rate": 4.4337456665534285e-05,
      "loss": 1.3056,
      "step": 109050
    },
    {
      "epoch": 0.34002686555069705,
      "grad_norm": 0.6795283555984497,
      "learning_rate": 4.433485945452336e-05,
      "loss": 1.2733,
      "step": 109100
    },
    {
      "epoch": 0.3401826982113527,
      "grad_norm": 0.7336065173149109,
      "learning_rate": 4.433226224351243e-05,
      "loss": 1.2569,
      "step": 109150
    },
    {
      "epoch": 0.3403385308720084,
      "grad_norm": 0.5362191200256348,
      "learning_rate": 4.43296650325015e-05,
      "loss": 1.2708,
      "step": 109200
    },
    {
      "epoch": 0.3404943635326641,
      "grad_norm": 0.5475890636444092,
      "learning_rate": 4.4327067821490575e-05,
      "loss": 1.2932,
      "step": 109250
    },
    {
      "epoch": 0.34065019619331977,
      "grad_norm": 0.5475131273269653,
      "learning_rate": 4.432447061047965e-05,
      "loss": 1.3113,
      "step": 109300
    },
    {
      "epoch": 0.34080602885397543,
      "grad_norm": 0.5205106139183044,
      "learning_rate": 4.432187339946871e-05,
      "loss": 1.272,
      "step": 109350
    },
    {
      "epoch": 0.34096186151463115,
      "grad_norm": 0.6044150590896606,
      "learning_rate": 4.4319276188457786e-05,
      "loss": 1.2927,
      "step": 109400
    },
    {
      "epoch": 0.3411176941752868,
      "grad_norm": 0.5836431980133057,
      "learning_rate": 4.4316678977446865e-05,
      "loss": 1.2664,
      "step": 109450
    },
    {
      "epoch": 0.3412735268359425,
      "grad_norm": 0.4727257788181305,
      "learning_rate": 4.431408176643593e-05,
      "loss": 1.2628,
      "step": 109500
    },
    {
      "epoch": 0.3414293594965982,
      "grad_norm": 0.7636374235153198,
      "learning_rate": 4.4311484555425e-05,
      "loss": 1.2694,
      "step": 109550
    },
    {
      "epoch": 0.34158519215725386,
      "grad_norm": 0.8061033487319946,
      "learning_rate": 4.4308887344414076e-05,
      "loss": 1.2736,
      "step": 109600
    },
    {
      "epoch": 0.3417410248179095,
      "grad_norm": 0.6514426469802856,
      "learning_rate": 4.430629013340315e-05,
      "loss": 1.2485,
      "step": 109650
    },
    {
      "epoch": 0.34189685747856524,
      "grad_norm": 0.5553919076919556,
      "learning_rate": 4.430369292239222e-05,
      "loss": 1.2701,
      "step": 109700
    },
    {
      "epoch": 0.3420526901392209,
      "grad_norm": 0.5434969067573547,
      "learning_rate": 4.430109571138129e-05,
      "loss": 1.2863,
      "step": 109750
    },
    {
      "epoch": 0.34220852279987657,
      "grad_norm": 0.6124758720397949,
      "learning_rate": 4.4298498500370366e-05,
      "loss": 1.2809,
      "step": 109800
    },
    {
      "epoch": 0.3423643554605323,
      "grad_norm": 0.6127581596374512,
      "learning_rate": 4.429590128935944e-05,
      "loss": 1.316,
      "step": 109850
    },
    {
      "epoch": 0.34252018812118795,
      "grad_norm": 0.5340418219566345,
      "learning_rate": 4.429330407834851e-05,
      "loss": 1.2707,
      "step": 109900
    },
    {
      "epoch": 0.3426760207818436,
      "grad_norm": 0.5414122939109802,
      "learning_rate": 4.4290706867337576e-05,
      "loss": 1.3204,
      "step": 109950
    },
    {
      "epoch": 0.3428318534424993,
      "grad_norm": 0.6009647846221924,
      "learning_rate": 4.4288109656326656e-05,
      "loss": 1.2812,
      "step": 110000
    },
    {
      "epoch": 0.342987686103155,
      "grad_norm": 0.5372095704078674,
      "learning_rate": 4.428551244531572e-05,
      "loss": 1.2808,
      "step": 110050
    },
    {
      "epoch": 0.34314351876381066,
      "grad_norm": 0.5249205231666565,
      "learning_rate": 4.4282915234304794e-05,
      "loss": 1.344,
      "step": 110100
    },
    {
      "epoch": 0.3432993514244663,
      "grad_norm": 0.5190884470939636,
      "learning_rate": 4.428031802329387e-05,
      "loss": 1.2713,
      "step": 110150
    },
    {
      "epoch": 0.34345518408512205,
      "grad_norm": 0.6011179089546204,
      "learning_rate": 4.427772081228294e-05,
      "loss": 1.282,
      "step": 110200
    },
    {
      "epoch": 0.3436110167457777,
      "grad_norm": 0.6341668367385864,
      "learning_rate": 4.427512360127201e-05,
      "loss": 1.2871,
      "step": 110250
    },
    {
      "epoch": 0.3437668494064334,
      "grad_norm": 0.5822368264198303,
      "learning_rate": 4.4272526390261084e-05,
      "loss": 1.3234,
      "step": 110300
    },
    {
      "epoch": 0.3439226820670891,
      "grad_norm": 0.47275039553642273,
      "learning_rate": 4.4269929179250156e-05,
      "loss": 1.2733,
      "step": 110350
    },
    {
      "epoch": 0.34407851472774476,
      "grad_norm": 0.5043335556983948,
      "learning_rate": 4.426733196823923e-05,
      "loss": 1.3332,
      "step": 110400
    },
    {
      "epoch": 0.3442343473884004,
      "grad_norm": 1.0886439085006714,
      "learning_rate": 4.42647347572283e-05,
      "loss": 1.2823,
      "step": 110450
    },
    {
      "epoch": 0.34439018004905614,
      "grad_norm": 0.7395328283309937,
      "learning_rate": 4.426213754621737e-05,
      "loss": 1.2857,
      "step": 110500
    },
    {
      "epoch": 0.3445460127097118,
      "grad_norm": 0.6749351024627686,
      "learning_rate": 4.4259540335206446e-05,
      "loss": 1.2533,
      "step": 110550
    },
    {
      "epoch": 0.34470184537036747,
      "grad_norm": 0.6646203994750977,
      "learning_rate": 4.425694312419552e-05,
      "loss": 1.2232,
      "step": 110600
    },
    {
      "epoch": 0.3448576780310232,
      "grad_norm": 0.5490636229515076,
      "learning_rate": 4.4254345913184585e-05,
      "loss": 1.2804,
      "step": 110650
    },
    {
      "epoch": 0.34501351069167885,
      "grad_norm": 0.6342117190361023,
      "learning_rate": 4.4251748702173664e-05,
      "loss": 1.2839,
      "step": 110700
    },
    {
      "epoch": 0.3451693433523345,
      "grad_norm": 0.7436380386352539,
      "learning_rate": 4.424915149116273e-05,
      "loss": 1.19,
      "step": 110750
    },
    {
      "epoch": 0.34532517601299023,
      "grad_norm": 0.42988523840904236,
      "learning_rate": 4.42465542801518e-05,
      "loss": 1.3071,
      "step": 110800
    },
    {
      "epoch": 0.3454810086736459,
      "grad_norm": 0.8467756509780884,
      "learning_rate": 4.4243957069140875e-05,
      "loss": 1.2628,
      "step": 110850
    },
    {
      "epoch": 0.34563684133430156,
      "grad_norm": 0.6491791605949402,
      "learning_rate": 4.424135985812995e-05,
      "loss": 1.3574,
      "step": 110900
    },
    {
      "epoch": 0.3457926739949573,
      "grad_norm": 0.613528847694397,
      "learning_rate": 4.423876264711902e-05,
      "loss": 1.2945,
      "step": 110950
    },
    {
      "epoch": 0.34594850665561294,
      "grad_norm": 0.5583953261375427,
      "learning_rate": 4.423616543610809e-05,
      "loss": 1.2968,
      "step": 111000
    },
    {
      "epoch": 0.3461043393162686,
      "grad_norm": 0.5787559747695923,
      "learning_rate": 4.4233568225097165e-05,
      "loss": 1.3063,
      "step": 111050
    },
    {
      "epoch": 0.3462601719769243,
      "grad_norm": 0.700124204158783,
      "learning_rate": 4.423097101408624e-05,
      "loss": 1.2793,
      "step": 111100
    },
    {
      "epoch": 0.34641600463758,
      "grad_norm": 0.47414299845695496,
      "learning_rate": 4.422837380307531e-05,
      "loss": 1.2458,
      "step": 111150
    },
    {
      "epoch": 0.34657183729823565,
      "grad_norm": 0.5962978601455688,
      "learning_rate": 4.4225776592064375e-05,
      "loss": 1.3249,
      "step": 111200
    },
    {
      "epoch": 0.34672766995889137,
      "grad_norm": 0.47730979323387146,
      "learning_rate": 4.4223179381053455e-05,
      "loss": 1.3087,
      "step": 111250
    },
    {
      "epoch": 0.34688350261954704,
      "grad_norm": 0.6633914113044739,
      "learning_rate": 4.422058217004253e-05,
      "loss": 1.2875,
      "step": 111300
    },
    {
      "epoch": 0.3470393352802027,
      "grad_norm": 0.6535377502441406,
      "learning_rate": 4.421798495903159e-05,
      "loss": 1.2876,
      "step": 111350
    },
    {
      "epoch": 0.34719516794085836,
      "grad_norm": 0.6824945211410522,
      "learning_rate": 4.421538774802067e-05,
      "loss": 1.3084,
      "step": 111400
    },
    {
      "epoch": 0.3473510006015141,
      "grad_norm": 0.6465268135070801,
      "learning_rate": 4.421279053700974e-05,
      "loss": 1.2953,
      "step": 111450
    },
    {
      "epoch": 0.34750683326216975,
      "grad_norm": 0.584234356880188,
      "learning_rate": 4.421019332599881e-05,
      "loss": 1.2418,
      "step": 111500
    },
    {
      "epoch": 0.3476626659228254,
      "grad_norm": 0.6138319969177246,
      "learning_rate": 4.420759611498788e-05,
      "loss": 1.2841,
      "step": 111550
    },
    {
      "epoch": 0.34781849858348113,
      "grad_norm": 0.5693316459655762,
      "learning_rate": 4.4204998903976955e-05,
      "loss": 1.2838,
      "step": 111600
    },
    {
      "epoch": 0.3479743312441368,
      "grad_norm": 0.597461998462677,
      "learning_rate": 4.420240169296603e-05,
      "loss": 1.2671,
      "step": 111650
    },
    {
      "epoch": 0.34813016390479246,
      "grad_norm": 0.5571076273918152,
      "learning_rate": 4.41998044819551e-05,
      "loss": 1.2601,
      "step": 111700
    },
    {
      "epoch": 0.3482859965654482,
      "grad_norm": 0.5974952578544617,
      "learning_rate": 4.4197207270944166e-05,
      "loss": 1.2886,
      "step": 111750
    },
    {
      "epoch": 0.34844182922610384,
      "grad_norm": 0.6387823820114136,
      "learning_rate": 4.4194610059933245e-05,
      "loss": 1.2737,
      "step": 111800
    },
    {
      "epoch": 0.3485976618867595,
      "grad_norm": 0.5534796118736267,
      "learning_rate": 4.419201284892232e-05,
      "loss": 1.3038,
      "step": 111850
    },
    {
      "epoch": 0.3487534945474152,
      "grad_norm": 0.555801510810852,
      "learning_rate": 4.4189415637911384e-05,
      "loss": 1.2697,
      "step": 111900
    },
    {
      "epoch": 0.3489093272080709,
      "grad_norm": 0.6210821866989136,
      "learning_rate": 4.418681842690046e-05,
      "loss": 1.3013,
      "step": 111950
    },
    {
      "epoch": 0.34906515986872655,
      "grad_norm": 0.5703189373016357,
      "learning_rate": 4.418422121588953e-05,
      "loss": 1.3087,
      "step": 112000
    },
    {
      "epoch": 0.34922099252938227,
      "grad_norm": 0.6166641116142273,
      "learning_rate": 4.41816240048786e-05,
      "loss": 1.2211,
      "step": 112050
    },
    {
      "epoch": 0.34937682519003793,
      "grad_norm": 0.6784728169441223,
      "learning_rate": 4.4179026793867674e-05,
      "loss": 1.2556,
      "step": 112100
    },
    {
      "epoch": 0.3495326578506936,
      "grad_norm": 0.5953404307365417,
      "learning_rate": 4.4176429582856746e-05,
      "loss": 1.3236,
      "step": 112150
    },
    {
      "epoch": 0.3496884905113493,
      "grad_norm": 0.6170783638954163,
      "learning_rate": 4.417383237184582e-05,
      "loss": 1.3329,
      "step": 112200
    },
    {
      "epoch": 0.349844323172005,
      "grad_norm": 0.5582447052001953,
      "learning_rate": 4.417123516083489e-05,
      "loss": 1.2717,
      "step": 112250
    },
    {
      "epoch": 0.35000015583266064,
      "grad_norm": 0.6063812971115112,
      "learning_rate": 4.4168637949823964e-05,
      "loss": 1.2804,
      "step": 112300
    },
    {
      "epoch": 0.35015598849331636,
      "grad_norm": 0.503989577293396,
      "learning_rate": 4.4166040738813036e-05,
      "loss": 1.2762,
      "step": 112350
    },
    {
      "epoch": 0.350311821153972,
      "grad_norm": 0.45014339685440063,
      "learning_rate": 4.416344352780211e-05,
      "loss": 1.3253,
      "step": 112400
    },
    {
      "epoch": 0.3504676538146277,
      "grad_norm": 0.7538781762123108,
      "learning_rate": 4.4160846316791174e-05,
      "loss": 1.3158,
      "step": 112450
    },
    {
      "epoch": 0.3506234864752834,
      "grad_norm": 0.5688425302505493,
      "learning_rate": 4.4158249105780254e-05,
      "loss": 1.3545,
      "step": 112500
    },
    {
      "epoch": 0.35077931913593907,
      "grad_norm": 0.54268878698349,
      "learning_rate": 4.4155651894769326e-05,
      "loss": 1.2406,
      "step": 112550
    },
    {
      "epoch": 0.35093515179659474,
      "grad_norm": 0.5337616205215454,
      "learning_rate": 4.415305468375839e-05,
      "loss": 1.2942,
      "step": 112600
    },
    {
      "epoch": 0.3510909844572504,
      "grad_norm": 0.5461151003837585,
      "learning_rate": 4.415045747274747e-05,
      "loss": 1.317,
      "step": 112650
    },
    {
      "epoch": 0.3512468171179061,
      "grad_norm": 0.5979509949684143,
      "learning_rate": 4.414786026173654e-05,
      "loss": 1.242,
      "step": 112700
    },
    {
      "epoch": 0.3514026497785618,
      "grad_norm": 0.5875179171562195,
      "learning_rate": 4.414526305072561e-05,
      "loss": 1.287,
      "step": 112750
    },
    {
      "epoch": 0.35155848243921745,
      "grad_norm": 0.6261317133903503,
      "learning_rate": 4.414266583971468e-05,
      "loss": 1.2777,
      "step": 112800
    },
    {
      "epoch": 0.35171431509987316,
      "grad_norm": 0.6655274629592896,
      "learning_rate": 4.4140068628703754e-05,
      "loss": 1.2566,
      "step": 112850
    },
    {
      "epoch": 0.35187014776052883,
      "grad_norm": 0.7226116061210632,
      "learning_rate": 4.413747141769283e-05,
      "loss": 1.2542,
      "step": 112900
    },
    {
      "epoch": 0.3520259804211845,
      "grad_norm": 0.626375138759613,
      "learning_rate": 4.41348742066819e-05,
      "loss": 1.33,
      "step": 112950
    },
    {
      "epoch": 0.3521818130818402,
      "grad_norm": 0.5071990489959717,
      "learning_rate": 4.4132276995670965e-05,
      "loss": 1.3108,
      "step": 113000
    },
    {
      "epoch": 0.3523376457424959,
      "grad_norm": 0.663275420665741,
      "learning_rate": 4.4129679784660044e-05,
      "loss": 1.3067,
      "step": 113050
    },
    {
      "epoch": 0.35249347840315154,
      "grad_norm": 0.6414699554443359,
      "learning_rate": 4.412708257364912e-05,
      "loss": 1.272,
      "step": 113100
    },
    {
      "epoch": 0.35264931106380726,
      "grad_norm": 0.7070514559745789,
      "learning_rate": 4.412448536263818e-05,
      "loss": 1.2785,
      "step": 113150
    },
    {
      "epoch": 0.3528051437244629,
      "grad_norm": 0.7964690923690796,
      "learning_rate": 4.412188815162726e-05,
      "loss": 1.3391,
      "step": 113200
    },
    {
      "epoch": 0.3529609763851186,
      "grad_norm": 0.5090782046318054,
      "learning_rate": 4.4119290940616334e-05,
      "loss": 1.2432,
      "step": 113250
    },
    {
      "epoch": 0.3531168090457743,
      "grad_norm": 0.6265032291412354,
      "learning_rate": 4.41166937296054e-05,
      "loss": 1.3273,
      "step": 113300
    },
    {
      "epoch": 0.35327264170642997,
      "grad_norm": 0.6059579253196716,
      "learning_rate": 4.411409651859447e-05,
      "loss": 1.3044,
      "step": 113350
    },
    {
      "epoch": 0.35342847436708563,
      "grad_norm": 0.5365706086158752,
      "learning_rate": 4.4111499307583545e-05,
      "loss": 1.2598,
      "step": 113400
    },
    {
      "epoch": 0.35358430702774135,
      "grad_norm": 0.5497106909751892,
      "learning_rate": 4.410890209657262e-05,
      "loss": 1.2923,
      "step": 113450
    },
    {
      "epoch": 0.353740139688397,
      "grad_norm": 0.4447192847728729,
      "learning_rate": 4.410630488556169e-05,
      "loss": 1.2513,
      "step": 113500
    },
    {
      "epoch": 0.3538959723490527,
      "grad_norm": 0.491627037525177,
      "learning_rate": 4.410370767455076e-05,
      "loss": 1.3367,
      "step": 113550
    },
    {
      "epoch": 0.3540518050097084,
      "grad_norm": 0.6113965511322021,
      "learning_rate": 4.4101110463539835e-05,
      "loss": 1.3132,
      "step": 113600
    },
    {
      "epoch": 0.35420763767036406,
      "grad_norm": 0.5870251655578613,
      "learning_rate": 4.409851325252891e-05,
      "loss": 1.3137,
      "step": 113650
    },
    {
      "epoch": 0.3543634703310197,
      "grad_norm": 0.6384857296943665,
      "learning_rate": 4.409591604151797e-05,
      "loss": 1.2988,
      "step": 113700
    },
    {
      "epoch": 0.35451930299167544,
      "grad_norm": 0.5833512544631958,
      "learning_rate": 4.409331883050705e-05,
      "loss": 1.2412,
      "step": 113750
    },
    {
      "epoch": 0.3546751356523311,
      "grad_norm": 0.5975962281227112,
      "learning_rate": 4.4090721619496125e-05,
      "loss": 1.2875,
      "step": 113800
    },
    {
      "epoch": 0.35483096831298677,
      "grad_norm": 0.5183025002479553,
      "learning_rate": 4.408812440848519e-05,
      "loss": 1.2596,
      "step": 113850
    },
    {
      "epoch": 0.3549868009736425,
      "grad_norm": 0.6294934153556824,
      "learning_rate": 4.408552719747427e-05,
      "loss": 1.2998,
      "step": 113900
    },
    {
      "epoch": 0.35514263363429815,
      "grad_norm": 0.5762377381324768,
      "learning_rate": 4.408292998646334e-05,
      "loss": 1.2751,
      "step": 113950
    },
    {
      "epoch": 0.3552984662949538,
      "grad_norm": 0.7035757303237915,
      "learning_rate": 4.408033277545241e-05,
      "loss": 1.3043,
      "step": 114000
    },
    {
      "epoch": 0.3554542989556095,
      "grad_norm": 0.6556268930435181,
      "learning_rate": 4.407773556444148e-05,
      "loss": 1.3201,
      "step": 114050
    },
    {
      "epoch": 0.3556101316162652,
      "grad_norm": 0.6323520541191101,
      "learning_rate": 4.407513835343055e-05,
      "loss": 1.2961,
      "step": 114100
    },
    {
      "epoch": 0.35576596427692087,
      "grad_norm": 0.6074135303497314,
      "learning_rate": 4.4072541142419626e-05,
      "loss": 1.3267,
      "step": 114150
    },
    {
      "epoch": 0.35592179693757653,
      "grad_norm": 0.5438929200172424,
      "learning_rate": 4.40699439314087e-05,
      "loss": 1.2719,
      "step": 114200
    },
    {
      "epoch": 0.35607762959823225,
      "grad_norm": 0.423168808221817,
      "learning_rate": 4.4067346720397764e-05,
      "loss": 1.2397,
      "step": 114250
    },
    {
      "epoch": 0.3562334622588879,
      "grad_norm": 0.6247608065605164,
      "learning_rate": 4.406474950938684e-05,
      "loss": 1.3051,
      "step": 114300
    },
    {
      "epoch": 0.3563892949195436,
      "grad_norm": 0.5785391330718994,
      "learning_rate": 4.4062152298375916e-05,
      "loss": 1.2429,
      "step": 114350
    },
    {
      "epoch": 0.3565451275801993,
      "grad_norm": 0.6005386710166931,
      "learning_rate": 4.405955508736498e-05,
      "loss": 1.3056,
      "step": 114400
    },
    {
      "epoch": 0.35670096024085496,
      "grad_norm": 0.6411465406417847,
      "learning_rate": 4.405695787635406e-05,
      "loss": 1.2481,
      "step": 114450
    },
    {
      "epoch": 0.3568567929015106,
      "grad_norm": 0.4334295392036438,
      "learning_rate": 4.405436066534313e-05,
      "loss": 1.2962,
      "step": 114500
    },
    {
      "epoch": 0.35701262556216634,
      "grad_norm": 0.6724950075149536,
      "learning_rate": 4.405181539855242e-05,
      "loss": 1.3091,
      "step": 114550
    },
    {
      "epoch": 0.357168458222822,
      "grad_norm": 0.6744892597198486,
      "learning_rate": 4.404921818754149e-05,
      "loss": 1.3141,
      "step": 114600
    },
    {
      "epoch": 0.35732429088347767,
      "grad_norm": 0.5155494809150696,
      "learning_rate": 4.4046620976530564e-05,
      "loss": 1.2876,
      "step": 114650
    },
    {
      "epoch": 0.3574801235441334,
      "grad_norm": 0.6542287468910217,
      "learning_rate": 4.4044023765519636e-05,
      "loss": 1.2803,
      "step": 114700
    },
    {
      "epoch": 0.35763595620478905,
      "grad_norm": 0.5757765769958496,
      "learning_rate": 4.404142655450871e-05,
      "loss": 1.3185,
      "step": 114750
    },
    {
      "epoch": 0.3577917888654447,
      "grad_norm": 0.5911329388618469,
      "learning_rate": 4.403882934349778e-05,
      "loss": 1.2467,
      "step": 114800
    },
    {
      "epoch": 0.35794762152610043,
      "grad_norm": 0.6156512498855591,
      "learning_rate": 4.4036232132486854e-05,
      "loss": 1.298,
      "step": 114850
    },
    {
      "epoch": 0.3581034541867561,
      "grad_norm": 0.5293723344802856,
      "learning_rate": 4.4033634921475926e-05,
      "loss": 1.2601,
      "step": 114900
    },
    {
      "epoch": 0.35825928684741176,
      "grad_norm": 0.699191689491272,
      "learning_rate": 4.403103771046499e-05,
      "loss": 1.2178,
      "step": 114950
    },
    {
      "epoch": 0.3584151195080675,
      "grad_norm": 0.6844967603683472,
      "learning_rate": 4.402844049945407e-05,
      "loss": 1.2727,
      "step": 115000
    },
    {
      "epoch": 0.35857095216872314,
      "grad_norm": 0.764631986618042,
      "learning_rate": 4.4025843288443144e-05,
      "loss": 1.2972,
      "step": 115050
    },
    {
      "epoch": 0.3587267848293788,
      "grad_norm": 0.5464830994606018,
      "learning_rate": 4.402324607743221e-05,
      "loss": 1.2373,
      "step": 115100
    },
    {
      "epoch": 0.35888261749003453,
      "grad_norm": 0.50187087059021,
      "learning_rate": 4.402064886642128e-05,
      "loss": 1.2393,
      "step": 115150
    },
    {
      "epoch": 0.3590384501506902,
      "grad_norm": 0.6361539959907532,
      "learning_rate": 4.401805165541036e-05,
      "loss": 1.3332,
      "step": 115200
    },
    {
      "epoch": 0.35919428281134586,
      "grad_norm": 0.5308080911636353,
      "learning_rate": 4.401545444439943e-05,
      "loss": 1.2696,
      "step": 115250
    },
    {
      "epoch": 0.3593501154720015,
      "grad_norm": 0.6689582467079163,
      "learning_rate": 4.40128572333885e-05,
      "loss": 1.3011,
      "step": 115300
    },
    {
      "epoch": 0.35950594813265724,
      "grad_norm": 0.67802494764328,
      "learning_rate": 4.401026002237757e-05,
      "loss": 1.3153,
      "step": 115350
    },
    {
      "epoch": 0.3596617807933129,
      "grad_norm": 0.6365830898284912,
      "learning_rate": 4.4007714755586864e-05,
      "loss": 1.2882,
      "step": 115400
    },
    {
      "epoch": 0.35981761345396857,
      "grad_norm": 0.65827876329422,
      "learning_rate": 4.400511754457594e-05,
      "loss": 1.2349,
      "step": 115450
    },
    {
      "epoch": 0.3599734461146243,
      "grad_norm": 0.5313448309898376,
      "learning_rate": 4.400252033356501e-05,
      "loss": 1.3228,
      "step": 115500
    },
    {
      "epoch": 0.36012927877527995,
      "grad_norm": 0.5937208533287048,
      "learning_rate": 4.3999923122554075e-05,
      "loss": 1.2749,
      "step": 115550
    },
    {
      "epoch": 0.3602851114359356,
      "grad_norm": 0.5786839723587036,
      "learning_rate": 4.3997325911543154e-05,
      "loss": 1.2365,
      "step": 115600
    },
    {
      "epoch": 0.36044094409659133,
      "grad_norm": 0.6488930583000183,
      "learning_rate": 4.399472870053222e-05,
      "loss": 1.3079,
      "step": 115650
    },
    {
      "epoch": 0.360596776757247,
      "grad_norm": 0.5983432531356812,
      "learning_rate": 4.399213148952129e-05,
      "loss": 1.3246,
      "step": 115700
    },
    {
      "epoch": 0.36075260941790266,
      "grad_norm": 0.6121058464050293,
      "learning_rate": 4.398953427851037e-05,
      "loss": 1.2505,
      "step": 115750
    },
    {
      "epoch": 0.3609084420785584,
      "grad_norm": 0.6189106702804565,
      "learning_rate": 4.398693706749944e-05,
      "loss": 1.3033,
      "step": 115800
    },
    {
      "epoch": 0.36106427473921404,
      "grad_norm": 0.6673219799995422,
      "learning_rate": 4.398433985648851e-05,
      "loss": 1.2568,
      "step": 115850
    },
    {
      "epoch": 0.3612201073998697,
      "grad_norm": 0.5182347297668457,
      "learning_rate": 4.398174264547758e-05,
      "loss": 1.2389,
      "step": 115900
    },
    {
      "epoch": 0.3613759400605254,
      "grad_norm": 0.7843739986419678,
      "learning_rate": 4.3979145434466655e-05,
      "loss": 1.3321,
      "step": 115950
    },
    {
      "epoch": 0.3615317727211811,
      "grad_norm": 0.5831396579742432,
      "learning_rate": 4.397654822345573e-05,
      "loss": 1.2879,
      "step": 116000
    },
    {
      "epoch": 0.36168760538183675,
      "grad_norm": 0.5571450591087341,
      "learning_rate": 4.39739510124448e-05,
      "loss": 1.2853,
      "step": 116050
    },
    {
      "epoch": 0.36184343804249247,
      "grad_norm": 0.7357829809188843,
      "learning_rate": 4.3971353801433866e-05,
      "loss": 1.2971,
      "step": 116100
    },
    {
      "epoch": 0.36199927070314813,
      "grad_norm": 0.44693219661712646,
      "learning_rate": 4.3968756590422945e-05,
      "loss": 1.2967,
      "step": 116150
    },
    {
      "epoch": 0.3621551033638038,
      "grad_norm": 0.5813679695129395,
      "learning_rate": 4.396615937941202e-05,
      "loss": 1.2139,
      "step": 116200
    },
    {
      "epoch": 0.3623109360244595,
      "grad_norm": 0.5317560434341431,
      "learning_rate": 4.396356216840108e-05,
      "loss": 1.3167,
      "step": 116250
    },
    {
      "epoch": 0.3624667686851152,
      "grad_norm": 0.6249122023582458,
      "learning_rate": 4.396096495739016e-05,
      "loss": 1.3457,
      "step": 116300
    },
    {
      "epoch": 0.36262260134577085,
      "grad_norm": 0.5121575593948364,
      "learning_rate": 4.395836774637923e-05,
      "loss": 1.3054,
      "step": 116350
    },
    {
      "epoch": 0.36277843400642656,
      "grad_norm": 0.6252250671386719,
      "learning_rate": 4.39557705353683e-05,
      "loss": 1.3278,
      "step": 116400
    },
    {
      "epoch": 0.36293426666708223,
      "grad_norm": 0.6190205216407776,
      "learning_rate": 4.395317332435737e-05,
      "loss": 1.2616,
      "step": 116450
    },
    {
      "epoch": 0.3630900993277379,
      "grad_norm": 0.5679930448532104,
      "learning_rate": 4.3950576113346446e-05,
      "loss": 1.2486,
      "step": 116500
    },
    {
      "epoch": 0.3632459319883936,
      "grad_norm": 0.7865418791770935,
      "learning_rate": 4.394797890233552e-05,
      "loss": 1.3073,
      "step": 116550
    },
    {
      "epoch": 0.3634017646490493,
      "grad_norm": 0.46468332409858704,
      "learning_rate": 4.394538169132459e-05,
      "loss": 1.2504,
      "step": 116600
    },
    {
      "epoch": 0.36355759730970494,
      "grad_norm": 0.5523152947425842,
      "learning_rate": 4.394278448031366e-05,
      "loss": 1.2891,
      "step": 116650
    },
    {
      "epoch": 0.3637134299703606,
      "grad_norm": 0.59577476978302,
      "learning_rate": 4.3940187269302736e-05,
      "loss": 1.2792,
      "step": 116700
    },
    {
      "epoch": 0.3638692626310163,
      "grad_norm": 0.6008528470993042,
      "learning_rate": 4.393759005829181e-05,
      "loss": 1.2992,
      "step": 116750
    },
    {
      "epoch": 0.364025095291672,
      "grad_norm": 0.5829377174377441,
      "learning_rate": 4.3934992847280874e-05,
      "loss": 1.2305,
      "step": 116800
    },
    {
      "epoch": 0.36418092795232765,
      "grad_norm": 0.59236741065979,
      "learning_rate": 4.393239563626995e-05,
      "loss": 1.2892,
      "step": 116850
    },
    {
      "epoch": 0.36433676061298337,
      "grad_norm": 0.7544500231742859,
      "learning_rate": 4.392979842525902e-05,
      "loss": 1.2666,
      "step": 116900
    },
    {
      "epoch": 0.36449259327363903,
      "grad_norm": 0.5955289602279663,
      "learning_rate": 4.392720121424809e-05,
      "loss": 1.2959,
      "step": 116950
    },
    {
      "epoch": 0.3646484259342947,
      "grad_norm": 0.5219318270683289,
      "learning_rate": 4.392460400323717e-05,
      "loss": 1.3584,
      "step": 117000
    },
    {
      "epoch": 0.3648042585949504,
      "grad_norm": 0.5848274230957031,
      "learning_rate": 4.3922006792226236e-05,
      "loss": 1.2931,
      "step": 117050
    },
    {
      "epoch": 0.3649600912556061,
      "grad_norm": 0.6389688849449158,
      "learning_rate": 4.391940958121531e-05,
      "loss": 1.2538,
      "step": 117100
    },
    {
      "epoch": 0.36511592391626174,
      "grad_norm": 0.6174463629722595,
      "learning_rate": 4.391681237020438e-05,
      "loss": 1.2445,
      "step": 117150
    },
    {
      "epoch": 0.36527175657691746,
      "grad_norm": 0.5809708833694458,
      "learning_rate": 4.3914215159193454e-05,
      "loss": 1.3051,
      "step": 117200
    },
    {
      "epoch": 0.3654275892375731,
      "grad_norm": 0.8223596811294556,
      "learning_rate": 4.3911617948182526e-05,
      "loss": 1.2596,
      "step": 117250
    },
    {
      "epoch": 0.3655834218982288,
      "grad_norm": 0.5254910588264465,
      "learning_rate": 4.39090207371716e-05,
      "loss": 1.2738,
      "step": 117300
    },
    {
      "epoch": 0.3657392545588845,
      "grad_norm": 0.7088721990585327,
      "learning_rate": 4.3906423526160665e-05,
      "loss": 1.2859,
      "step": 117350
    },
    {
      "epoch": 0.36589508721954017,
      "grad_norm": 0.5446974039077759,
      "learning_rate": 4.3903826315149744e-05,
      "loss": 1.3155,
      "step": 117400
    },
    {
      "epoch": 0.36605091988019584,
      "grad_norm": 0.571243166923523,
      "learning_rate": 4.3901281048359036e-05,
      "loss": 1.2503,
      "step": 117450
    },
    {
      "epoch": 0.36620675254085155,
      "grad_norm": 0.5430217981338501,
      "learning_rate": 4.38986838373481e-05,
      "loss": 1.2969,
      "step": 117500
    },
    {
      "epoch": 0.3663625852015072,
      "grad_norm": 0.5630930066108704,
      "learning_rate": 4.3896086626337174e-05,
      "loss": 1.2466,
      "step": 117550
    },
    {
      "epoch": 0.3665184178621629,
      "grad_norm": 0.5513562560081482,
      "learning_rate": 4.389348941532625e-05,
      "loss": 1.2718,
      "step": 117600
    },
    {
      "epoch": 0.3666742505228186,
      "grad_norm": 0.8082510828971863,
      "learning_rate": 4.389089220431532e-05,
      "loss": 1.2632,
      "step": 117650
    },
    {
      "epoch": 0.36683008318347426,
      "grad_norm": 0.6579007506370544,
      "learning_rate": 4.388829499330439e-05,
      "loss": 1.2778,
      "step": 117700
    },
    {
      "epoch": 0.36698591584412993,
      "grad_norm": 0.6802477836608887,
      "learning_rate": 4.3885697782293464e-05,
      "loss": 1.29,
      "step": 117750
    },
    {
      "epoch": 0.36714174850478565,
      "grad_norm": 0.6946637630462646,
      "learning_rate": 4.388310057128254e-05,
      "loss": 1.3303,
      "step": 117800
    },
    {
      "epoch": 0.3672975811654413,
      "grad_norm": 0.6903021931648254,
      "learning_rate": 4.388050336027161e-05,
      "loss": 1.3082,
      "step": 117850
    },
    {
      "epoch": 0.367453413826097,
      "grad_norm": 0.6730428338050842,
      "learning_rate": 4.3877906149260675e-05,
      "loss": 1.313,
      "step": 117900
    },
    {
      "epoch": 0.36760924648675264,
      "grad_norm": 0.6205770969390869,
      "learning_rate": 4.3875308938249754e-05,
      "loss": 1.2444,
      "step": 117950
    },
    {
      "epoch": 0.36776507914740836,
      "grad_norm": 0.6700164675712585,
      "learning_rate": 4.387271172723883e-05,
      "loss": 1.3525,
      "step": 118000
    },
    {
      "epoch": 0.367920911808064,
      "grad_norm": 0.6803284883499146,
      "learning_rate": 4.387011451622789e-05,
      "loss": 1.2999,
      "step": 118050
    },
    {
      "epoch": 0.3680767444687197,
      "grad_norm": 0.5600902438163757,
      "learning_rate": 4.3867517305216965e-05,
      "loss": 1.3239,
      "step": 118100
    },
    {
      "epoch": 0.3682325771293754,
      "grad_norm": 0.6041138768196106,
      "learning_rate": 4.3864920094206044e-05,
      "loss": 1.322,
      "step": 118150
    },
    {
      "epoch": 0.36838840979003107,
      "grad_norm": 0.49200376868247986,
      "learning_rate": 4.386232288319511e-05,
      "loss": 1.2495,
      "step": 118200
    },
    {
      "epoch": 0.36854424245068673,
      "grad_norm": 0.5960775017738342,
      "learning_rate": 4.385972567218418e-05,
      "loss": 1.2862,
      "step": 118250
    },
    {
      "epoch": 0.36870007511134245,
      "grad_norm": 0.6338363885879517,
      "learning_rate": 4.3857128461173255e-05,
      "loss": 1.2697,
      "step": 118300
    },
    {
      "epoch": 0.3688559077719981,
      "grad_norm": 0.6461572647094727,
      "learning_rate": 4.385453125016233e-05,
      "loss": 1.2748,
      "step": 118350
    },
    {
      "epoch": 0.3690117404326538,
      "grad_norm": 0.6654154062271118,
      "learning_rate": 4.38519340391514e-05,
      "loss": 1.2605,
      "step": 118400
    },
    {
      "epoch": 0.3691675730933095,
      "grad_norm": 0.6971449255943298,
      "learning_rate": 4.384933682814047e-05,
      "loss": 1.2647,
      "step": 118450
    },
    {
      "epoch": 0.36932340575396516,
      "grad_norm": 0.5397875308990479,
      "learning_rate": 4.3846739617129545e-05,
      "loss": 1.3016,
      "step": 118500
    },
    {
      "epoch": 0.3694792384146208,
      "grad_norm": 0.709551990032196,
      "learning_rate": 4.384414240611862e-05,
      "loss": 1.2931,
      "step": 118550
    },
    {
      "epoch": 0.36963507107527654,
      "grad_norm": 0.6097021102905273,
      "learning_rate": 4.384154519510768e-05,
      "loss": 1.2859,
      "step": 118600
    },
    {
      "epoch": 0.3697909037359322,
      "grad_norm": 0.7476877570152283,
      "learning_rate": 4.383894798409676e-05,
      "loss": 1.3617,
      "step": 118650
    },
    {
      "epoch": 0.36994673639658787,
      "grad_norm": 0.7178996205329895,
      "learning_rate": 4.3836350773085835e-05,
      "loss": 1.2421,
      "step": 118700
    },
    {
      "epoch": 0.3701025690572436,
      "grad_norm": 0.5718066096305847,
      "learning_rate": 4.38337535620749e-05,
      "loss": 1.2598,
      "step": 118750
    },
    {
      "epoch": 0.37025840171789925,
      "grad_norm": 0.5451909303665161,
      "learning_rate": 4.383115635106397e-05,
      "loss": 1.3029,
      "step": 118800
    },
    {
      "epoch": 0.3704142343785549,
      "grad_norm": 0.7210837602615356,
      "learning_rate": 4.382855914005305e-05,
      "loss": 1.2791,
      "step": 118850
    },
    {
      "epoch": 0.37057006703921064,
      "grad_norm": 0.6100269556045532,
      "learning_rate": 4.382596192904212e-05,
      "loss": 1.2856,
      "step": 118900
    },
    {
      "epoch": 0.3707258996998663,
      "grad_norm": 0.6027399301528931,
      "learning_rate": 4.382336471803119e-05,
      "loss": 1.2685,
      "step": 118950
    },
    {
      "epoch": 0.37088173236052197,
      "grad_norm": 0.6255103349685669,
      "learning_rate": 4.382076750702026e-05,
      "loss": 1.2985,
      "step": 119000
    },
    {
      "epoch": 0.3710375650211777,
      "grad_norm": 0.7824333906173706,
      "learning_rate": 4.3818170296009336e-05,
      "loss": 1.2859,
      "step": 119050
    },
    {
      "epoch": 0.37119339768183335,
      "grad_norm": 0.4481971561908722,
      "learning_rate": 4.381557308499841e-05,
      "loss": 1.2716,
      "step": 119100
    },
    {
      "epoch": 0.371349230342489,
      "grad_norm": 0.6017318367958069,
      "learning_rate": 4.3812975873987474e-05,
      "loss": 1.256,
      "step": 119150
    },
    {
      "epoch": 0.3715050630031447,
      "grad_norm": 0.5693778395652771,
      "learning_rate": 4.381037866297655e-05,
      "loss": 1.2728,
      "step": 119200
    },
    {
      "epoch": 0.3716608956638004,
      "grad_norm": 0.5932562351226807,
      "learning_rate": 4.3807781451965626e-05,
      "loss": 1.2561,
      "step": 119250
    },
    {
      "epoch": 0.37181672832445606,
      "grad_norm": 0.6757050156593323,
      "learning_rate": 4.380518424095469e-05,
      "loss": 1.2668,
      "step": 119300
    },
    {
      "epoch": 0.3719725609851117,
      "grad_norm": 0.5207232236862183,
      "learning_rate": 4.3802587029943764e-05,
      "loss": 1.3334,
      "step": 119350
    },
    {
      "epoch": 0.37212839364576744,
      "grad_norm": 0.6031019687652588,
      "learning_rate": 4.379998981893284e-05,
      "loss": 1.2379,
      "step": 119400
    },
    {
      "epoch": 0.3722842263064231,
      "grad_norm": 0.7011832594871521,
      "learning_rate": 4.379739260792191e-05,
      "loss": 1.3509,
      "step": 119450
    },
    {
      "epoch": 0.37244005896707877,
      "grad_norm": 0.6817960739135742,
      "learning_rate": 4.379479539691098e-05,
      "loss": 1.2877,
      "step": 119500
    },
    {
      "epoch": 0.3725958916277345,
      "grad_norm": 0.6280450820922852,
      "learning_rate": 4.3792198185900054e-05,
      "loss": 1.2846,
      "step": 119550
    },
    {
      "epoch": 0.37275172428839015,
      "grad_norm": 0.5418813824653625,
      "learning_rate": 4.3789600974889127e-05,
      "loss": 1.2891,
      "step": 119600
    },
    {
      "epoch": 0.3729075569490458,
      "grad_norm": 0.462857186794281,
      "learning_rate": 4.37870037638782e-05,
      "loss": 1.1889,
      "step": 119650
    },
    {
      "epoch": 0.37306338960970153,
      "grad_norm": 0.6069427728652954,
      "learning_rate": 4.378440655286727e-05,
      "loss": 1.3183,
      "step": 119700
    },
    {
      "epoch": 0.3732192222703572,
      "grad_norm": 0.6910990476608276,
      "learning_rate": 4.3781809341856344e-05,
      "loss": 1.3115,
      "step": 119750
    },
    {
      "epoch": 0.37337505493101286,
      "grad_norm": 0.6270799040794373,
      "learning_rate": 4.3779212130845417e-05,
      "loss": 1.2456,
      "step": 119800
    },
    {
      "epoch": 0.3735308875916686,
      "grad_norm": 0.5795145630836487,
      "learning_rate": 4.377661491983448e-05,
      "loss": 1.2792,
      "step": 119850
    },
    {
      "epoch": 0.37368672025232424,
      "grad_norm": 0.6099186539649963,
      "learning_rate": 4.3774069653043774e-05,
      "loss": 1.2857,
      "step": 119900
    },
    {
      "epoch": 0.3738425529129799,
      "grad_norm": 0.5823861360549927,
      "learning_rate": 4.3771472442032854e-05,
      "loss": 1.2725,
      "step": 119950
    },
    {
      "epoch": 0.3739983855736356,
      "grad_norm": 0.504576563835144,
      "learning_rate": 4.376887523102192e-05,
      "loss": 1.2349,
      "step": 120000
    },
    {
      "epoch": 0.3741542182342913,
      "grad_norm": 0.645169734954834,
      "learning_rate": 4.376627802001099e-05,
      "loss": 1.2657,
      "step": 120050
    },
    {
      "epoch": 0.37431005089494696,
      "grad_norm": 0.5409004092216492,
      "learning_rate": 4.3763680809000064e-05,
      "loss": 1.2849,
      "step": 120100
    },
    {
      "epoch": 0.3744658835556027,
      "grad_norm": 0.5702266693115234,
      "learning_rate": 4.376108359798914e-05,
      "loss": 1.2901,
      "step": 120150
    },
    {
      "epoch": 0.37462171621625834,
      "grad_norm": 0.5280563235282898,
      "learning_rate": 4.375848638697821e-05,
      "loss": 1.3132,
      "step": 120200
    },
    {
      "epoch": 0.374777548876914,
      "grad_norm": 0.4211035966873169,
      "learning_rate": 4.375588917596728e-05,
      "loss": 1.2902,
      "step": 120250
    },
    {
      "epoch": 0.3749333815375697,
      "grad_norm": 0.6293310523033142,
      "learning_rate": 4.3753291964956354e-05,
      "loss": 1.2612,
      "step": 120300
    },
    {
      "epoch": 0.3750892141982254,
      "grad_norm": 0.6913221478462219,
      "learning_rate": 4.375069475394543e-05,
      "loss": 1.2268,
      "step": 120350
    },
    {
      "epoch": 0.37524504685888105,
      "grad_norm": 0.5070938467979431,
      "learning_rate": 4.37480975429345e-05,
      "loss": 1.3402,
      "step": 120400
    },
    {
      "epoch": 0.37540087951953677,
      "grad_norm": 0.6621866822242737,
      "learning_rate": 4.3745500331923565e-05,
      "loss": 1.2531,
      "step": 120450
    },
    {
      "epoch": 0.37555671218019243,
      "grad_norm": 0.6405518054962158,
      "learning_rate": 4.3742903120912645e-05,
      "loss": 1.2344,
      "step": 120500
    },
    {
      "epoch": 0.3757125448408481,
      "grad_norm": 0.7759639620780945,
      "learning_rate": 4.374030590990171e-05,
      "loss": 1.3141,
      "step": 120550
    },
    {
      "epoch": 0.37586837750150376,
      "grad_norm": 0.4949381351470947,
      "learning_rate": 4.373770869889078e-05,
      "loss": 1.2648,
      "step": 120600
    },
    {
      "epoch": 0.3760242101621595,
      "grad_norm": 0.47031596302986145,
      "learning_rate": 4.373511148787986e-05,
      "loss": 1.3091,
      "step": 120650
    },
    {
      "epoch": 0.37618004282281514,
      "grad_norm": 0.5327876210212708,
      "learning_rate": 4.373251427686893e-05,
      "loss": 1.2798,
      "step": 120700
    },
    {
      "epoch": 0.3763358754834708,
      "grad_norm": 0.6559816002845764,
      "learning_rate": 4.3729917065858e-05,
      "loss": 1.2542,
      "step": 120750
    },
    {
      "epoch": 0.3764917081441265,
      "grad_norm": 0.5198267102241516,
      "learning_rate": 4.372731985484707e-05,
      "loss": 1.2824,
      "step": 120800
    },
    {
      "epoch": 0.3766475408047822,
      "grad_norm": 0.6602588891983032,
      "learning_rate": 4.3724722643836145e-05,
      "loss": 1.2758,
      "step": 120850
    },
    {
      "epoch": 0.37680337346543785,
      "grad_norm": 0.5450400114059448,
      "learning_rate": 4.372212543282522e-05,
      "loss": 1.2859,
      "step": 120900
    },
    {
      "epoch": 0.37695920612609357,
      "grad_norm": 0.5342373251914978,
      "learning_rate": 4.371952822181429e-05,
      "loss": 1.2868,
      "step": 120950
    },
    {
      "epoch": 0.37711503878674923,
      "grad_norm": 0.6614928245544434,
      "learning_rate": 4.371693101080336e-05,
      "loss": 1.2649,
      "step": 121000
    },
    {
      "epoch": 0.3772708714474049,
      "grad_norm": 0.5100746154785156,
      "learning_rate": 4.3714333799792435e-05,
      "loss": 1.2805,
      "step": 121050
    },
    {
      "epoch": 0.3774267041080606,
      "grad_norm": 0.5720688104629517,
      "learning_rate": 4.371173658878151e-05,
      "loss": 1.2308,
      "step": 121100
    },
    {
      "epoch": 0.3775825367687163,
      "grad_norm": 0.5470685362815857,
      "learning_rate": 4.3709139377770573e-05,
      "loss": 1.2537,
      "step": 121150
    },
    {
      "epoch": 0.37773836942937195,
      "grad_norm": 0.6411160230636597,
      "learning_rate": 4.370654216675965e-05,
      "loss": 1.3036,
      "step": 121200
    },
    {
      "epoch": 0.37789420209002766,
      "grad_norm": 0.5407906770706177,
      "learning_rate": 4.370394495574872e-05,
      "loss": 1.261,
      "step": 121250
    },
    {
      "epoch": 0.37805003475068333,
      "grad_norm": 0.5207412838935852,
      "learning_rate": 4.370134774473779e-05,
      "loss": 1.2299,
      "step": 121300
    },
    {
      "epoch": 0.378205867411339,
      "grad_norm": 0.6496469974517822,
      "learning_rate": 4.3698750533726864e-05,
      "loss": 1.272,
      "step": 121350
    },
    {
      "epoch": 0.3783617000719947,
      "grad_norm": 0.6811959147453308,
      "learning_rate": 4.3696153322715936e-05,
      "loss": 1.2595,
      "step": 121400
    },
    {
      "epoch": 0.3785175327326504,
      "grad_norm": 0.5937258005142212,
      "learning_rate": 4.369355611170501e-05,
      "loss": 1.2702,
      "step": 121450
    },
    {
      "epoch": 0.37867336539330604,
      "grad_norm": 0.4660084843635559,
      "learning_rate": 4.369095890069408e-05,
      "loss": 1.2896,
      "step": 121500
    },
    {
      "epoch": 0.37882919805396176,
      "grad_norm": 0.5648842453956604,
      "learning_rate": 4.3688361689683154e-05,
      "loss": 1.2611,
      "step": 121550
    },
    {
      "epoch": 0.3789850307146174,
      "grad_norm": 0.6689195036888123,
      "learning_rate": 4.3685764478672226e-05,
      "loss": 1.2529,
      "step": 121600
    },
    {
      "epoch": 0.3791408633752731,
      "grad_norm": 0.5344876050949097,
      "learning_rate": 4.36831672676613e-05,
      "loss": 1.313,
      "step": 121650
    },
    {
      "epoch": 0.3792966960359288,
      "grad_norm": 0.7079824209213257,
      "learning_rate": 4.3680570056650364e-05,
      "loss": 1.2696,
      "step": 121700
    },
    {
      "epoch": 0.37945252869658447,
      "grad_norm": 0.5679979920387268,
      "learning_rate": 4.3677972845639444e-05,
      "loss": 1.2678,
      "step": 121750
    },
    {
      "epoch": 0.37960836135724013,
      "grad_norm": 0.6870812177658081,
      "learning_rate": 4.367537563462851e-05,
      "loss": 1.3231,
      "step": 121800
    },
    {
      "epoch": 0.3797641940178958,
      "grad_norm": 0.6326819658279419,
      "learning_rate": 4.367277842361758e-05,
      "loss": 1.2067,
      "step": 121850
    },
    {
      "epoch": 0.3799200266785515,
      "grad_norm": 0.45683977007865906,
      "learning_rate": 4.367018121260666e-05,
      "loss": 1.2116,
      "step": 121900
    },
    {
      "epoch": 0.3800758593392072,
      "grad_norm": 0.5688557624816895,
      "learning_rate": 4.3667635945815946e-05,
      "loss": 1.2957,
      "step": 121950
    },
    {
      "epoch": 0.38023169199986284,
      "grad_norm": 0.48968246579170227,
      "learning_rate": 4.366503873480502e-05,
      "loss": 1.2996,
      "step": 122000
    },
    {
      "epoch": 0.38038752466051856,
      "grad_norm": 0.511894166469574,
      "learning_rate": 4.366244152379409e-05,
      "loss": 1.31,
      "step": 122050
    },
    {
      "epoch": 0.3805433573211742,
      "grad_norm": 0.4698556363582611,
      "learning_rate": 4.3659844312783164e-05,
      "loss": 1.2612,
      "step": 122100
    },
    {
      "epoch": 0.3806991899818299,
      "grad_norm": 0.6599114537239075,
      "learning_rate": 4.3657247101772236e-05,
      "loss": 1.2702,
      "step": 122150
    },
    {
      "epoch": 0.3808550226424856,
      "grad_norm": 0.7010509371757507,
      "learning_rate": 4.365464989076131e-05,
      "loss": 1.2794,
      "step": 122200
    },
    {
      "epoch": 0.38101085530314127,
      "grad_norm": 0.641582727432251,
      "learning_rate": 4.3652052679750375e-05,
      "loss": 1.3252,
      "step": 122250
    },
    {
      "epoch": 0.38116668796379694,
      "grad_norm": 0.5522006750106812,
      "learning_rate": 4.3649455468739454e-05,
      "loss": 1.2317,
      "step": 122300
    },
    {
      "epoch": 0.38132252062445265,
      "grad_norm": 0.601041853427887,
      "learning_rate": 4.3646858257728526e-05,
      "loss": 1.3068,
      "step": 122350
    },
    {
      "epoch": 0.3814783532851083,
      "grad_norm": 0.5748610496520996,
      "learning_rate": 4.364426104671759e-05,
      "loss": 1.2917,
      "step": 122400
    },
    {
      "epoch": 0.381634185945764,
      "grad_norm": 0.6394314169883728,
      "learning_rate": 4.3641663835706665e-05,
      "loss": 1.238,
      "step": 122450
    },
    {
      "epoch": 0.3817900186064197,
      "grad_norm": 0.5869917273521423,
      "learning_rate": 4.363906662469574e-05,
      "loss": 1.3058,
      "step": 122500
    },
    {
      "epoch": 0.38194585126707536,
      "grad_norm": 0.5693693161010742,
      "learning_rate": 4.363646941368481e-05,
      "loss": 1.2702,
      "step": 122550
    },
    {
      "epoch": 0.38210168392773103,
      "grad_norm": 0.6299892067909241,
      "learning_rate": 4.363387220267388e-05,
      "loss": 1.2833,
      "step": 122600
    },
    {
      "epoch": 0.38225751658838675,
      "grad_norm": 0.6243429780006409,
      "learning_rate": 4.3631274991662955e-05,
      "loss": 1.2625,
      "step": 122650
    },
    {
      "epoch": 0.3824133492490424,
      "grad_norm": 0.6028339862823486,
      "learning_rate": 4.362867778065203e-05,
      "loss": 1.2728,
      "step": 122700
    },
    {
      "epoch": 0.3825691819096981,
      "grad_norm": 0.5650950074195862,
      "learning_rate": 4.36260805696411e-05,
      "loss": 1.2695,
      "step": 122750
    },
    {
      "epoch": 0.3827250145703538,
      "grad_norm": 0.651763916015625,
      "learning_rate": 4.3623483358630165e-05,
      "loss": 1.2863,
      "step": 122800
    },
    {
      "epoch": 0.38288084723100946,
      "grad_norm": 0.552039384841919,
      "learning_rate": 4.3620886147619245e-05,
      "loss": 1.2844,
      "step": 122850
    },
    {
      "epoch": 0.3830366798916651,
      "grad_norm": 0.5265936255455017,
      "learning_rate": 4.361828893660832e-05,
      "loss": 1.2439,
      "step": 122900
    },
    {
      "epoch": 0.38319251255232084,
      "grad_norm": 0.5958473682403564,
      "learning_rate": 4.36157436698176e-05,
      "loss": 1.3185,
      "step": 122950
    },
    {
      "epoch": 0.3833483452129765,
      "grad_norm": 0.5359919667243958,
      "learning_rate": 4.3613146458806675e-05,
      "loss": 1.2895,
      "step": 123000
    },
    {
      "epoch": 0.38350417787363217,
      "grad_norm": 0.6377437710762024,
      "learning_rate": 4.3610549247795754e-05,
      "loss": 1.2732,
      "step": 123050
    },
    {
      "epoch": 0.3836600105342879,
      "grad_norm": 0.48535341024398804,
      "learning_rate": 4.360795203678482e-05,
      "loss": 1.2983,
      "step": 123100
    },
    {
      "epoch": 0.38381584319494355,
      "grad_norm": 0.7187165021896362,
      "learning_rate": 4.360535482577389e-05,
      "loss": 1.2741,
      "step": 123150
    },
    {
      "epoch": 0.3839716758555992,
      "grad_norm": 0.5630184412002563,
      "learning_rate": 4.3602757614762965e-05,
      "loss": 1.2574,
      "step": 123200
    },
    {
      "epoch": 0.3841275085162549,
      "grad_norm": 0.5991930961608887,
      "learning_rate": 4.360016040375204e-05,
      "loss": 1.2987,
      "step": 123250
    },
    {
      "epoch": 0.3842833411769106,
      "grad_norm": 0.6155419945716858,
      "learning_rate": 4.359756319274111e-05,
      "loss": 1.3012,
      "step": 123300
    },
    {
      "epoch": 0.38443917383756626,
      "grad_norm": 0.7111846804618835,
      "learning_rate": 4.359496598173018e-05,
      "loss": 1.3139,
      "step": 123350
    },
    {
      "epoch": 0.3845950064982219,
      "grad_norm": 0.6607616543769836,
      "learning_rate": 4.3592368770719255e-05,
      "loss": 1.2951,
      "step": 123400
    },
    {
      "epoch": 0.38475083915887764,
      "grad_norm": 0.5766223073005676,
      "learning_rate": 4.358977155970833e-05,
      "loss": 1.2826,
      "step": 123450
    },
    {
      "epoch": 0.3849066718195333,
      "grad_norm": 0.5902954936027527,
      "learning_rate": 4.358717434869739e-05,
      "loss": 1.3098,
      "step": 123500
    },
    {
      "epoch": 0.38506250448018897,
      "grad_norm": 0.5763415098190308,
      "learning_rate": 4.3584577137686466e-05,
      "loss": 1.2411,
      "step": 123550
    },
    {
      "epoch": 0.3852183371408447,
      "grad_norm": 0.4495627284049988,
      "learning_rate": 4.3581979926675545e-05,
      "loss": 1.2582,
      "step": 123600
    },
    {
      "epoch": 0.38537416980150035,
      "grad_norm": 0.6439781785011292,
      "learning_rate": 4.357938271566461e-05,
      "loss": 1.3202,
      "step": 123650
    },
    {
      "epoch": 0.385530002462156,
      "grad_norm": 0.6535604596138,
      "learning_rate": 4.357678550465368e-05,
      "loss": 1.3086,
      "step": 123700
    },
    {
      "epoch": 0.38568583512281174,
      "grad_norm": 0.71649169921875,
      "learning_rate": 4.357418829364276e-05,
      "loss": 1.2548,
      "step": 123750
    },
    {
      "epoch": 0.3858416677834674,
      "grad_norm": 0.41576188802719116,
      "learning_rate": 4.357159108263183e-05,
      "loss": 1.2808,
      "step": 123800
    },
    {
      "epoch": 0.38599750044412307,
      "grad_norm": 0.6413590908050537,
      "learning_rate": 4.35689938716209e-05,
      "loss": 1.3353,
      "step": 123850
    },
    {
      "epoch": 0.3861533331047788,
      "grad_norm": 0.5919172763824463,
      "learning_rate": 4.356639666060997e-05,
      "loss": 1.3043,
      "step": 123900
    },
    {
      "epoch": 0.38630916576543445,
      "grad_norm": 0.5729137063026428,
      "learning_rate": 4.3563799449599046e-05,
      "loss": 1.308,
      "step": 123950
    },
    {
      "epoch": 0.3864649984260901,
      "grad_norm": 0.6264764070510864,
      "learning_rate": 4.356120223858812e-05,
      "loss": 1.2463,
      "step": 124000
    },
    {
      "epoch": 0.38662083108674583,
      "grad_norm": 0.6139302849769592,
      "learning_rate": 4.3558605027577184e-05,
      "loss": 1.294,
      "step": 124050
    },
    {
      "epoch": 0.3867766637474015,
      "grad_norm": 0.5994769334793091,
      "learning_rate": 4.355600781656626e-05,
      "loss": 1.2779,
      "step": 124100
    },
    {
      "epoch": 0.38693249640805716,
      "grad_norm": 0.5168734192848206,
      "learning_rate": 4.3553410605555336e-05,
      "loss": 1.3054,
      "step": 124150
    },
    {
      "epoch": 0.3870883290687129,
      "grad_norm": 0.6389521360397339,
      "learning_rate": 4.35508133945444e-05,
      "loss": 1.3314,
      "step": 124200
    },
    {
      "epoch": 0.38724416172936854,
      "grad_norm": 0.6100038290023804,
      "learning_rate": 4.3548216183533474e-05,
      "loss": 1.3021,
      "step": 124250
    },
    {
      "epoch": 0.3873999943900242,
      "grad_norm": 0.6581131219863892,
      "learning_rate": 4.354561897252255e-05,
      "loss": 1.2684,
      "step": 124300
    },
    {
      "epoch": 0.3875558270506799,
      "grad_norm": 0.6291976571083069,
      "learning_rate": 4.354302176151162e-05,
      "loss": 1.2919,
      "step": 124350
    },
    {
      "epoch": 0.3877116597113356,
      "grad_norm": 0.6946045756340027,
      "learning_rate": 4.354042455050069e-05,
      "loss": 1.2547,
      "step": 124400
    },
    {
      "epoch": 0.38786749237199125,
      "grad_norm": 0.5476803183555603,
      "learning_rate": 4.3537827339489764e-05,
      "loss": 1.2642,
      "step": 124450
    },
    {
      "epoch": 0.3880233250326469,
      "grad_norm": 0.7108947038650513,
      "learning_rate": 4.3535230128478837e-05,
      "loss": 1.2897,
      "step": 124500
    },
    {
      "epoch": 0.38817915769330263,
      "grad_norm": 0.7129521369934082,
      "learning_rate": 4.353263291746791e-05,
      "loss": 1.2608,
      "step": 124550
    },
    {
      "epoch": 0.3883349903539583,
      "grad_norm": 0.7266551852226257,
      "learning_rate": 4.353003570645698e-05,
      "loss": 1.2907,
      "step": 124600
    },
    {
      "epoch": 0.38849082301461396,
      "grad_norm": 0.43176567554473877,
      "learning_rate": 4.3527438495446054e-05,
      "loss": 1.3234,
      "step": 124650
    },
    {
      "epoch": 0.3886466556752697,
      "grad_norm": 0.6235330104827881,
      "learning_rate": 4.3524841284435127e-05,
      "loss": 1.2725,
      "step": 124700
    },
    {
      "epoch": 0.38880248833592534,
      "grad_norm": 0.6986300349235535,
      "learning_rate": 4.352224407342419e-05,
      "loss": 1.3004,
      "step": 124750
    },
    {
      "epoch": 0.388958320996581,
      "grad_norm": 0.7112565040588379,
      "learning_rate": 4.3519646862413265e-05,
      "loss": 1.3098,
      "step": 124800
    },
    {
      "epoch": 0.3891141536572367,
      "grad_norm": 0.7244734764099121,
      "learning_rate": 4.3517049651402344e-05,
      "loss": 1.2737,
      "step": 124850
    },
    {
      "epoch": 0.3892699863178924,
      "grad_norm": 0.5485036969184875,
      "learning_rate": 4.351445244039141e-05,
      "loss": 1.2872,
      "step": 124900
    },
    {
      "epoch": 0.38942581897854806,
      "grad_norm": 0.5238649845123291,
      "learning_rate": 4.351185522938048e-05,
      "loss": 1.2984,
      "step": 124950
    },
    {
      "epoch": 0.3895816516392038,
      "grad_norm": 0.6725048422813416,
      "learning_rate": 4.350925801836956e-05,
      "loss": 1.2756,
      "step": 125000
    },
    {
      "epoch": 0.38973748429985944,
      "grad_norm": 0.5167462825775146,
      "learning_rate": 4.350666080735863e-05,
      "loss": 1.262,
      "step": 125050
    },
    {
      "epoch": 0.3898933169605151,
      "grad_norm": 0.6059038639068604,
      "learning_rate": 4.350411554056792e-05,
      "loss": 1.2494,
      "step": 125100
    },
    {
      "epoch": 0.3900491496211708,
      "grad_norm": 0.5923278331756592,
      "learning_rate": 4.350151832955699e-05,
      "loss": 1.294,
      "step": 125150
    },
    {
      "epoch": 0.3902049822818265,
      "grad_norm": 0.6561193466186523,
      "learning_rate": 4.349892111854606e-05,
      "loss": 1.2711,
      "step": 125200
    },
    {
      "epoch": 0.39036081494248215,
      "grad_norm": 0.5371524691581726,
      "learning_rate": 4.349632390753514e-05,
      "loss": 1.2718,
      "step": 125250
    },
    {
      "epoch": 0.39051664760313787,
      "grad_norm": 0.588132381439209,
      "learning_rate": 4.349372669652421e-05,
      "loss": 1.2832,
      "step": 125300
    },
    {
      "epoch": 0.39067248026379353,
      "grad_norm": 0.5634927749633789,
      "learning_rate": 4.3491129485513275e-05,
      "loss": 1.3055,
      "step": 125350
    },
    {
      "epoch": 0.3908283129244492,
      "grad_norm": 0.604904294013977,
      "learning_rate": 4.3488532274502354e-05,
      "loss": 1.261,
      "step": 125400
    },
    {
      "epoch": 0.3909841455851049,
      "grad_norm": 0.5485107898712158,
      "learning_rate": 4.348593506349142e-05,
      "loss": 1.2781,
      "step": 125450
    },
    {
      "epoch": 0.3911399782457606,
      "grad_norm": 0.7634595036506653,
      "learning_rate": 4.348333785248049e-05,
      "loss": 1.2496,
      "step": 125500
    },
    {
      "epoch": 0.39129581090641624,
      "grad_norm": 0.5259556770324707,
      "learning_rate": 4.3480740641469565e-05,
      "loss": 1.2881,
      "step": 125550
    },
    {
      "epoch": 0.39145164356707196,
      "grad_norm": 0.7208914756774902,
      "learning_rate": 4.347814343045864e-05,
      "loss": 1.2401,
      "step": 125600
    },
    {
      "epoch": 0.3916074762277276,
      "grad_norm": 0.6037072539329529,
      "learning_rate": 4.347554621944771e-05,
      "loss": 1.2674,
      "step": 125650
    },
    {
      "epoch": 0.3917633088883833,
      "grad_norm": 0.663367748260498,
      "learning_rate": 4.347294900843678e-05,
      "loss": 1.2662,
      "step": 125700
    },
    {
      "epoch": 0.391919141549039,
      "grad_norm": 0.5298296213150024,
      "learning_rate": 4.3470351797425855e-05,
      "loss": 1.2759,
      "step": 125750
    },
    {
      "epoch": 0.39207497420969467,
      "grad_norm": 0.5614224672317505,
      "learning_rate": 4.346775458641493e-05,
      "loss": 1.2953,
      "step": 125800
    },
    {
      "epoch": 0.39223080687035033,
      "grad_norm": 0.6830313205718994,
      "learning_rate": 4.3465157375404e-05,
      "loss": 1.2939,
      "step": 125850
    },
    {
      "epoch": 0.392386639531006,
      "grad_norm": 0.5731850266456604,
      "learning_rate": 4.3462560164393066e-05,
      "loss": 1.2347,
      "step": 125900
    },
    {
      "epoch": 0.3925424721916617,
      "grad_norm": 0.6664340496063232,
      "learning_rate": 4.3459962953382145e-05,
      "loss": 1.2219,
      "step": 125950
    },
    {
      "epoch": 0.3926983048523174,
      "grad_norm": 0.5234832167625427,
      "learning_rate": 4.345736574237122e-05,
      "loss": 1.2767,
      "step": 126000
    },
    {
      "epoch": 0.39285413751297305,
      "grad_norm": 0.4368324875831604,
      "learning_rate": 4.3454768531360283e-05,
      "loss": 1.2249,
      "step": 126050
    },
    {
      "epoch": 0.39300997017362876,
      "grad_norm": 0.6030871272087097,
      "learning_rate": 4.345217132034936e-05,
      "loss": 1.3125,
      "step": 126100
    },
    {
      "epoch": 0.39316580283428443,
      "grad_norm": 0.5746402740478516,
      "learning_rate": 4.344957410933843e-05,
      "loss": 1.3391,
      "step": 126150
    },
    {
      "epoch": 0.3933216354949401,
      "grad_norm": 0.5962190628051758,
      "learning_rate": 4.34469768983275e-05,
      "loss": 1.297,
      "step": 126200
    },
    {
      "epoch": 0.3934774681555958,
      "grad_norm": 0.6135668754577637,
      "learning_rate": 4.3444379687316573e-05,
      "loss": 1.3326,
      "step": 126250
    },
    {
      "epoch": 0.3936333008162515,
      "grad_norm": 0.5225206613540649,
      "learning_rate": 4.3441782476305646e-05,
      "loss": 1.2713,
      "step": 126300
    },
    {
      "epoch": 0.39378913347690714,
      "grad_norm": 0.5913875699043274,
      "learning_rate": 4.343918526529472e-05,
      "loss": 1.2815,
      "step": 126350
    },
    {
      "epoch": 0.39394496613756286,
      "grad_norm": 0.6139169335365295,
      "learning_rate": 4.343658805428379e-05,
      "loss": 1.2923,
      "step": 126400
    },
    {
      "epoch": 0.3941007987982185,
      "grad_norm": 0.7265818119049072,
      "learning_rate": 4.343399084327286e-05,
      "loss": 1.3148,
      "step": 126450
    },
    {
      "epoch": 0.3942566314588742,
      "grad_norm": 0.5251086950302124,
      "learning_rate": 4.3431393632261936e-05,
      "loss": 1.2968,
      "step": 126500
    },
    {
      "epoch": 0.3944124641195299,
      "grad_norm": 0.6282180547714233,
      "learning_rate": 4.342879642125101e-05,
      "loss": 1.2586,
      "step": 126550
    },
    {
      "epoch": 0.39456829678018557,
      "grad_norm": 0.5280331969261169,
      "learning_rate": 4.3426199210240074e-05,
      "loss": 1.2963,
      "step": 126600
    },
    {
      "epoch": 0.39472412944084123,
      "grad_norm": 0.5932570099830627,
      "learning_rate": 4.3423601999229154e-05,
      "loss": 1.2567,
      "step": 126650
    },
    {
      "epoch": 0.39487996210149695,
      "grad_norm": 0.6541065573692322,
      "learning_rate": 4.342100478821822e-05,
      "loss": 1.3495,
      "step": 126700
    },
    {
      "epoch": 0.3950357947621526,
      "grad_norm": 0.6384478211402893,
      "learning_rate": 4.341840757720729e-05,
      "loss": 1.2941,
      "step": 126750
    },
    {
      "epoch": 0.3951916274228083,
      "grad_norm": 0.667477011680603,
      "learning_rate": 4.3415810366196364e-05,
      "loss": 1.2591,
      "step": 126800
    },
    {
      "epoch": 0.395347460083464,
      "grad_norm": 0.5436590909957886,
      "learning_rate": 4.341321315518544e-05,
      "loss": 1.3064,
      "step": 126850
    },
    {
      "epoch": 0.39550329274411966,
      "grad_norm": 0.5375807881355286,
      "learning_rate": 4.341061594417451e-05,
      "loss": 1.2709,
      "step": 126900
    },
    {
      "epoch": 0.3956591254047753,
      "grad_norm": 0.5566298365592957,
      "learning_rate": 4.340801873316358e-05,
      "loss": 1.2224,
      "step": 126950
    },
    {
      "epoch": 0.39581495806543104,
      "grad_norm": 0.5604212284088135,
      "learning_rate": 4.3405421522152654e-05,
      "loss": 1.2016,
      "step": 127000
    },
    {
      "epoch": 0.3959707907260867,
      "grad_norm": 0.6087833642959595,
      "learning_rate": 4.340282431114173e-05,
      "loss": 1.2503,
      "step": 127050
    },
    {
      "epoch": 0.39612662338674237,
      "grad_norm": 0.6958162188529968,
      "learning_rate": 4.34002271001308e-05,
      "loss": 1.2776,
      "step": 127100
    },
    {
      "epoch": 0.39628245604739804,
      "grad_norm": 0.49615204334259033,
      "learning_rate": 4.3397629889119865e-05,
      "loss": 1.3104,
      "step": 127150
    },
    {
      "epoch": 0.39643828870805375,
      "grad_norm": 0.6282706260681152,
      "learning_rate": 4.3395032678108944e-05,
      "loss": 1.2581,
      "step": 127200
    },
    {
      "epoch": 0.3965941213687094,
      "grad_norm": 0.5497264266014099,
      "learning_rate": 4.339243546709802e-05,
      "loss": 1.2839,
      "step": 127250
    },
    {
      "epoch": 0.3967499540293651,
      "grad_norm": 0.5944322347640991,
      "learning_rate": 4.338983825608708e-05,
      "loss": 1.2731,
      "step": 127300
    },
    {
      "epoch": 0.3969057866900208,
      "grad_norm": 0.6286641955375671,
      "learning_rate": 4.338724104507616e-05,
      "loss": 1.2789,
      "step": 127350
    },
    {
      "epoch": 0.39706161935067646,
      "grad_norm": 0.6595600843429565,
      "learning_rate": 4.338464383406523e-05,
      "loss": 1.3424,
      "step": 127400
    },
    {
      "epoch": 0.39721745201133213,
      "grad_norm": 0.6118144989013672,
      "learning_rate": 4.33820466230543e-05,
      "loss": 1.2628,
      "step": 127450
    },
    {
      "epoch": 0.39737328467198785,
      "grad_norm": 0.5849618315696716,
      "learning_rate": 4.337944941204337e-05,
      "loss": 1.258,
      "step": 127500
    },
    {
      "epoch": 0.3975291173326435,
      "grad_norm": 0.6171559691429138,
      "learning_rate": 4.3376852201032445e-05,
      "loss": 1.2578,
      "step": 127550
    },
    {
      "epoch": 0.3976849499932992,
      "grad_norm": 0.5631725788116455,
      "learning_rate": 4.337425499002152e-05,
      "loss": 1.2645,
      "step": 127600
    },
    {
      "epoch": 0.3978407826539549,
      "grad_norm": 0.5291363596916199,
      "learning_rate": 4.337165777901059e-05,
      "loss": 1.2561,
      "step": 127650
    },
    {
      "epoch": 0.39799661531461056,
      "grad_norm": 0.5251559615135193,
      "learning_rate": 4.3369060567999656e-05,
      "loss": 1.2632,
      "step": 127700
    },
    {
      "epoch": 0.3981524479752662,
      "grad_norm": 0.6058637499809265,
      "learning_rate": 4.3366463356988735e-05,
      "loss": 1.2824,
      "step": 127750
    },
    {
      "epoch": 0.39830828063592194,
      "grad_norm": 0.613477885723114,
      "learning_rate": 4.336386614597781e-05,
      "loss": 1.2706,
      "step": 127800
    },
    {
      "epoch": 0.3984641132965776,
      "grad_norm": 0.6389579772949219,
      "learning_rate": 4.336126893496687e-05,
      "loss": 1.2742,
      "step": 127850
    },
    {
      "epoch": 0.39861994595723327,
      "grad_norm": 0.5035978555679321,
      "learning_rate": 4.335867172395595e-05,
      "loss": 1.2641,
      "step": 127900
    },
    {
      "epoch": 0.398775778617889,
      "grad_norm": 0.5365591049194336,
      "learning_rate": 4.3356074512945025e-05,
      "loss": 1.2331,
      "step": 127950
    },
    {
      "epoch": 0.39893161127854465,
      "grad_norm": 0.751268208026886,
      "learning_rate": 4.335347730193409e-05,
      "loss": 1.2795,
      "step": 128000
    },
    {
      "epoch": 0.3990874439392003,
      "grad_norm": 0.4509322941303253,
      "learning_rate": 4.335088009092316e-05,
      "loss": 1.2602,
      "step": 128050
    },
    {
      "epoch": 0.39924327659985603,
      "grad_norm": 0.5440673828125,
      "learning_rate": 4.3348282879912236e-05,
      "loss": 1.2702,
      "step": 128100
    },
    {
      "epoch": 0.3993991092605117,
      "grad_norm": 0.6015974879264832,
      "learning_rate": 4.334568566890131e-05,
      "loss": 1.293,
      "step": 128150
    },
    {
      "epoch": 0.39955494192116736,
      "grad_norm": 0.6941984295845032,
      "learning_rate": 4.334308845789038e-05,
      "loss": 1.3,
      "step": 128200
    },
    {
      "epoch": 0.3997107745818231,
      "grad_norm": 0.5325855016708374,
      "learning_rate": 4.334049124687945e-05,
      "loss": 1.2676,
      "step": 128250
    },
    {
      "epoch": 0.39986660724247874,
      "grad_norm": 0.6768778562545776,
      "learning_rate": 4.3337894035868526e-05,
      "loss": 1.3067,
      "step": 128300
    },
    {
      "epoch": 0.4000224399031344,
      "grad_norm": 0.8519375324249268,
      "learning_rate": 4.33352968248576e-05,
      "loss": 1.318,
      "step": 128350
    },
    {
      "epoch": 0.4001782725637901,
      "grad_norm": 0.5587887763977051,
      "learning_rate": 4.3332699613846664e-05,
      "loss": 1.2037,
      "step": 128400
    },
    {
      "epoch": 0.4003341052244458,
      "grad_norm": 0.6789969205856323,
      "learning_rate": 4.333010240283574e-05,
      "loss": 1.2925,
      "step": 128450
    },
    {
      "epoch": 0.40048993788510145,
      "grad_norm": 0.5345014333724976,
      "learning_rate": 4.3327505191824816e-05,
      "loss": 1.2233,
      "step": 128500
    },
    {
      "epoch": 0.4006457705457571,
      "grad_norm": 0.537188708782196,
      "learning_rate": 4.332490798081388e-05,
      "loss": 1.2356,
      "step": 128550
    },
    {
      "epoch": 0.40080160320641284,
      "grad_norm": 0.6431164145469666,
      "learning_rate": 4.332231076980296e-05,
      "loss": 1.2475,
      "step": 128600
    },
    {
      "epoch": 0.4009574358670685,
      "grad_norm": 0.5191900134086609,
      "learning_rate": 4.331971355879203e-05,
      "loss": 1.3267,
      "step": 128650
    },
    {
      "epoch": 0.40111326852772416,
      "grad_norm": 0.5654402375221252,
      "learning_rate": 4.33171163477811e-05,
      "loss": 1.3508,
      "step": 128700
    },
    {
      "epoch": 0.4012691011883799,
      "grad_norm": 0.5472760796546936,
      "learning_rate": 4.331451913677017e-05,
      "loss": 1.2488,
      "step": 128750
    },
    {
      "epoch": 0.40142493384903555,
      "grad_norm": 0.6736763715744019,
      "learning_rate": 4.3311921925759244e-05,
      "loss": 1.2802,
      "step": 128800
    },
    {
      "epoch": 0.4015807665096912,
      "grad_norm": 0.5725709795951843,
      "learning_rate": 4.3309324714748317e-05,
      "loss": 1.2928,
      "step": 128850
    },
    {
      "epoch": 0.40173659917034693,
      "grad_norm": 0.489077627658844,
      "learning_rate": 4.330672750373739e-05,
      "loss": 1.2945,
      "step": 128900
    },
    {
      "epoch": 0.4018924318310026,
      "grad_norm": 0.630953848361969,
      "learning_rate": 4.3304130292726455e-05,
      "loss": 1.2673,
      "step": 128950
    },
    {
      "epoch": 0.40204826449165826,
      "grad_norm": 0.6124792098999023,
      "learning_rate": 4.3301533081715534e-05,
      "loss": 1.2964,
      "step": 129000
    },
    {
      "epoch": 0.402204097152314,
      "grad_norm": 0.6764472723007202,
      "learning_rate": 4.3298935870704607e-05,
      "loss": 1.2858,
      "step": 129050
    },
    {
      "epoch": 0.40235992981296964,
      "grad_norm": 0.5142732858657837,
      "learning_rate": 4.329633865969367e-05,
      "loss": 1.285,
      "step": 129100
    },
    {
      "epoch": 0.4025157624736253,
      "grad_norm": 0.5934566259384155,
      "learning_rate": 4.329374144868275e-05,
      "loss": 1.3002,
      "step": 129150
    },
    {
      "epoch": 0.402671595134281,
      "grad_norm": 0.6398521065711975,
      "learning_rate": 4.3291144237671824e-05,
      "loss": 1.2911,
      "step": 129200
    },
    {
      "epoch": 0.4028274277949367,
      "grad_norm": 0.6829758882522583,
      "learning_rate": 4.328854702666089e-05,
      "loss": 1.2812,
      "step": 129250
    },
    {
      "epoch": 0.40298326045559235,
      "grad_norm": 0.6244954466819763,
      "learning_rate": 4.328594981564996e-05,
      "loss": 1.2645,
      "step": 129300
    },
    {
      "epoch": 0.40313909311624807,
      "grad_norm": 0.5079953670501709,
      "learning_rate": 4.3283352604639035e-05,
      "loss": 1.258,
      "step": 129350
    },
    {
      "epoch": 0.40329492577690373,
      "grad_norm": 0.6256757378578186,
      "learning_rate": 4.328075539362811e-05,
      "loss": 1.2934,
      "step": 129400
    },
    {
      "epoch": 0.4034507584375594,
      "grad_norm": 0.4919795095920563,
      "learning_rate": 4.327815818261718e-05,
      "loss": 1.3096,
      "step": 129450
    },
    {
      "epoch": 0.4036065910982151,
      "grad_norm": 0.5859843492507935,
      "learning_rate": 4.327561291582647e-05,
      "loss": 1.3441,
      "step": 129500
    },
    {
      "epoch": 0.4037624237588708,
      "grad_norm": 0.5797820687294006,
      "learning_rate": 4.3273015704815544e-05,
      "loss": 1.2698,
      "step": 129550
    },
    {
      "epoch": 0.40391825641952644,
      "grad_norm": 0.6369097828865051,
      "learning_rate": 4.327041849380462e-05,
      "loss": 1.2227,
      "step": 129600
    },
    {
      "epoch": 0.40407408908018216,
      "grad_norm": 0.5268089175224304,
      "learning_rate": 4.326782128279368e-05,
      "loss": 1.2585,
      "step": 129650
    },
    {
      "epoch": 0.4042299217408378,
      "grad_norm": 0.697039783000946,
      "learning_rate": 4.3265224071782755e-05,
      "loss": 1.314,
      "step": 129700
    },
    {
      "epoch": 0.4043857544014935,
      "grad_norm": 0.49122169613838196,
      "learning_rate": 4.3262626860771834e-05,
      "loss": 1.3046,
      "step": 129750
    },
    {
      "epoch": 0.40454158706214915,
      "grad_norm": 0.5374799370765686,
      "learning_rate": 4.32600296497609e-05,
      "loss": 1.2493,
      "step": 129800
    },
    {
      "epoch": 0.4046974197228049,
      "grad_norm": 0.5329189300537109,
      "learning_rate": 4.325743243874997e-05,
      "loss": 1.2781,
      "step": 129850
    },
    {
      "epoch": 0.40485325238346054,
      "grad_norm": 0.6265085339546204,
      "learning_rate": 4.325483522773905e-05,
      "loss": 1.2291,
      "step": 129900
    },
    {
      "epoch": 0.4050090850441162,
      "grad_norm": 0.48764920234680176,
      "learning_rate": 4.325223801672812e-05,
      "loss": 1.2527,
      "step": 129950
    },
    {
      "epoch": 0.4051649177047719,
      "grad_norm": 0.6196596622467041,
      "learning_rate": 4.324964080571719e-05,
      "loss": 1.293,
      "step": 130000
    },
    {
      "epoch": 0.4053207503654276,
      "grad_norm": 0.6462233662605286,
      "learning_rate": 4.324704359470626e-05,
      "loss": 1.2563,
      "step": 130050
    },
    {
      "epoch": 0.40547658302608325,
      "grad_norm": 0.5701438784599304,
      "learning_rate": 4.3244446383695335e-05,
      "loss": 1.2196,
      "step": 130100
    },
    {
      "epoch": 0.40563241568673897,
      "grad_norm": 0.6551206707954407,
      "learning_rate": 4.324184917268441e-05,
      "loss": 1.2401,
      "step": 130150
    },
    {
      "epoch": 0.40578824834739463,
      "grad_norm": 0.5403002500534058,
      "learning_rate": 4.323925196167348e-05,
      "loss": 1.217,
      "step": 130200
    },
    {
      "epoch": 0.4059440810080503,
      "grad_norm": 0.7770232558250427,
      "learning_rate": 4.323665475066255e-05,
      "loss": 1.2926,
      "step": 130250
    },
    {
      "epoch": 0.406099913668706,
      "grad_norm": 0.6480324864387512,
      "learning_rate": 4.3234057539651625e-05,
      "loss": 1.2762,
      "step": 130300
    },
    {
      "epoch": 0.4062557463293617,
      "grad_norm": 0.6065077781677246,
      "learning_rate": 4.323146032864069e-05,
      "loss": 1.2941,
      "step": 130350
    },
    {
      "epoch": 0.40641157899001734,
      "grad_norm": 0.5054750442504883,
      "learning_rate": 4.3228863117629763e-05,
      "loss": 1.2928,
      "step": 130400
    },
    {
      "epoch": 0.40656741165067306,
      "grad_norm": 0.6750600934028625,
      "learning_rate": 4.322626590661884e-05,
      "loss": 1.27,
      "step": 130450
    },
    {
      "epoch": 0.4067232443113287,
      "grad_norm": 0.6010610461235046,
      "learning_rate": 4.322366869560791e-05,
      "loss": 1.2583,
      "step": 130500
    },
    {
      "epoch": 0.4068790769719844,
      "grad_norm": 0.7199938297271729,
      "learning_rate": 4.322107148459698e-05,
      "loss": 1.2989,
      "step": 130550
    },
    {
      "epoch": 0.4070349096326401,
      "grad_norm": 0.5829344987869263,
      "learning_rate": 4.321847427358606e-05,
      "loss": 1.2375,
      "step": 130600
    },
    {
      "epoch": 0.40719074229329577,
      "grad_norm": 0.5924407839775085,
      "learning_rate": 4.3215877062575126e-05,
      "loss": 1.317,
      "step": 130650
    },
    {
      "epoch": 0.40734657495395143,
      "grad_norm": 0.7201712131500244,
      "learning_rate": 4.32132798515642e-05,
      "loss": 1.2578,
      "step": 130700
    },
    {
      "epoch": 0.40750240761460715,
      "grad_norm": 0.5803160667419434,
      "learning_rate": 4.321068264055327e-05,
      "loss": 1.2995,
      "step": 130750
    },
    {
      "epoch": 0.4076582402752628,
      "grad_norm": 0.6341825723648071,
      "learning_rate": 4.3208085429542343e-05,
      "loss": 1.3242,
      "step": 130800
    },
    {
      "epoch": 0.4078140729359185,
      "grad_norm": 0.49980130791664124,
      "learning_rate": 4.3205488218531416e-05,
      "loss": 1.2384,
      "step": 130850
    },
    {
      "epoch": 0.4079699055965742,
      "grad_norm": 0.7559208273887634,
      "learning_rate": 4.320289100752049e-05,
      "loss": 1.2741,
      "step": 130900
    },
    {
      "epoch": 0.40812573825722986,
      "grad_norm": 0.6077607870101929,
      "learning_rate": 4.3200293796509554e-05,
      "loss": 1.2546,
      "step": 130950
    },
    {
      "epoch": 0.40828157091788553,
      "grad_norm": 0.5931982398033142,
      "learning_rate": 4.3197696585498633e-05,
      "loss": 1.2207,
      "step": 131000
    },
    {
      "epoch": 0.4084374035785412,
      "grad_norm": 0.5656701326370239,
      "learning_rate": 4.31950993744877e-05,
      "loss": 1.3415,
      "step": 131050
    },
    {
      "epoch": 0.4085932362391969,
      "grad_norm": 0.6615855097770691,
      "learning_rate": 4.319250216347677e-05,
      "loss": 1.2774,
      "step": 131100
    },
    {
      "epoch": 0.4087490688998526,
      "grad_norm": 0.6351200342178345,
      "learning_rate": 4.318990495246585e-05,
      "loss": 1.2805,
      "step": 131150
    },
    {
      "epoch": 0.40890490156050824,
      "grad_norm": 0.4892980754375458,
      "learning_rate": 4.318730774145492e-05,
      "loss": 1.2725,
      "step": 131200
    },
    {
      "epoch": 0.40906073422116396,
      "grad_norm": 0.5204506516456604,
      "learning_rate": 4.318471053044399e-05,
      "loss": 1.2731,
      "step": 131250
    },
    {
      "epoch": 0.4092165668818196,
      "grad_norm": 0.7032626867294312,
      "learning_rate": 4.318211331943306e-05,
      "loss": 1.261,
      "step": 131300
    },
    {
      "epoch": 0.4093723995424753,
      "grad_norm": 0.5166312456130981,
      "learning_rate": 4.3179516108422134e-05,
      "loss": 1.3832,
      "step": 131350
    },
    {
      "epoch": 0.409528232203131,
      "grad_norm": 0.5219990611076355,
      "learning_rate": 4.317691889741121e-05,
      "loss": 1.2572,
      "step": 131400
    },
    {
      "epoch": 0.40968406486378667,
      "grad_norm": 0.6357908248901367,
      "learning_rate": 4.317432168640028e-05,
      "loss": 1.3315,
      "step": 131450
    },
    {
      "epoch": 0.40983989752444233,
      "grad_norm": 0.6401694416999817,
      "learning_rate": 4.317172447538935e-05,
      "loss": 1.237,
      "step": 131500
    },
    {
      "epoch": 0.40999573018509805,
      "grad_norm": 0.5995790958404541,
      "learning_rate": 4.3169127264378424e-05,
      "loss": 1.3008,
      "step": 131550
    },
    {
      "epoch": 0.4101515628457537,
      "grad_norm": 0.7261896133422852,
      "learning_rate": 4.316653005336749e-05,
      "loss": 1.2792,
      "step": 131600
    },
    {
      "epoch": 0.4103073955064094,
      "grad_norm": 0.5053651332855225,
      "learning_rate": 4.316393284235656e-05,
      "loss": 1.2937,
      "step": 131650
    },
    {
      "epoch": 0.4104632281670651,
      "grad_norm": 0.6622916460037231,
      "learning_rate": 4.316133563134564e-05,
      "loss": 1.3161,
      "step": 131700
    },
    {
      "epoch": 0.41061906082772076,
      "grad_norm": 0.5868291258811951,
      "learning_rate": 4.315873842033471e-05,
      "loss": 1.2692,
      "step": 131750
    },
    {
      "epoch": 0.4107748934883764,
      "grad_norm": 0.7292623519897461,
      "learning_rate": 4.3156193153544e-05,
      "loss": 1.3226,
      "step": 131800
    },
    {
      "epoch": 0.41093072614903214,
      "grad_norm": 0.6183932423591614,
      "learning_rate": 4.315359594253307e-05,
      "loss": 1.2887,
      "step": 131850
    },
    {
      "epoch": 0.4110865588096878,
      "grad_norm": 0.6388256549835205,
      "learning_rate": 4.3150998731522145e-05,
      "loss": 1.2829,
      "step": 131900
    },
    {
      "epoch": 0.41124239147034347,
      "grad_norm": 0.49520039558410645,
      "learning_rate": 4.314840152051122e-05,
      "loss": 1.3484,
      "step": 131950
    },
    {
      "epoch": 0.4113982241309992,
      "grad_norm": 0.6130410432815552,
      "learning_rate": 4.314580430950029e-05,
      "loss": 1.2427,
      "step": 132000
    },
    {
      "epoch": 0.41155405679165485,
      "grad_norm": 0.721832275390625,
      "learning_rate": 4.3143207098489355e-05,
      "loss": 1.2922,
      "step": 132050
    },
    {
      "epoch": 0.4117098894523105,
      "grad_norm": 0.6415041089057922,
      "learning_rate": 4.3140609887478435e-05,
      "loss": 1.2616,
      "step": 132100
    },
    {
      "epoch": 0.41186572211296624,
      "grad_norm": 0.5906524062156677,
      "learning_rate": 4.313801267646751e-05,
      "loss": 1.2761,
      "step": 132150
    },
    {
      "epoch": 0.4120215547736219,
      "grad_norm": 0.6394011378288269,
      "learning_rate": 4.313541546545657e-05,
      "loss": 1.273,
      "step": 132200
    },
    {
      "epoch": 0.41217738743427756,
      "grad_norm": 0.7864432334899902,
      "learning_rate": 4.313281825444565e-05,
      "loss": 1.2778,
      "step": 132250
    },
    {
      "epoch": 0.4123332200949333,
      "grad_norm": 0.6063726544380188,
      "learning_rate": 4.313022104343472e-05,
      "loss": 1.3192,
      "step": 132300
    },
    {
      "epoch": 0.41248905275558895,
      "grad_norm": 0.8228977918624878,
      "learning_rate": 4.312762383242379e-05,
      "loss": 1.2768,
      "step": 132350
    },
    {
      "epoch": 0.4126448854162446,
      "grad_norm": 0.7078390717506409,
      "learning_rate": 4.312502662141286e-05,
      "loss": 1.2472,
      "step": 132400
    },
    {
      "epoch": 0.4128007180769003,
      "grad_norm": 0.6323947906494141,
      "learning_rate": 4.3122429410401935e-05,
      "loss": 1.2948,
      "step": 132450
    },
    {
      "epoch": 0.412956550737556,
      "grad_norm": 0.6630433797836304,
      "learning_rate": 4.311983219939101e-05,
      "loss": 1.2886,
      "step": 132500
    },
    {
      "epoch": 0.41311238339821166,
      "grad_norm": 0.5654430985450745,
      "learning_rate": 4.311723498838008e-05,
      "loss": 1.2513,
      "step": 132550
    },
    {
      "epoch": 0.4132682160588673,
      "grad_norm": 0.6478428244590759,
      "learning_rate": 4.311463777736915e-05,
      "loss": 1.3347,
      "step": 132600
    },
    {
      "epoch": 0.41342404871952304,
      "grad_norm": 0.6228793859481812,
      "learning_rate": 4.3112040566358225e-05,
      "loss": 1.2784,
      "step": 132650
    },
    {
      "epoch": 0.4135798813801787,
      "grad_norm": 0.5482504367828369,
      "learning_rate": 4.31094433553473e-05,
      "loss": 1.3391,
      "step": 132700
    },
    {
      "epoch": 0.41373571404083437,
      "grad_norm": 0.5533774495124817,
      "learning_rate": 4.3106846144336364e-05,
      "loss": 1.3171,
      "step": 132750
    },
    {
      "epoch": 0.4138915467014901,
      "grad_norm": 0.5980605483055115,
      "learning_rate": 4.310424893332544e-05,
      "loss": 1.2508,
      "step": 132800
    },
    {
      "epoch": 0.41404737936214575,
      "grad_norm": 0.6519829630851746,
      "learning_rate": 4.3101651722314515e-05,
      "loss": 1.2391,
      "step": 132850
    },
    {
      "epoch": 0.4142032120228014,
      "grad_norm": 0.6122337579727173,
      "learning_rate": 4.309905451130358e-05,
      "loss": 1.2808,
      "step": 132900
    },
    {
      "epoch": 0.41435904468345713,
      "grad_norm": 0.5463085770606995,
      "learning_rate": 4.3096457300292654e-05,
      "loss": 1.2753,
      "step": 132950
    },
    {
      "epoch": 0.4145148773441128,
      "grad_norm": 0.6509996652603149,
      "learning_rate": 4.3093860089281726e-05,
      "loss": 1.2505,
      "step": 133000
    },
    {
      "epoch": 0.41467071000476846,
      "grad_norm": 0.6057698726654053,
      "learning_rate": 4.30912628782708e-05,
      "loss": 1.2641,
      "step": 133050
    },
    {
      "epoch": 0.4148265426654242,
      "grad_norm": 0.5532364249229431,
      "learning_rate": 4.308866566725987e-05,
      "loss": 1.3049,
      "step": 133100
    },
    {
      "epoch": 0.41498237532607984,
      "grad_norm": 0.6781042218208313,
      "learning_rate": 4.3086068456248944e-05,
      "loss": 1.2316,
      "step": 133150
    },
    {
      "epoch": 0.4151382079867355,
      "grad_norm": 0.6927700638771057,
      "learning_rate": 4.3083471245238016e-05,
      "loss": 1.2672,
      "step": 133200
    },
    {
      "epoch": 0.4152940406473912,
      "grad_norm": 0.5728213787078857,
      "learning_rate": 4.308087403422709e-05,
      "loss": 1.2869,
      "step": 133250
    },
    {
      "epoch": 0.4154498733080469,
      "grad_norm": 0.6179444193840027,
      "learning_rate": 4.3078276823216154e-05,
      "loss": 1.3222,
      "step": 133300
    },
    {
      "epoch": 0.41560570596870255,
      "grad_norm": 0.6959733963012695,
      "learning_rate": 4.3075679612205234e-05,
      "loss": 1.3082,
      "step": 133350
    },
    {
      "epoch": 0.4157615386293583,
      "grad_norm": 0.5714619755744934,
      "learning_rate": 4.3073082401194306e-05,
      "loss": 1.2602,
      "step": 133400
    },
    {
      "epoch": 0.41591737129001394,
      "grad_norm": 0.6184346079826355,
      "learning_rate": 4.307048519018337e-05,
      "loss": 1.3465,
      "step": 133450
    },
    {
      "epoch": 0.4160732039506696,
      "grad_norm": 0.7281902432441711,
      "learning_rate": 4.306788797917245e-05,
      "loss": 1.2647,
      "step": 133500
    },
    {
      "epoch": 0.4162290366113253,
      "grad_norm": 0.7990896105766296,
      "learning_rate": 4.3065290768161524e-05,
      "loss": 1.2664,
      "step": 133550
    },
    {
      "epoch": 0.416384869271981,
      "grad_norm": 0.5374918580055237,
      "learning_rate": 4.306269355715059e-05,
      "loss": 1.2673,
      "step": 133600
    },
    {
      "epoch": 0.41654070193263665,
      "grad_norm": 0.546167254447937,
      "learning_rate": 4.306009634613966e-05,
      "loss": 1.2635,
      "step": 133650
    },
    {
      "epoch": 0.4166965345932923,
      "grad_norm": 0.5561685562133789,
      "learning_rate": 4.3057499135128734e-05,
      "loss": 1.2513,
      "step": 133700
    },
    {
      "epoch": 0.41685236725394803,
      "grad_norm": 0.6610720157623291,
      "learning_rate": 4.305490192411781e-05,
      "loss": 1.2539,
      "step": 133750
    },
    {
      "epoch": 0.4170081999146037,
      "grad_norm": 0.4526958167552948,
      "learning_rate": 4.305230471310688e-05,
      "loss": 1.2644,
      "step": 133800
    },
    {
      "epoch": 0.41716403257525936,
      "grad_norm": 0.681488037109375,
      "learning_rate": 4.304970750209595e-05,
      "loss": 1.2808,
      "step": 133850
    },
    {
      "epoch": 0.4173198652359151,
      "grad_norm": 0.6878519654273987,
      "learning_rate": 4.3047110291085024e-05,
      "loss": 1.3418,
      "step": 133900
    },
    {
      "epoch": 0.41747569789657074,
      "grad_norm": 0.6553367376327515,
      "learning_rate": 4.30445130800741e-05,
      "loss": 1.2694,
      "step": 133950
    },
    {
      "epoch": 0.4176315305572264,
      "grad_norm": 0.7013148069381714,
      "learning_rate": 4.304191586906316e-05,
      "loss": 1.2725,
      "step": 134000
    },
    {
      "epoch": 0.4177873632178821,
      "grad_norm": 0.6055176258087158,
      "learning_rate": 4.303931865805224e-05,
      "loss": 1.3033,
      "step": 134050
    },
    {
      "epoch": 0.4179431958785378,
      "grad_norm": 0.511165201663971,
      "learning_rate": 4.3036721447041314e-05,
      "loss": 1.3175,
      "step": 134100
    },
    {
      "epoch": 0.41809902853919345,
      "grad_norm": 0.6487141847610474,
      "learning_rate": 4.303412423603038e-05,
      "loss": 1.296,
      "step": 134150
    },
    {
      "epoch": 0.41825486119984917,
      "grad_norm": 0.6904723048210144,
      "learning_rate": 4.303152702501945e-05,
      "loss": 1.2698,
      "step": 134200
    },
    {
      "epoch": 0.41841069386050483,
      "grad_norm": 0.46414878964424133,
      "learning_rate": 4.3028929814008525e-05,
      "loss": 1.2477,
      "step": 134250
    },
    {
      "epoch": 0.4185665265211605,
      "grad_norm": 0.5957569479942322,
      "learning_rate": 4.302638454721782e-05,
      "loss": 1.3308,
      "step": 134300
    },
    {
      "epoch": 0.4187223591818162,
      "grad_norm": 0.6301669478416443,
      "learning_rate": 4.302378733620689e-05,
      "loss": 1.334,
      "step": 134350
    },
    {
      "epoch": 0.4188781918424719,
      "grad_norm": 0.5405575633049011,
      "learning_rate": 4.302119012519596e-05,
      "loss": 1.3327,
      "step": 134400
    },
    {
      "epoch": 0.41903402450312754,
      "grad_norm": 0.48541060090065,
      "learning_rate": 4.3018592914185035e-05,
      "loss": 1.3085,
      "step": 134450
    },
    {
      "epoch": 0.41918985716378326,
      "grad_norm": 0.6393131613731384,
      "learning_rate": 4.301599570317411e-05,
      "loss": 1.3401,
      "step": 134500
    },
    {
      "epoch": 0.4193456898244389,
      "grad_norm": 0.5676529407501221,
      "learning_rate": 4.301339849216317e-05,
      "loss": 1.2761,
      "step": 134550
    },
    {
      "epoch": 0.4195015224850946,
      "grad_norm": 0.7045438289642334,
      "learning_rate": 4.301080128115225e-05,
      "loss": 1.2895,
      "step": 134600
    },
    {
      "epoch": 0.4196573551457503,
      "grad_norm": 0.5100283026695251,
      "learning_rate": 4.3008204070141325e-05,
      "loss": 1.2709,
      "step": 134650
    },
    {
      "epoch": 0.419813187806406,
      "grad_norm": 0.5259745121002197,
      "learning_rate": 4.300560685913039e-05,
      "loss": 1.2815,
      "step": 134700
    },
    {
      "epoch": 0.41996902046706164,
      "grad_norm": 0.5026425719261169,
      "learning_rate": 4.300300964811946e-05,
      "loss": 1.2993,
      "step": 134750
    },
    {
      "epoch": 0.42012485312771736,
      "grad_norm": 0.5190137028694153,
      "learning_rate": 4.300041243710854e-05,
      "loss": 1.3068,
      "step": 134800
    },
    {
      "epoch": 0.420280685788373,
      "grad_norm": 0.49419093132019043,
      "learning_rate": 4.299781522609761e-05,
      "loss": 1.2816,
      "step": 134850
    },
    {
      "epoch": 0.4204365184490287,
      "grad_norm": 0.5065749287605286,
      "learning_rate": 4.299521801508668e-05,
      "loss": 1.2392,
      "step": 134900
    },
    {
      "epoch": 0.4205923511096844,
      "grad_norm": 0.601936399936676,
      "learning_rate": 4.299262080407575e-05,
      "loss": 1.3378,
      "step": 134950
    },
    {
      "epoch": 0.42074818377034007,
      "grad_norm": 0.6668760776519775,
      "learning_rate": 4.2990023593064826e-05,
      "loss": 1.3019,
      "step": 135000
    },
    {
      "epoch": 0.42090401643099573,
      "grad_norm": 0.5293488502502441,
      "learning_rate": 4.29874263820539e-05,
      "loss": 1.2718,
      "step": 135050
    },
    {
      "epoch": 0.4210598490916514,
      "grad_norm": 0.6920080780982971,
      "learning_rate": 4.298482917104297e-05,
      "loss": 1.2905,
      "step": 135100
    },
    {
      "epoch": 0.4212156817523071,
      "grad_norm": 0.4742751717567444,
      "learning_rate": 4.298223196003204e-05,
      "loss": 1.2787,
      "step": 135150
    },
    {
      "epoch": 0.4213715144129628,
      "grad_norm": 0.7729480862617493,
      "learning_rate": 4.2979634749021116e-05,
      "loss": 1.2956,
      "step": 135200
    },
    {
      "epoch": 0.42152734707361844,
      "grad_norm": 0.5169295072555542,
      "learning_rate": 4.297703753801018e-05,
      "loss": 1.2994,
      "step": 135250
    },
    {
      "epoch": 0.42168317973427416,
      "grad_norm": 0.5328879356384277,
      "learning_rate": 4.2974440326999254e-05,
      "loss": 1.2976,
      "step": 135300
    },
    {
      "epoch": 0.4218390123949298,
      "grad_norm": 0.5818734765052795,
      "learning_rate": 4.297184311598833e-05,
      "loss": 1.3146,
      "step": 135350
    },
    {
      "epoch": 0.4219948450555855,
      "grad_norm": 0.622751772403717,
      "learning_rate": 4.29692459049774e-05,
      "loss": 1.2882,
      "step": 135400
    },
    {
      "epoch": 0.4221506777162412,
      "grad_norm": 0.6969478726387024,
      "learning_rate": 4.296664869396647e-05,
      "loss": 1.2768,
      "step": 135450
    },
    {
      "epoch": 0.42230651037689687,
      "grad_norm": 0.5776302814483643,
      "learning_rate": 4.296405148295555e-05,
      "loss": 1.2813,
      "step": 135500
    },
    {
      "epoch": 0.42246234303755253,
      "grad_norm": 0.5693468451499939,
      "learning_rate": 4.2961454271944616e-05,
      "loss": 1.2448,
      "step": 135550
    },
    {
      "epoch": 0.42261817569820825,
      "grad_norm": 0.6402776837348938,
      "learning_rate": 4.295885706093369e-05,
      "loss": 1.2645,
      "step": 135600
    },
    {
      "epoch": 0.4227740083588639,
      "grad_norm": 0.524029016494751,
      "learning_rate": 4.295625984992276e-05,
      "loss": 1.2896,
      "step": 135650
    },
    {
      "epoch": 0.4229298410195196,
      "grad_norm": 0.5608789324760437,
      "learning_rate": 4.2953662638911834e-05,
      "loss": 1.2815,
      "step": 135700
    },
    {
      "epoch": 0.4230856736801753,
      "grad_norm": 0.5245426297187805,
      "learning_rate": 4.2951065427900906e-05,
      "loss": 1.2936,
      "step": 135750
    },
    {
      "epoch": 0.42324150634083096,
      "grad_norm": 0.5902687311172485,
      "learning_rate": 4.294846821688998e-05,
      "loss": 1.3004,
      "step": 135800
    },
    {
      "epoch": 0.42339733900148663,
      "grad_norm": 0.6973357200622559,
      "learning_rate": 4.294587100587905e-05,
      "loss": 1.2743,
      "step": 135850
    },
    {
      "epoch": 0.42355317166214235,
      "grad_norm": 0.5286003947257996,
      "learning_rate": 4.2943273794868124e-05,
      "loss": 1.2702,
      "step": 135900
    },
    {
      "epoch": 0.423709004322798,
      "grad_norm": 0.7148835062980652,
      "learning_rate": 4.294067658385719e-05,
      "loss": 1.2779,
      "step": 135950
    },
    {
      "epoch": 0.4238648369834537,
      "grad_norm": 0.6468535661697388,
      "learning_rate": 4.293807937284626e-05,
      "loss": 1.2702,
      "step": 136000
    },
    {
      "epoch": 0.4240206696441094,
      "grad_norm": 0.4808603525161743,
      "learning_rate": 4.2935534106055554e-05,
      "loss": 1.2628,
      "step": 136050
    },
    {
      "epoch": 0.42417650230476506,
      "grad_norm": 0.5130918025970459,
      "learning_rate": 4.293293689504463e-05,
      "loss": 1.2665,
      "step": 136100
    },
    {
      "epoch": 0.4243323349654207,
      "grad_norm": 0.7057242393493652,
      "learning_rate": 4.29303396840337e-05,
      "loss": 1.2514,
      "step": 136150
    },
    {
      "epoch": 0.42448816762607644,
      "grad_norm": 0.6227570176124573,
      "learning_rate": 4.292774247302277e-05,
      "loss": 1.2773,
      "step": 136200
    },
    {
      "epoch": 0.4246440002867321,
      "grad_norm": 0.5839701294898987,
      "learning_rate": 4.2925145262011844e-05,
      "loss": 1.2843,
      "step": 136250
    },
    {
      "epoch": 0.42479983294738777,
      "grad_norm": 0.5704029202461243,
      "learning_rate": 4.292254805100092e-05,
      "loss": 1.2423,
      "step": 136300
    },
    {
      "epoch": 0.42495566560804343,
      "grad_norm": 0.5972390174865723,
      "learning_rate": 4.291995083998999e-05,
      "loss": 1.2909,
      "step": 136350
    },
    {
      "epoch": 0.42511149826869915,
      "grad_norm": 0.6971369385719299,
      "learning_rate": 4.2917353628979055e-05,
      "loss": 1.3464,
      "step": 136400
    },
    {
      "epoch": 0.4252673309293548,
      "grad_norm": 0.5785402059555054,
      "learning_rate": 4.2914756417968134e-05,
      "loss": 1.3256,
      "step": 136450
    },
    {
      "epoch": 0.4254231635900105,
      "grad_norm": 0.6358340382575989,
      "learning_rate": 4.29121592069572e-05,
      "loss": 1.2423,
      "step": 136500
    },
    {
      "epoch": 0.4255789962506662,
      "grad_norm": 0.5971900224685669,
      "learning_rate": 4.290956199594627e-05,
      "loss": 1.275,
      "step": 136550
    },
    {
      "epoch": 0.42573482891132186,
      "grad_norm": 0.5716942548751831,
      "learning_rate": 4.290696478493535e-05,
      "loss": 1.3225,
      "step": 136600
    },
    {
      "epoch": 0.4258906615719775,
      "grad_norm": 0.4922384023666382,
      "learning_rate": 4.290436757392442e-05,
      "loss": 1.2535,
      "step": 136650
    },
    {
      "epoch": 0.42604649423263324,
      "grad_norm": 0.6209067106246948,
      "learning_rate": 4.290177036291349e-05,
      "loss": 1.2895,
      "step": 136700
    },
    {
      "epoch": 0.4262023268932889,
      "grad_norm": 0.6045655608177185,
      "learning_rate": 4.289917315190256e-05,
      "loss": 1.37,
      "step": 136750
    },
    {
      "epoch": 0.42635815955394457,
      "grad_norm": 0.6353049278259277,
      "learning_rate": 4.2896575940891635e-05,
      "loss": 1.2928,
      "step": 136800
    },
    {
      "epoch": 0.4265139922146003,
      "grad_norm": 0.5744171738624573,
      "learning_rate": 4.289397872988071e-05,
      "loss": 1.1789,
      "step": 136850
    },
    {
      "epoch": 0.42666982487525595,
      "grad_norm": 0.5428194403648376,
      "learning_rate": 4.289138151886978e-05,
      "loss": 1.2515,
      "step": 136900
    },
    {
      "epoch": 0.4268256575359116,
      "grad_norm": 0.7506817579269409,
      "learning_rate": 4.288878430785885e-05,
      "loss": 1.2828,
      "step": 136950
    },
    {
      "epoch": 0.42698149019656734,
      "grad_norm": 0.426395058631897,
      "learning_rate": 4.2886187096847925e-05,
      "loss": 1.2325,
      "step": 137000
    },
    {
      "epoch": 0.427137322857223,
      "grad_norm": 0.6005715727806091,
      "learning_rate": 4.2883589885837e-05,
      "loss": 1.3163,
      "step": 137050
    },
    {
      "epoch": 0.42729315551787866,
      "grad_norm": 0.512582004070282,
      "learning_rate": 4.288099267482606e-05,
      "loss": 1.3013,
      "step": 137100
    },
    {
      "epoch": 0.4274489881785344,
      "grad_norm": 0.5658825039863586,
      "learning_rate": 4.287839546381514e-05,
      "loss": 1.2551,
      "step": 137150
    },
    {
      "epoch": 0.42760482083919005,
      "grad_norm": 0.5816141963005066,
      "learning_rate": 4.287579825280421e-05,
      "loss": 1.236,
      "step": 137200
    },
    {
      "epoch": 0.4277606534998457,
      "grad_norm": 0.6544012427330017,
      "learning_rate": 4.287320104179328e-05,
      "loss": 1.2906,
      "step": 137250
    },
    {
      "epoch": 0.42791648616050143,
      "grad_norm": 0.5466270446777344,
      "learning_rate": 4.287060383078235e-05,
      "loss": 1.3371,
      "step": 137300
    },
    {
      "epoch": 0.4280723188211571,
      "grad_norm": 0.6532124280929565,
      "learning_rate": 4.2868006619771426e-05,
      "loss": 1.2528,
      "step": 137350
    },
    {
      "epoch": 0.42822815148181276,
      "grad_norm": 0.5832200646400452,
      "learning_rate": 4.28654094087605e-05,
      "loss": 1.2415,
      "step": 137400
    },
    {
      "epoch": 0.4283839841424685,
      "grad_norm": 0.5541166663169861,
      "learning_rate": 4.286281219774957e-05,
      "loss": 1.2795,
      "step": 137450
    },
    {
      "epoch": 0.42853981680312414,
      "grad_norm": 0.5627046227455139,
      "learning_rate": 4.286021498673864e-05,
      "loss": 1.289,
      "step": 137500
    },
    {
      "epoch": 0.4286956494637798,
      "grad_norm": 0.6968520879745483,
      "learning_rate": 4.2857617775727716e-05,
      "loss": 1.2769,
      "step": 137550
    },
    {
      "epoch": 0.4288514821244355,
      "grad_norm": 0.5807182788848877,
      "learning_rate": 4.285502056471679e-05,
      "loss": 1.289,
      "step": 137600
    },
    {
      "epoch": 0.4290073147850912,
      "grad_norm": 0.5923229455947876,
      "learning_rate": 4.2852423353705854e-05,
      "loss": 1.2832,
      "step": 137650
    },
    {
      "epoch": 0.42916314744574685,
      "grad_norm": 0.5978808999061584,
      "learning_rate": 4.284982614269493e-05,
      "loss": 1.2632,
      "step": 137700
    },
    {
      "epoch": 0.4293189801064025,
      "grad_norm": 0.5511294007301331,
      "learning_rate": 4.2847228931684006e-05,
      "loss": 1.2741,
      "step": 137750
    },
    {
      "epoch": 0.42947481276705823,
      "grad_norm": 0.5512678027153015,
      "learning_rate": 4.284463172067307e-05,
      "loss": 1.3152,
      "step": 137800
    },
    {
      "epoch": 0.4296306454277139,
      "grad_norm": 0.548896312713623,
      "learning_rate": 4.284203450966215e-05,
      "loss": 1.2734,
      "step": 137850
    },
    {
      "epoch": 0.42978647808836956,
      "grad_norm": 0.5124638676643372,
      "learning_rate": 4.2839437298651216e-05,
      "loss": 1.2974,
      "step": 137900
    },
    {
      "epoch": 0.4299423107490253,
      "grad_norm": 0.5933184027671814,
      "learning_rate": 4.283684008764029e-05,
      "loss": 1.2114,
      "step": 137950
    },
    {
      "epoch": 0.43009814340968094,
      "grad_norm": 0.5052688717842102,
      "learning_rate": 4.283424287662936e-05,
      "loss": 1.2629,
      "step": 138000
    },
    {
      "epoch": 0.4302539760703366,
      "grad_norm": 0.4373190402984619,
      "learning_rate": 4.2831645665618434e-05,
      "loss": 1.2851,
      "step": 138050
    },
    {
      "epoch": 0.4304098087309923,
      "grad_norm": 0.5971293449401855,
      "learning_rate": 4.2829048454607506e-05,
      "loss": 1.2068,
      "step": 138100
    },
    {
      "epoch": 0.430565641391648,
      "grad_norm": 0.6261790990829468,
      "learning_rate": 4.282645124359658e-05,
      "loss": 1.309,
      "step": 138150
    },
    {
      "epoch": 0.43072147405230365,
      "grad_norm": 0.609525203704834,
      "learning_rate": 4.282385403258565e-05,
      "loss": 1.2635,
      "step": 138200
    },
    {
      "epoch": 0.4308773067129594,
      "grad_norm": 0.6428813338279724,
      "learning_rate": 4.2821256821574724e-05,
      "loss": 1.29,
      "step": 138250
    },
    {
      "epoch": 0.43103313937361504,
      "grad_norm": 0.4301297068595886,
      "learning_rate": 4.2818659610563796e-05,
      "loss": 1.2771,
      "step": 138300
    },
    {
      "epoch": 0.4311889720342707,
      "grad_norm": 0.6259307265281677,
      "learning_rate": 4.281606239955286e-05,
      "loss": 1.2982,
      "step": 138350
    },
    {
      "epoch": 0.4313448046949264,
      "grad_norm": 0.600193977355957,
      "learning_rate": 4.281346518854194e-05,
      "loss": 1.2506,
      "step": 138400
    },
    {
      "epoch": 0.4315006373555821,
      "grad_norm": 0.5226466059684753,
      "learning_rate": 4.2810867977531014e-05,
      "loss": 1.3132,
      "step": 138450
    },
    {
      "epoch": 0.43165647001623775,
      "grad_norm": 0.602805495262146,
      "learning_rate": 4.280827076652008e-05,
      "loss": 1.3091,
      "step": 138500
    },
    {
      "epoch": 0.43181230267689347,
      "grad_norm": 0.6254682540893555,
      "learning_rate": 4.280567355550915e-05,
      "loss": 1.2184,
      "step": 138550
    },
    {
      "epoch": 0.43196813533754913,
      "grad_norm": 0.6318889856338501,
      "learning_rate": 4.2803076344498225e-05,
      "loss": 1.2317,
      "step": 138600
    },
    {
      "epoch": 0.4321239679982048,
      "grad_norm": 0.7016654014587402,
      "learning_rate": 4.28004791334873e-05,
      "loss": 1.279,
      "step": 138650
    },
    {
      "epoch": 0.4322798006588605,
      "grad_norm": 0.5745775103569031,
      "learning_rate": 4.279788192247637e-05,
      "loss": 1.2939,
      "step": 138700
    },
    {
      "epoch": 0.4324356333195162,
      "grad_norm": 0.5973522663116455,
      "learning_rate": 4.279528471146544e-05,
      "loss": 1.2341,
      "step": 138750
    },
    {
      "epoch": 0.43259146598017184,
      "grad_norm": 0.555239200592041,
      "learning_rate": 4.2792687500454515e-05,
      "loss": 1.2906,
      "step": 138800
    },
    {
      "epoch": 0.43274729864082756,
      "grad_norm": 0.6405017971992493,
      "learning_rate": 4.279009028944359e-05,
      "loss": 1.2724,
      "step": 138850
    },
    {
      "epoch": 0.4329031313014832,
      "grad_norm": 0.4608364701271057,
      "learning_rate": 4.278749307843265e-05,
      "loss": 1.3082,
      "step": 138900
    },
    {
      "epoch": 0.4330589639621389,
      "grad_norm": 0.5633665323257446,
      "learning_rate": 4.278489586742173e-05,
      "loss": 1.275,
      "step": 138950
    },
    {
      "epoch": 0.43321479662279455,
      "grad_norm": 0.6338832378387451,
      "learning_rate": 4.2782298656410805e-05,
      "loss": 1.3107,
      "step": 139000
    },
    {
      "epoch": 0.43337062928345027,
      "grad_norm": 0.7238165140151978,
      "learning_rate": 4.277970144539987e-05,
      "loss": 1.2349,
      "step": 139050
    },
    {
      "epoch": 0.43352646194410593,
      "grad_norm": 0.5238448977470398,
      "learning_rate": 4.277710423438895e-05,
      "loss": 1.2755,
      "step": 139100
    },
    {
      "epoch": 0.4336822946047616,
      "grad_norm": 0.545289158821106,
      "learning_rate": 4.2774507023378015e-05,
      "loss": 1.227,
      "step": 139150
    },
    {
      "epoch": 0.4338381272654173,
      "grad_norm": 0.6670827865600586,
      "learning_rate": 4.277190981236709e-05,
      "loss": 1.2742,
      "step": 139200
    },
    {
      "epoch": 0.433993959926073,
      "grad_norm": 0.6487471461296082,
      "learning_rate": 4.276931260135616e-05,
      "loss": 1.3259,
      "step": 139250
    },
    {
      "epoch": 0.43414979258672864,
      "grad_norm": 0.6876869201660156,
      "learning_rate": 4.276671539034523e-05,
      "loss": 1.3019,
      "step": 139300
    },
    {
      "epoch": 0.43430562524738436,
      "grad_norm": 0.6168668270111084,
      "learning_rate": 4.2764118179334305e-05,
      "loss": 1.3001,
      "step": 139350
    },
    {
      "epoch": 0.43446145790804,
      "grad_norm": 0.6331563591957092,
      "learning_rate": 4.276152096832338e-05,
      "loss": 1.2686,
      "step": 139400
    },
    {
      "epoch": 0.4346172905686957,
      "grad_norm": 0.7066587805747986,
      "learning_rate": 4.275892375731245e-05,
      "loss": 1.2442,
      "step": 139450
    },
    {
      "epoch": 0.4347731232293514,
      "grad_norm": 0.6527059674263,
      "learning_rate": 4.275632654630152e-05,
      "loss": 1.2169,
      "step": 139500
    },
    {
      "epoch": 0.4349289558900071,
      "grad_norm": 0.6999309659004211,
      "learning_rate": 4.2753729335290595e-05,
      "loss": 1.2881,
      "step": 139550
    },
    {
      "epoch": 0.43508478855066274,
      "grad_norm": 0.6967529654502869,
      "learning_rate": 4.275113212427966e-05,
      "loss": 1.2579,
      "step": 139600
    },
    {
      "epoch": 0.43524062121131846,
      "grad_norm": 0.6487486958503723,
      "learning_rate": 4.274853491326874e-05,
      "loss": 1.2364,
      "step": 139650
    },
    {
      "epoch": 0.4353964538719741,
      "grad_norm": 0.4689052402973175,
      "learning_rate": 4.274593770225781e-05,
      "loss": 1.2873,
      "step": 139700
    },
    {
      "epoch": 0.4355522865326298,
      "grad_norm": 0.5245174765586853,
      "learning_rate": 4.274334049124688e-05,
      "loss": 1.291,
      "step": 139750
    },
    {
      "epoch": 0.4357081191932855,
      "grad_norm": 0.7401936650276184,
      "learning_rate": 4.274074328023595e-05,
      "loss": 1.2766,
      "step": 139800
    },
    {
      "epoch": 0.43586395185394117,
      "grad_norm": 0.6649388670921326,
      "learning_rate": 4.2738146069225024e-05,
      "loss": 1.2633,
      "step": 139850
    },
    {
      "epoch": 0.43601978451459683,
      "grad_norm": 0.5975944399833679,
      "learning_rate": 4.2735548858214096e-05,
      "loss": 1.3188,
      "step": 139900
    },
    {
      "epoch": 0.43617561717525255,
      "grad_norm": 0.5637433528900146,
      "learning_rate": 4.273295164720317e-05,
      "loss": 1.2985,
      "step": 139950
    },
    {
      "epoch": 0.4363314498359082,
      "grad_norm": 0.5419530272483826,
      "learning_rate": 4.273035443619224e-05,
      "loss": 1.2775,
      "step": 140000
    },
    {
      "epoch": 0.4364872824965639,
      "grad_norm": 0.8287741541862488,
      "learning_rate": 4.2727757225181314e-05,
      "loss": 1.2717,
      "step": 140050
    },
    {
      "epoch": 0.4366431151572196,
      "grad_norm": 0.7064484357833862,
      "learning_rate": 4.2725160014170386e-05,
      "loss": 1.2938,
      "step": 140100
    },
    {
      "epoch": 0.43679894781787526,
      "grad_norm": 0.4212914705276489,
      "learning_rate": 4.272256280315945e-05,
      "loss": 1.2506,
      "step": 140150
    },
    {
      "epoch": 0.4369547804785309,
      "grad_norm": 0.5309863686561584,
      "learning_rate": 4.271996559214853e-05,
      "loss": 1.2321,
      "step": 140200
    },
    {
      "epoch": 0.43711061313918664,
      "grad_norm": 0.6226975917816162,
      "learning_rate": 4.2717368381137604e-05,
      "loss": 1.3017,
      "step": 140250
    },
    {
      "epoch": 0.4372664457998423,
      "grad_norm": 0.47725147008895874,
      "learning_rate": 4.271477117012667e-05,
      "loss": 1.2755,
      "step": 140300
    },
    {
      "epoch": 0.43742227846049797,
      "grad_norm": 0.6255890727043152,
      "learning_rate": 4.271217395911575e-05,
      "loss": 1.31,
      "step": 140350
    },
    {
      "epoch": 0.43757811112115363,
      "grad_norm": 0.6470043659210205,
      "learning_rate": 4.270962869232504e-05,
      "loss": 1.3126,
      "step": 140400
    },
    {
      "epoch": 0.43773394378180935,
      "grad_norm": 0.5317716002464294,
      "learning_rate": 4.270703148131411e-05,
      "loss": 1.2831,
      "step": 140450
    },
    {
      "epoch": 0.437889776442465,
      "grad_norm": 0.45789799094200134,
      "learning_rate": 4.270443427030318e-05,
      "loss": 1.3377,
      "step": 140500
    },
    {
      "epoch": 0.4380456091031207,
      "grad_norm": 0.5694745779037476,
      "learning_rate": 4.270183705929225e-05,
      "loss": 1.3192,
      "step": 140550
    },
    {
      "epoch": 0.4382014417637764,
      "grad_norm": 0.5271258354187012,
      "learning_rate": 4.2699239848281324e-05,
      "loss": 1.3164,
      "step": 140600
    },
    {
      "epoch": 0.43835727442443206,
      "grad_norm": 0.6160433292388916,
      "learning_rate": 4.26966426372704e-05,
      "loss": 1.2962,
      "step": 140650
    },
    {
      "epoch": 0.4385131070850877,
      "grad_norm": 0.5906858444213867,
      "learning_rate": 4.269404542625947e-05,
      "loss": 1.2833,
      "step": 140700
    },
    {
      "epoch": 0.43866893974574345,
      "grad_norm": 0.5767704844474792,
      "learning_rate": 4.269144821524854e-05,
      "loss": 1.2778,
      "step": 140750
    },
    {
      "epoch": 0.4388247724063991,
      "grad_norm": 0.6004765033721924,
      "learning_rate": 4.2688851004237614e-05,
      "loss": 1.207,
      "step": 140800
    },
    {
      "epoch": 0.4389806050670548,
      "grad_norm": 0.6664509773254395,
      "learning_rate": 4.268625379322668e-05,
      "loss": 1.2614,
      "step": 140850
    },
    {
      "epoch": 0.4391364377277105,
      "grad_norm": 0.6107288599014282,
      "learning_rate": 4.268365658221575e-05,
      "loss": 1.251,
      "step": 140900
    },
    {
      "epoch": 0.43929227038836616,
      "grad_norm": 0.6005709171295166,
      "learning_rate": 4.268105937120483e-05,
      "loss": 1.3254,
      "step": 140950
    },
    {
      "epoch": 0.4394481030490218,
      "grad_norm": 0.5425281524658203,
      "learning_rate": 4.26784621601939e-05,
      "loss": 1.3145,
      "step": 141000
    },
    {
      "epoch": 0.43960393570967754,
      "grad_norm": 0.6527447700500488,
      "learning_rate": 4.267586494918297e-05,
      "loss": 1.3067,
      "step": 141050
    },
    {
      "epoch": 0.4397597683703332,
      "grad_norm": 0.5858181715011597,
      "learning_rate": 4.267326773817205e-05,
      "loss": 1.2432,
      "step": 141100
    },
    {
      "epoch": 0.43991560103098887,
      "grad_norm": 0.64423006772995,
      "learning_rate": 4.2670670527161115e-05,
      "loss": 1.2348,
      "step": 141150
    },
    {
      "epoch": 0.4400714336916446,
      "grad_norm": 0.7217581868171692,
      "learning_rate": 4.266807331615019e-05,
      "loss": 1.2797,
      "step": 141200
    },
    {
      "epoch": 0.44022726635230025,
      "grad_norm": 0.7657564282417297,
      "learning_rate": 4.266547610513926e-05,
      "loss": 1.3071,
      "step": 141250
    },
    {
      "epoch": 0.4403830990129559,
      "grad_norm": 0.6668713688850403,
      "learning_rate": 4.266287889412833e-05,
      "loss": 1.2316,
      "step": 141300
    },
    {
      "epoch": 0.44053893167361163,
      "grad_norm": 0.7555744051933289,
      "learning_rate": 4.2660281683117405e-05,
      "loss": 1.3339,
      "step": 141350
    },
    {
      "epoch": 0.4406947643342673,
      "grad_norm": 0.5001208186149597,
      "learning_rate": 4.265768447210647e-05,
      "loss": 1.3095,
      "step": 141400
    },
    {
      "epoch": 0.44085059699492296,
      "grad_norm": 0.6307204365730286,
      "learning_rate": 4.265508726109555e-05,
      "loss": 1.272,
      "step": 141450
    },
    {
      "epoch": 0.4410064296555787,
      "grad_norm": 0.5383930206298828,
      "learning_rate": 4.265249005008462e-05,
      "loss": 1.3014,
      "step": 141500
    },
    {
      "epoch": 0.44116226231623434,
      "grad_norm": 0.4789126217365265,
      "learning_rate": 4.264989283907369e-05,
      "loss": 1.2581,
      "step": 141550
    },
    {
      "epoch": 0.44131809497689,
      "grad_norm": 0.39738354086875916,
      "learning_rate": 4.264729562806276e-05,
      "loss": 1.2736,
      "step": 141600
    },
    {
      "epoch": 0.44147392763754567,
      "grad_norm": 0.4792408049106598,
      "learning_rate": 4.264469841705184e-05,
      "loss": 1.3079,
      "step": 141650
    },
    {
      "epoch": 0.4416297602982014,
      "grad_norm": 0.560107946395874,
      "learning_rate": 4.2642101206040906e-05,
      "loss": 1.2573,
      "step": 141700
    },
    {
      "epoch": 0.44178559295885705,
      "grad_norm": 0.5424948334693909,
      "learning_rate": 4.263950399502998e-05,
      "loss": 1.3156,
      "step": 141750
    },
    {
      "epoch": 0.4419414256195127,
      "grad_norm": 0.5238531231880188,
      "learning_rate": 4.263690678401905e-05,
      "loss": 1.3164,
      "step": 141800
    },
    {
      "epoch": 0.44209725828016844,
      "grad_norm": 0.6832441687583923,
      "learning_rate": 4.263430957300812e-05,
      "loss": 1.2483,
      "step": 141850
    },
    {
      "epoch": 0.4422530909408241,
      "grad_norm": 0.5379935503005981,
      "learning_rate": 4.2631712361997196e-05,
      "loss": 1.346,
      "step": 141900
    },
    {
      "epoch": 0.44240892360147976,
      "grad_norm": 0.6256793737411499,
      "learning_rate": 4.262911515098627e-05,
      "loss": 1.2834,
      "step": 141950
    },
    {
      "epoch": 0.4425647562621355,
      "grad_norm": 0.6351701021194458,
      "learning_rate": 4.262651793997534e-05,
      "loss": 1.2926,
      "step": 142000
    },
    {
      "epoch": 0.44272058892279115,
      "grad_norm": 0.4034344255924225,
      "learning_rate": 4.262392072896441e-05,
      "loss": 1.318,
      "step": 142050
    },
    {
      "epoch": 0.4428764215834468,
      "grad_norm": 0.6519914269447327,
      "learning_rate": 4.262132351795348e-05,
      "loss": 1.2511,
      "step": 142100
    },
    {
      "epoch": 0.44303225424410253,
      "grad_norm": 0.5809458494186401,
      "learning_rate": 4.261872630694255e-05,
      "loss": 1.316,
      "step": 142150
    },
    {
      "epoch": 0.4431880869047582,
      "grad_norm": 0.6538113355636597,
      "learning_rate": 4.261612909593163e-05,
      "loss": 1.2757,
      "step": 142200
    },
    {
      "epoch": 0.44334391956541386,
      "grad_norm": 0.5527684092521667,
      "learning_rate": 4.2613531884920696e-05,
      "loss": 1.2544,
      "step": 142250
    },
    {
      "epoch": 0.4434997522260696,
      "grad_norm": 0.5573214292526245,
      "learning_rate": 4.261093467390977e-05,
      "loss": 1.2868,
      "step": 142300
    },
    {
      "epoch": 0.44365558488672524,
      "grad_norm": 0.5287637114524841,
      "learning_rate": 4.260833746289885e-05,
      "loss": 1.2857,
      "step": 142350
    },
    {
      "epoch": 0.4438114175473809,
      "grad_norm": 0.7362526059150696,
      "learning_rate": 4.2605740251887914e-05,
      "loss": 1.2732,
      "step": 142400
    },
    {
      "epoch": 0.4439672502080366,
      "grad_norm": 0.5970208048820496,
      "learning_rate": 4.2603143040876986e-05,
      "loss": 1.2752,
      "step": 142450
    },
    {
      "epoch": 0.4441230828686923,
      "grad_norm": 0.5365230441093445,
      "learning_rate": 4.260054582986606e-05,
      "loss": 1.2717,
      "step": 142500
    },
    {
      "epoch": 0.44427891552934795,
      "grad_norm": 0.435651034116745,
      "learning_rate": 4.259794861885513e-05,
      "loss": 1.2894,
      "step": 142550
    },
    {
      "epoch": 0.44443474819000367,
      "grad_norm": 0.48989787697792053,
      "learning_rate": 4.2595351407844204e-05,
      "loss": 1.2413,
      "step": 142600
    },
    {
      "epoch": 0.44459058085065933,
      "grad_norm": 0.5683519244194031,
      "learning_rate": 4.2592754196833276e-05,
      "loss": 1.2563,
      "step": 142650
    },
    {
      "epoch": 0.444746413511315,
      "grad_norm": 0.7444705367088318,
      "learning_rate": 4.259015698582235e-05,
      "loss": 1.2755,
      "step": 142700
    },
    {
      "epoch": 0.4449022461719707,
      "grad_norm": 0.592247724533081,
      "learning_rate": 4.258755977481142e-05,
      "loss": 1.2684,
      "step": 142750
    },
    {
      "epoch": 0.4450580788326264,
      "grad_norm": 0.5278087258338928,
      "learning_rate": 4.258496256380049e-05,
      "loss": 1.2752,
      "step": 142800
    },
    {
      "epoch": 0.44521391149328204,
      "grad_norm": 0.5823138952255249,
      "learning_rate": 4.258241729700978e-05,
      "loss": 1.2648,
      "step": 142850
    },
    {
      "epoch": 0.4453697441539377,
      "grad_norm": 0.683838427066803,
      "learning_rate": 4.257982008599885e-05,
      "loss": 1.2565,
      "step": 142900
    },
    {
      "epoch": 0.4455255768145934,
      "grad_norm": 0.7059759497642517,
      "learning_rate": 4.2577222874987924e-05,
      "loss": 1.2439,
      "step": 142950
    },
    {
      "epoch": 0.4456814094752491,
      "grad_norm": 0.5403481721878052,
      "learning_rate": 4.2574625663977e-05,
      "loss": 1.3012,
      "step": 143000
    },
    {
      "epoch": 0.44583724213590475,
      "grad_norm": 0.7009792923927307,
      "learning_rate": 4.257202845296607e-05,
      "loss": 1.2727,
      "step": 143050
    },
    {
      "epoch": 0.4459930747965605,
      "grad_norm": 0.664461612701416,
      "learning_rate": 4.256943124195514e-05,
      "loss": 1.3256,
      "step": 143100
    },
    {
      "epoch": 0.44614890745721614,
      "grad_norm": 0.7104405760765076,
      "learning_rate": 4.2566834030944214e-05,
      "loss": 1.3146,
      "step": 143150
    },
    {
      "epoch": 0.4463047401178718,
      "grad_norm": 0.4078638553619385,
      "learning_rate": 4.256423681993329e-05,
      "loss": 1.3043,
      "step": 143200
    },
    {
      "epoch": 0.4464605727785275,
      "grad_norm": 0.6897454261779785,
      "learning_rate": 4.256163960892235e-05,
      "loss": 1.2465,
      "step": 143250
    },
    {
      "epoch": 0.4466164054391832,
      "grad_norm": 0.6349118947982788,
      "learning_rate": 4.255904239791143e-05,
      "loss": 1.2994,
      "step": 143300
    },
    {
      "epoch": 0.44677223809983885,
      "grad_norm": 0.6689389944076538,
      "learning_rate": 4.2556445186900504e-05,
      "loss": 1.2265,
      "step": 143350
    },
    {
      "epoch": 0.44692807076049457,
      "grad_norm": 0.39667800068855286,
      "learning_rate": 4.255384797588957e-05,
      "loss": 1.262,
      "step": 143400
    },
    {
      "epoch": 0.44708390342115023,
      "grad_norm": 0.6513898968696594,
      "learning_rate": 4.255125076487865e-05,
      "loss": 1.2957,
      "step": 143450
    },
    {
      "epoch": 0.4472397360818059,
      "grad_norm": 0.5676484704017639,
      "learning_rate": 4.2548653553867715e-05,
      "loss": 1.3582,
      "step": 143500
    },
    {
      "epoch": 0.4473955687424616,
      "grad_norm": 0.5723763108253479,
      "learning_rate": 4.254605634285679e-05,
      "loss": 1.2632,
      "step": 143550
    },
    {
      "epoch": 0.4475514014031173,
      "grad_norm": 0.5360863208770752,
      "learning_rate": 4.254345913184586e-05,
      "loss": 1.3177,
      "step": 143600
    },
    {
      "epoch": 0.44770723406377294,
      "grad_norm": 0.46747463941574097,
      "learning_rate": 4.254086192083493e-05,
      "loss": 1.257,
      "step": 143650
    },
    {
      "epoch": 0.44786306672442866,
      "grad_norm": 0.668825626373291,
      "learning_rate": 4.2538264709824005e-05,
      "loss": 1.1981,
      "step": 143700
    },
    {
      "epoch": 0.4480188993850843,
      "grad_norm": 0.6244479417800903,
      "learning_rate": 4.253566749881308e-05,
      "loss": 1.2355,
      "step": 143750
    },
    {
      "epoch": 0.44817473204574,
      "grad_norm": 0.8188702464103699,
      "learning_rate": 4.253307028780214e-05,
      "loss": 1.2501,
      "step": 143800
    },
    {
      "epoch": 0.4483305647063957,
      "grad_norm": 0.5326386094093323,
      "learning_rate": 4.253047307679122e-05,
      "loss": 1.2678,
      "step": 143850
    },
    {
      "epoch": 0.44848639736705137,
      "grad_norm": 0.6652974486351013,
      "learning_rate": 4.2527875865780295e-05,
      "loss": 1.3662,
      "step": 143900
    },
    {
      "epoch": 0.44864223002770703,
      "grad_norm": 0.7439395189285278,
      "learning_rate": 4.252527865476936e-05,
      "loss": 1.2803,
      "step": 143950
    },
    {
      "epoch": 0.44879806268836275,
      "grad_norm": 0.6286420226097107,
      "learning_rate": 4.252268144375844e-05,
      "loss": 1.2955,
      "step": 144000
    },
    {
      "epoch": 0.4489538953490184,
      "grad_norm": 0.515476644039154,
      "learning_rate": 4.2520084232747506e-05,
      "loss": 1.2688,
      "step": 144050
    },
    {
      "epoch": 0.4491097280096741,
      "grad_norm": 0.4583539366722107,
      "learning_rate": 4.251748702173658e-05,
      "loss": 1.2975,
      "step": 144100
    },
    {
      "epoch": 0.4492655606703298,
      "grad_norm": 0.6789419054985046,
      "learning_rate": 4.251488981072565e-05,
      "loss": 1.243,
      "step": 144150
    },
    {
      "epoch": 0.44942139333098546,
      "grad_norm": 0.7443599104881287,
      "learning_rate": 4.251229259971472e-05,
      "loss": 1.2658,
      "step": 144200
    },
    {
      "epoch": 0.4495772259916411,
      "grad_norm": 0.5342220664024353,
      "learning_rate": 4.2509695388703796e-05,
      "loss": 1.2803,
      "step": 144250
    },
    {
      "epoch": 0.4497330586522968,
      "grad_norm": 0.5600520968437195,
      "learning_rate": 4.250709817769287e-05,
      "loss": 1.2356,
      "step": 144300
    },
    {
      "epoch": 0.4498888913129525,
      "grad_norm": 0.635953426361084,
      "learning_rate": 4.250450096668194e-05,
      "loss": 1.2362,
      "step": 144350
    },
    {
      "epoch": 0.4500447239736082,
      "grad_norm": 0.4942173957824707,
      "learning_rate": 4.250190375567101e-05,
      "loss": 1.2061,
      "step": 144400
    },
    {
      "epoch": 0.45020055663426384,
      "grad_norm": 0.5421839356422424,
      "learning_rate": 4.2499306544660086e-05,
      "loss": 1.274,
      "step": 144450
    },
    {
      "epoch": 0.45035638929491956,
      "grad_norm": 0.71430903673172,
      "learning_rate": 4.249670933364915e-05,
      "loss": 1.2602,
      "step": 144500
    },
    {
      "epoch": 0.4505122219555752,
      "grad_norm": 0.5606258511543274,
      "learning_rate": 4.249411212263823e-05,
      "loss": 1.3327,
      "step": 144550
    },
    {
      "epoch": 0.4506680546162309,
      "grad_norm": 0.5609415173530579,
      "learning_rate": 4.24915149116273e-05,
      "loss": 1.2623,
      "step": 144600
    },
    {
      "epoch": 0.4508238872768866,
      "grad_norm": 0.45712578296661377,
      "learning_rate": 4.248891770061637e-05,
      "loss": 1.2488,
      "step": 144650
    },
    {
      "epoch": 0.45097971993754227,
      "grad_norm": 0.5114942789077759,
      "learning_rate": 4.248632048960545e-05,
      "loss": 1.2374,
      "step": 144700
    },
    {
      "epoch": 0.45113555259819793,
      "grad_norm": 0.6113365292549133,
      "learning_rate": 4.2483723278594514e-05,
      "loss": 1.2452,
      "step": 144750
    },
    {
      "epoch": 0.45129138525885365,
      "grad_norm": 0.5792688727378845,
      "learning_rate": 4.2481126067583587e-05,
      "loss": 1.2756,
      "step": 144800
    },
    {
      "epoch": 0.4514472179195093,
      "grad_norm": 0.5293658375740051,
      "learning_rate": 4.247852885657266e-05,
      "loss": 1.2431,
      "step": 144850
    },
    {
      "epoch": 0.451603050580165,
      "grad_norm": 0.5743003487586975,
      "learning_rate": 4.247593164556173e-05,
      "loss": 1.3281,
      "step": 144900
    },
    {
      "epoch": 0.4517588832408207,
      "grad_norm": 0.5531899333000183,
      "learning_rate": 4.2473334434550804e-05,
      "loss": 1.23,
      "step": 144950
    },
    {
      "epoch": 0.45191471590147636,
      "grad_norm": 0.7992388606071472,
      "learning_rate": 4.2470737223539877e-05,
      "loss": 1.2807,
      "step": 145000
    },
    {
      "epoch": 0.452070548562132,
      "grad_norm": 0.5676021575927734,
      "learning_rate": 4.246814001252894e-05,
      "loss": 1.2558,
      "step": 145050
    },
    {
      "epoch": 0.45222638122278774,
      "grad_norm": 0.7798810005187988,
      "learning_rate": 4.246554280151802e-05,
      "loss": 1.2549,
      "step": 145100
    },
    {
      "epoch": 0.4523822138834434,
      "grad_norm": 0.5607732534408569,
      "learning_rate": 4.2462945590507094e-05,
      "loss": 1.2089,
      "step": 145150
    },
    {
      "epoch": 0.45253804654409907,
      "grad_norm": 0.47958508133888245,
      "learning_rate": 4.246040032371638e-05,
      "loss": 1.2319,
      "step": 145200
    },
    {
      "epoch": 0.4526938792047548,
      "grad_norm": 0.6515282988548279,
      "learning_rate": 4.245780311270545e-05,
      "loss": 1.3145,
      "step": 145250
    },
    {
      "epoch": 0.45284971186541045,
      "grad_norm": 0.6379883885383606,
      "learning_rate": 4.245520590169453e-05,
      "loss": 1.3448,
      "step": 145300
    },
    {
      "epoch": 0.4530055445260661,
      "grad_norm": 0.5635586380958557,
      "learning_rate": 4.24526086906836e-05,
      "loss": 1.2917,
      "step": 145350
    },
    {
      "epoch": 0.45316137718672184,
      "grad_norm": 0.5812817811965942,
      "learning_rate": 4.245001147967267e-05,
      "loss": 1.3538,
      "step": 145400
    },
    {
      "epoch": 0.4533172098473775,
      "grad_norm": 0.6934101581573486,
      "learning_rate": 4.244741426866174e-05,
      "loss": 1.3365,
      "step": 145450
    },
    {
      "epoch": 0.45347304250803316,
      "grad_norm": 0.6091089248657227,
      "learning_rate": 4.2444817057650814e-05,
      "loss": 1.3297,
      "step": 145500
    },
    {
      "epoch": 0.4536288751686888,
      "grad_norm": 0.60914146900177,
      "learning_rate": 4.244221984663989e-05,
      "loss": 1.23,
      "step": 145550
    },
    {
      "epoch": 0.45378470782934455,
      "grad_norm": 0.6331242918968201,
      "learning_rate": 4.243962263562896e-05,
      "loss": 1.256,
      "step": 145600
    },
    {
      "epoch": 0.4539405404900002,
      "grad_norm": 0.5733739137649536,
      "learning_rate": 4.243702542461803e-05,
      "loss": 1.2868,
      "step": 145650
    },
    {
      "epoch": 0.4540963731506559,
      "grad_norm": 0.6102425456047058,
      "learning_rate": 4.2434428213607105e-05,
      "loss": 1.3088,
      "step": 145700
    },
    {
      "epoch": 0.4542522058113116,
      "grad_norm": 0.6338919997215271,
      "learning_rate": 4.243183100259617e-05,
      "loss": 1.2831,
      "step": 145750
    },
    {
      "epoch": 0.45440803847196726,
      "grad_norm": 0.5684939622879028,
      "learning_rate": 4.242923379158524e-05,
      "loss": 1.28,
      "step": 145800
    },
    {
      "epoch": 0.4545638711326229,
      "grad_norm": 0.6318175792694092,
      "learning_rate": 4.242663658057432e-05,
      "loss": 1.2954,
      "step": 145850
    },
    {
      "epoch": 0.45471970379327864,
      "grad_norm": 0.6185164451599121,
      "learning_rate": 4.242403936956339e-05,
      "loss": 1.2991,
      "step": 145900
    },
    {
      "epoch": 0.4548755364539343,
      "grad_norm": 0.492595374584198,
      "learning_rate": 4.242144215855246e-05,
      "loss": 1.2953,
      "step": 145950
    },
    {
      "epoch": 0.45503136911458997,
      "grad_norm": 0.6463547348976135,
      "learning_rate": 4.241884494754154e-05,
      "loss": 1.2803,
      "step": 146000
    },
    {
      "epoch": 0.4551872017752457,
      "grad_norm": 0.5615179538726807,
      "learning_rate": 4.2416247736530605e-05,
      "loss": 1.2715,
      "step": 146050
    },
    {
      "epoch": 0.45534303443590135,
      "grad_norm": 0.6541779041290283,
      "learning_rate": 4.241365052551968e-05,
      "loss": 1.2603,
      "step": 146100
    },
    {
      "epoch": 0.455498867096557,
      "grad_norm": 0.624725878238678,
      "learning_rate": 4.241105331450875e-05,
      "loss": 1.289,
      "step": 146150
    },
    {
      "epoch": 0.45565469975721273,
      "grad_norm": 0.5615142583847046,
      "learning_rate": 4.240845610349782e-05,
      "loss": 1.217,
      "step": 146200
    },
    {
      "epoch": 0.4558105324178684,
      "grad_norm": 0.5682568550109863,
      "learning_rate": 4.2405858892486895e-05,
      "loss": 1.2866,
      "step": 146250
    },
    {
      "epoch": 0.45596636507852406,
      "grad_norm": 0.5076221227645874,
      "learning_rate": 4.240326168147596e-05,
      "loss": 1.2434,
      "step": 146300
    },
    {
      "epoch": 0.4561221977391798,
      "grad_norm": 0.6496109962463379,
      "learning_rate": 4.240066447046504e-05,
      "loss": 1.2744,
      "step": 146350
    },
    {
      "epoch": 0.45627803039983544,
      "grad_norm": 0.5738704800605774,
      "learning_rate": 4.239806725945411e-05,
      "loss": 1.2223,
      "step": 146400
    },
    {
      "epoch": 0.4564338630604911,
      "grad_norm": 0.6406452059745789,
      "learning_rate": 4.239547004844318e-05,
      "loss": 1.2573,
      "step": 146450
    },
    {
      "epoch": 0.4565896957211468,
      "grad_norm": 0.5034839510917664,
      "learning_rate": 4.239287283743225e-05,
      "loss": 1.2813,
      "step": 146500
    },
    {
      "epoch": 0.4567455283818025,
      "grad_norm": 0.6787810325622559,
      "learning_rate": 4.239027562642133e-05,
      "loss": 1.2971,
      "step": 146550
    },
    {
      "epoch": 0.45690136104245815,
      "grad_norm": 0.5639052391052246,
      "learning_rate": 4.2387678415410396e-05,
      "loss": 1.2188,
      "step": 146600
    },
    {
      "epoch": 0.4570571937031139,
      "grad_norm": 0.7078456282615662,
      "learning_rate": 4.238508120439947e-05,
      "loss": 1.357,
      "step": 146650
    },
    {
      "epoch": 0.45721302636376954,
      "grad_norm": 0.7119666337966919,
      "learning_rate": 4.238248399338854e-05,
      "loss": 1.2705,
      "step": 146700
    },
    {
      "epoch": 0.4573688590244252,
      "grad_norm": 0.46446841955184937,
      "learning_rate": 4.2379886782377614e-05,
      "loss": 1.2832,
      "step": 146750
    },
    {
      "epoch": 0.4575246916850809,
      "grad_norm": 0.4498194754123688,
      "learning_rate": 4.2377289571366686e-05,
      "loss": 1.1633,
      "step": 146800
    },
    {
      "epoch": 0.4576805243457366,
      "grad_norm": 0.7557423710823059,
      "learning_rate": 4.237469236035576e-05,
      "loss": 1.281,
      "step": 146850
    },
    {
      "epoch": 0.45783635700639225,
      "grad_norm": 0.6021068096160889,
      "learning_rate": 4.237209514934483e-05,
      "loss": 1.3189,
      "step": 146900
    },
    {
      "epoch": 0.4579921896670479,
      "grad_norm": 0.595687210559845,
      "learning_rate": 4.2369497938333904e-05,
      "loss": 1.2359,
      "step": 146950
    },
    {
      "epoch": 0.45814802232770363,
      "grad_norm": 0.5788222551345825,
      "learning_rate": 4.236690072732297e-05,
      "loss": 1.1991,
      "step": 147000
    },
    {
      "epoch": 0.4583038549883593,
      "grad_norm": 0.7938109040260315,
      "learning_rate": 4.236430351631204e-05,
      "loss": 1.2994,
      "step": 147050
    },
    {
      "epoch": 0.45845968764901496,
      "grad_norm": 0.6327061653137207,
      "learning_rate": 4.236170630530112e-05,
      "loss": 1.2418,
      "step": 147100
    },
    {
      "epoch": 0.4586155203096707,
      "grad_norm": 0.550606906414032,
      "learning_rate": 4.235910909429019e-05,
      "loss": 1.298,
      "step": 147150
    },
    {
      "epoch": 0.45877135297032634,
      "grad_norm": 0.49318504333496094,
      "learning_rate": 4.235651188327926e-05,
      "loss": 1.2733,
      "step": 147200
    },
    {
      "epoch": 0.458927185630982,
      "grad_norm": 0.6350602507591248,
      "learning_rate": 4.235391467226834e-05,
      "loss": 1.3205,
      "step": 147250
    },
    {
      "epoch": 0.4590830182916377,
      "grad_norm": 0.6153093576431274,
      "learning_rate": 4.2351317461257404e-05,
      "loss": 1.2118,
      "step": 147300
    },
    {
      "epoch": 0.4592388509522934,
      "grad_norm": 0.5452014803886414,
      "learning_rate": 4.234872025024648e-05,
      "loss": 1.3138,
      "step": 147350
    },
    {
      "epoch": 0.45939468361294905,
      "grad_norm": 0.7495633363723755,
      "learning_rate": 4.234612303923555e-05,
      "loss": 1.3124,
      "step": 147400
    },
    {
      "epoch": 0.45955051627360477,
      "grad_norm": 0.5906842350959778,
      "learning_rate": 4.234352582822462e-05,
      "loss": 1.2579,
      "step": 147450
    },
    {
      "epoch": 0.45970634893426043,
      "grad_norm": 0.6016054749488831,
      "learning_rate": 4.2340980561433914e-05,
      "loss": 1.2903,
      "step": 147500
    },
    {
      "epoch": 0.4598621815949161,
      "grad_norm": 0.710358202457428,
      "learning_rate": 4.2338383350422986e-05,
      "loss": 1.3488,
      "step": 147550
    },
    {
      "epoch": 0.4600180142555718,
      "grad_norm": 0.744913637638092,
      "learning_rate": 4.233578613941205e-05,
      "loss": 1.2609,
      "step": 147600
    },
    {
      "epoch": 0.4601738469162275,
      "grad_norm": 0.6149197816848755,
      "learning_rate": 4.233318892840113e-05,
      "loss": 1.2819,
      "step": 147650
    },
    {
      "epoch": 0.46032967957688314,
      "grad_norm": 0.6116307377815247,
      "learning_rate": 4.23305917173902e-05,
      "loss": 1.2716,
      "step": 147700
    },
    {
      "epoch": 0.46048551223753886,
      "grad_norm": 0.5151024460792542,
      "learning_rate": 4.232799450637927e-05,
      "loss": 1.2868,
      "step": 147750
    },
    {
      "epoch": 0.4606413448981945,
      "grad_norm": 0.48947060108184814,
      "learning_rate": 4.232539729536835e-05,
      "loss": 1.2708,
      "step": 147800
    },
    {
      "epoch": 0.4607971775588502,
      "grad_norm": 0.5977181196212769,
      "learning_rate": 4.2322800084357415e-05,
      "loss": 1.2785,
      "step": 147850
    },
    {
      "epoch": 0.4609530102195059,
      "grad_norm": 0.6106827855110168,
      "learning_rate": 4.232020287334649e-05,
      "loss": 1.3116,
      "step": 147900
    },
    {
      "epoch": 0.4611088428801616,
      "grad_norm": 0.5918396711349487,
      "learning_rate": 4.231760566233556e-05,
      "loss": 1.2759,
      "step": 147950
    },
    {
      "epoch": 0.46126467554081724,
      "grad_norm": 0.46817129850387573,
      "learning_rate": 4.231500845132463e-05,
      "loss": 1.2226,
      "step": 148000
    },
    {
      "epoch": 0.46142050820147296,
      "grad_norm": 0.5153906941413879,
      "learning_rate": 4.2312411240313705e-05,
      "loss": 1.2649,
      "step": 148050
    },
    {
      "epoch": 0.4615763408621286,
      "grad_norm": 0.6441766619682312,
      "learning_rate": 4.230981402930278e-05,
      "loss": 1.2843,
      "step": 148100
    },
    {
      "epoch": 0.4617321735227843,
      "grad_norm": 0.5874351263046265,
      "learning_rate": 4.230721681829184e-05,
      "loss": 1.2703,
      "step": 148150
    },
    {
      "epoch": 0.46188800618343995,
      "grad_norm": 0.6111196875572205,
      "learning_rate": 4.230461960728092e-05,
      "loss": 1.2858,
      "step": 148200
    },
    {
      "epoch": 0.46204383884409567,
      "grad_norm": 0.6138521432876587,
      "learning_rate": 4.2302022396269995e-05,
      "loss": 1.2308,
      "step": 148250
    },
    {
      "epoch": 0.46219967150475133,
      "grad_norm": 0.8341515064239502,
      "learning_rate": 4.229942518525906e-05,
      "loss": 1.2681,
      "step": 148300
    },
    {
      "epoch": 0.462355504165407,
      "grad_norm": 0.613582193851471,
      "learning_rate": 4.229682797424814e-05,
      "loss": 1.2737,
      "step": 148350
    },
    {
      "epoch": 0.4625113368260627,
      "grad_norm": 0.5936121344566345,
      "learning_rate": 4.2294230763237205e-05,
      "loss": 1.2458,
      "step": 148400
    },
    {
      "epoch": 0.4626671694867184,
      "grad_norm": 0.5802986025810242,
      "learning_rate": 4.229163355222628e-05,
      "loss": 1.2026,
      "step": 148450
    },
    {
      "epoch": 0.46282300214737404,
      "grad_norm": 0.5804781317710876,
      "learning_rate": 4.228903634121535e-05,
      "loss": 1.2424,
      "step": 148500
    },
    {
      "epoch": 0.46297883480802976,
      "grad_norm": 0.5953569412231445,
      "learning_rate": 4.228643913020442e-05,
      "loss": 1.3167,
      "step": 148550
    },
    {
      "epoch": 0.4631346674686854,
      "grad_norm": 0.49765458703041077,
      "learning_rate": 4.2283841919193495e-05,
      "loss": 1.2837,
      "step": 148600
    },
    {
      "epoch": 0.4632905001293411,
      "grad_norm": 0.571160614490509,
      "learning_rate": 4.228124470818257e-05,
      "loss": 1.2862,
      "step": 148650
    },
    {
      "epoch": 0.4634463327899968,
      "grad_norm": 0.626380443572998,
      "learning_rate": 4.227864749717164e-05,
      "loss": 1.3045,
      "step": 148700
    },
    {
      "epoch": 0.46360216545065247,
      "grad_norm": 0.5138612985610962,
      "learning_rate": 4.227605028616071e-05,
      "loss": 1.3144,
      "step": 148750
    },
    {
      "epoch": 0.46375799811130813,
      "grad_norm": 0.5320244431495667,
      "learning_rate": 4.2273453075149785e-05,
      "loss": 1.2612,
      "step": 148800
    },
    {
      "epoch": 0.46391383077196385,
      "grad_norm": 0.6394197344779968,
      "learning_rate": 4.227085586413885e-05,
      "loss": 1.3041,
      "step": 148850
    },
    {
      "epoch": 0.4640696634326195,
      "grad_norm": 0.5182879567146301,
      "learning_rate": 4.226825865312793e-05,
      "loss": 1.2503,
      "step": 148900
    },
    {
      "epoch": 0.4642254960932752,
      "grad_norm": 0.5490014553070068,
      "learning_rate": 4.2265661442116996e-05,
      "loss": 1.279,
      "step": 148950
    },
    {
      "epoch": 0.4643813287539309,
      "grad_norm": 0.6917511224746704,
      "learning_rate": 4.226306423110607e-05,
      "loss": 1.3068,
      "step": 149000
    },
    {
      "epoch": 0.46453716141458656,
      "grad_norm": 0.7113648056983948,
      "learning_rate": 4.226046702009515e-05,
      "loss": 1.2578,
      "step": 149050
    },
    {
      "epoch": 0.4646929940752422,
      "grad_norm": 0.6771793365478516,
      "learning_rate": 4.2257869809084214e-05,
      "loss": 1.2522,
      "step": 149100
    },
    {
      "epoch": 0.46484882673589795,
      "grad_norm": 0.5860544443130493,
      "learning_rate": 4.2255272598073286e-05,
      "loss": 1.3185,
      "step": 149150
    },
    {
      "epoch": 0.4650046593965536,
      "grad_norm": 0.6103643774986267,
      "learning_rate": 4.225267538706236e-05,
      "loss": 1.2976,
      "step": 149200
    },
    {
      "epoch": 0.4651604920572093,
      "grad_norm": 0.503316342830658,
      "learning_rate": 4.225007817605143e-05,
      "loss": 1.2347,
      "step": 149250
    },
    {
      "epoch": 0.465316324717865,
      "grad_norm": 0.4858147203922272,
      "learning_rate": 4.2247480965040504e-05,
      "loss": 1.2944,
      "step": 149300
    },
    {
      "epoch": 0.46547215737852066,
      "grad_norm": 0.4692315459251404,
      "learning_rate": 4.2244935698249796e-05,
      "loss": 1.2977,
      "step": 149350
    },
    {
      "epoch": 0.4656279900391763,
      "grad_norm": 0.5750187039375305,
      "learning_rate": 4.224233848723886e-05,
      "loss": 1.3218,
      "step": 149400
    },
    {
      "epoch": 0.46578382269983204,
      "grad_norm": 0.642426609992981,
      "learning_rate": 4.223974127622794e-05,
      "loss": 1.3176,
      "step": 149450
    },
    {
      "epoch": 0.4659396553604877,
      "grad_norm": 0.43384119868278503,
      "learning_rate": 4.223714406521701e-05,
      "loss": 1.255,
      "step": 149500
    },
    {
      "epoch": 0.46609548802114337,
      "grad_norm": 0.6673340797424316,
      "learning_rate": 4.223454685420608e-05,
      "loss": 1.3157,
      "step": 149550
    },
    {
      "epoch": 0.46625132068179903,
      "grad_norm": 0.5027472376823425,
      "learning_rate": 4.223194964319515e-05,
      "loss": 1.2506,
      "step": 149600
    },
    {
      "epoch": 0.46640715334245475,
      "grad_norm": 0.6036056876182556,
      "learning_rate": 4.2229352432184224e-05,
      "loss": 1.2997,
      "step": 149650
    },
    {
      "epoch": 0.4665629860031104,
      "grad_norm": 0.5581344366073608,
      "learning_rate": 4.2226755221173297e-05,
      "loss": 1.2789,
      "step": 149700
    },
    {
      "epoch": 0.4667188186637661,
      "grad_norm": 0.5821503400802612,
      "learning_rate": 4.222415801016237e-05,
      "loss": 1.3044,
      "step": 149750
    },
    {
      "epoch": 0.4668746513244218,
      "grad_norm": 0.729479193687439,
      "learning_rate": 4.222156079915144e-05,
      "loss": 1.2917,
      "step": 149800
    },
    {
      "epoch": 0.46703048398507746,
      "grad_norm": 0.6428681015968323,
      "learning_rate": 4.2218963588140514e-05,
      "loss": 1.2924,
      "step": 149850
    },
    {
      "epoch": 0.4671863166457331,
      "grad_norm": 0.6360871195793152,
      "learning_rate": 4.2216366377129587e-05,
      "loss": 1.2292,
      "step": 149900
    },
    {
      "epoch": 0.46734214930638884,
      "grad_norm": 0.5238432884216309,
      "learning_rate": 4.221376916611865e-05,
      "loss": 1.2902,
      "step": 149950
    },
    {
      "epoch": 0.4674979819670445,
      "grad_norm": 0.6560788154602051,
      "learning_rate": 4.221117195510773e-05,
      "loss": 1.2996,
      "step": 150000
    },
    {
      "epoch": 0.46765381462770017,
      "grad_norm": 0.6781294345855713,
      "learning_rate": 4.2208574744096804e-05,
      "loss": 1.2841,
      "step": 150050
    },
    {
      "epoch": 0.4678096472883559,
      "grad_norm": 0.6169135570526123,
      "learning_rate": 4.220597753308587e-05,
      "loss": 1.2043,
      "step": 150100
    },
    {
      "epoch": 0.46796547994901155,
      "grad_norm": 0.6422361135482788,
      "learning_rate": 4.220338032207494e-05,
      "loss": 1.3072,
      "step": 150150
    },
    {
      "epoch": 0.4681213126096672,
      "grad_norm": 0.577815592288971,
      "learning_rate": 4.220078311106402e-05,
      "loss": 1.3159,
      "step": 150200
    },
    {
      "epoch": 0.46827714527032294,
      "grad_norm": 0.5478798151016235,
      "learning_rate": 4.219818590005309e-05,
      "loss": 1.2851,
      "step": 150250
    },
    {
      "epoch": 0.4684329779309786,
      "grad_norm": 0.5799128413200378,
      "learning_rate": 4.219558868904216e-05,
      "loss": 1.2867,
      "step": 150300
    },
    {
      "epoch": 0.46858881059163426,
      "grad_norm": 0.615375280380249,
      "learning_rate": 4.219299147803123e-05,
      "loss": 1.3214,
      "step": 150350
    },
    {
      "epoch": 0.46874464325229,
      "grad_norm": 0.43336746096611023,
      "learning_rate": 4.2190394267020305e-05,
      "loss": 1.3551,
      "step": 150400
    },
    {
      "epoch": 0.46890047591294565,
      "grad_norm": 0.596048891544342,
      "learning_rate": 4.218779705600938e-05,
      "loss": 1.2698,
      "step": 150450
    },
    {
      "epoch": 0.4690563085736013,
      "grad_norm": 0.6759743094444275,
      "learning_rate": 4.218519984499845e-05,
      "loss": 1.2583,
      "step": 150500
    },
    {
      "epoch": 0.46921214123425703,
      "grad_norm": 0.4603629410266876,
      "learning_rate": 4.218260263398752e-05,
      "loss": 1.2878,
      "step": 150550
    },
    {
      "epoch": 0.4693679738949127,
      "grad_norm": 0.6695666909217834,
      "learning_rate": 4.2180005422976595e-05,
      "loss": 1.258,
      "step": 150600
    },
    {
      "epoch": 0.46952380655556836,
      "grad_norm": 0.5964467525482178,
      "learning_rate": 4.217740821196566e-05,
      "loss": 1.2724,
      "step": 150650
    },
    {
      "epoch": 0.4696796392162241,
      "grad_norm": 0.6494300365447998,
      "learning_rate": 4.217481100095474e-05,
      "loss": 1.2416,
      "step": 150700
    },
    {
      "epoch": 0.46983547187687974,
      "grad_norm": 0.5346879363059998,
      "learning_rate": 4.217221378994381e-05,
      "loss": 1.2374,
      "step": 150750
    },
    {
      "epoch": 0.4699913045375354,
      "grad_norm": 0.5991619825363159,
      "learning_rate": 4.216961657893288e-05,
      "loss": 1.2904,
      "step": 150800
    },
    {
      "epoch": 0.47014713719819107,
      "grad_norm": 0.5778970122337341,
      "learning_rate": 4.216701936792195e-05,
      "loss": 1.2932,
      "step": 150850
    },
    {
      "epoch": 0.4703029698588468,
      "grad_norm": 0.6925605535507202,
      "learning_rate": 4.216442215691103e-05,
      "loss": 1.3198,
      "step": 150900
    },
    {
      "epoch": 0.47045880251950245,
      "grad_norm": 0.43115562200546265,
      "learning_rate": 4.2161824945900096e-05,
      "loss": 1.22,
      "step": 150950
    },
    {
      "epoch": 0.4706146351801581,
      "grad_norm": 0.5369544625282288,
      "learning_rate": 4.215922773488917e-05,
      "loss": 1.2892,
      "step": 151000
    },
    {
      "epoch": 0.47077046784081383,
      "grad_norm": 0.7606465220451355,
      "learning_rate": 4.215663052387824e-05,
      "loss": 1.3052,
      "step": 151050
    },
    {
      "epoch": 0.4709263005014695,
      "grad_norm": 0.45386558771133423,
      "learning_rate": 4.215403331286731e-05,
      "loss": 1.2417,
      "step": 151100
    },
    {
      "epoch": 0.47108213316212516,
      "grad_norm": 0.7384780645370483,
      "learning_rate": 4.2151436101856386e-05,
      "loss": 1.2708,
      "step": 151150
    },
    {
      "epoch": 0.4712379658227809,
      "grad_norm": 0.49295851588249207,
      "learning_rate": 4.214883889084545e-05,
      "loss": 1.3074,
      "step": 151200
    },
    {
      "epoch": 0.47139379848343654,
      "grad_norm": 0.8045796155929565,
      "learning_rate": 4.214624167983453e-05,
      "loss": 1.2956,
      "step": 151250
    },
    {
      "epoch": 0.4715496311440922,
      "grad_norm": 0.4542510509490967,
      "learning_rate": 4.21436444688236e-05,
      "loss": 1.2533,
      "step": 151300
    },
    {
      "epoch": 0.4717054638047479,
      "grad_norm": 0.6314621567726135,
      "learning_rate": 4.214104725781267e-05,
      "loss": 1.2665,
      "step": 151350
    },
    {
      "epoch": 0.4718612964654036,
      "grad_norm": 0.46329033374786377,
      "learning_rate": 4.213850199102196e-05,
      "loss": 1.2741,
      "step": 151400
    },
    {
      "epoch": 0.47201712912605925,
      "grad_norm": 0.5776058435440063,
      "learning_rate": 4.213590478001104e-05,
      "loss": 1.2068,
      "step": 151450
    },
    {
      "epoch": 0.472172961786715,
      "grad_norm": 0.6377293467521667,
      "learning_rate": 4.2133307569000106e-05,
      "loss": 1.2683,
      "step": 151500
    },
    {
      "epoch": 0.47232879444737064,
      "grad_norm": 0.555417537689209,
      "learning_rate": 4.213071035798918e-05,
      "loss": 1.2788,
      "step": 151550
    },
    {
      "epoch": 0.4724846271080263,
      "grad_norm": 0.5076077580451965,
      "learning_rate": 4.212811314697825e-05,
      "loss": 1.3032,
      "step": 151600
    },
    {
      "epoch": 0.472640459768682,
      "grad_norm": 0.4996242821216583,
      "learning_rate": 4.2125515935967323e-05,
      "loss": 1.29,
      "step": 151650
    },
    {
      "epoch": 0.4727962924293377,
      "grad_norm": 0.7564601898193359,
      "learning_rate": 4.2122918724956396e-05,
      "loss": 1.2968,
      "step": 151700
    },
    {
      "epoch": 0.47295212508999335,
      "grad_norm": 0.4844939410686493,
      "learning_rate": 4.212032151394547e-05,
      "loss": 1.2663,
      "step": 151750
    },
    {
      "epoch": 0.47310795775064907,
      "grad_norm": 0.4832257926464081,
      "learning_rate": 4.211772430293454e-05,
      "loss": 1.2918,
      "step": 151800
    },
    {
      "epoch": 0.47326379041130473,
      "grad_norm": 0.6264627575874329,
      "learning_rate": 4.2115127091923614e-05,
      "loss": 1.2643,
      "step": 151850
    },
    {
      "epoch": 0.4734196230719604,
      "grad_norm": 0.5579020977020264,
      "learning_rate": 4.211252988091268e-05,
      "loss": 1.2508,
      "step": 151900
    },
    {
      "epoch": 0.4735754557326161,
      "grad_norm": 0.647810697555542,
      "learning_rate": 4.210993266990175e-05,
      "loss": 1.2568,
      "step": 151950
    },
    {
      "epoch": 0.4737312883932718,
      "grad_norm": 0.661586582660675,
      "learning_rate": 4.210733545889083e-05,
      "loss": 1.293,
      "step": 152000
    },
    {
      "epoch": 0.47388712105392744,
      "grad_norm": 0.5839937329292297,
      "learning_rate": 4.21047382478799e-05,
      "loss": 1.2841,
      "step": 152050
    },
    {
      "epoch": 0.47404295371458316,
      "grad_norm": 0.502791702747345,
      "learning_rate": 4.210214103686897e-05,
      "loss": 1.2742,
      "step": 152100
    },
    {
      "epoch": 0.4741987863752388,
      "grad_norm": 0.5529463887214661,
      "learning_rate": 4.209954382585804e-05,
      "loss": 1.2184,
      "step": 152150
    },
    {
      "epoch": 0.4743546190358945,
      "grad_norm": 0.669430136680603,
      "learning_rate": 4.2096946614847114e-05,
      "loss": 1.3085,
      "step": 152200
    },
    {
      "epoch": 0.47451045169655015,
      "grad_norm": 0.6204220056533813,
      "learning_rate": 4.209434940383619e-05,
      "loss": 1.3247,
      "step": 152250
    },
    {
      "epoch": 0.47466628435720587,
      "grad_norm": 0.5695353150367737,
      "learning_rate": 4.209175219282526e-05,
      "loss": 1.347,
      "step": 152300
    },
    {
      "epoch": 0.47482211701786153,
      "grad_norm": 0.4700964093208313,
      "learning_rate": 4.208915498181433e-05,
      "loss": 1.2534,
      "step": 152350
    },
    {
      "epoch": 0.4749779496785172,
      "grad_norm": 0.6032530069351196,
      "learning_rate": 4.2086557770803404e-05,
      "loss": 1.249,
      "step": 152400
    },
    {
      "epoch": 0.4751337823391729,
      "grad_norm": 0.595019519329071,
      "learning_rate": 4.208396055979248e-05,
      "loss": 1.2957,
      "step": 152450
    },
    {
      "epoch": 0.4752896149998286,
      "grad_norm": 0.5445607900619507,
      "learning_rate": 4.208136334878154e-05,
      "loss": 1.2338,
      "step": 152500
    },
    {
      "epoch": 0.47544544766048424,
      "grad_norm": 0.599544107913971,
      "learning_rate": 4.207876613777062e-05,
      "loss": 1.3038,
      "step": 152550
    },
    {
      "epoch": 0.47560128032113996,
      "grad_norm": 0.6252534985542297,
      "learning_rate": 4.207616892675969e-05,
      "loss": 1.2376,
      "step": 152600
    },
    {
      "epoch": 0.4757571129817956,
      "grad_norm": 0.7050631046295166,
      "learning_rate": 4.207357171574876e-05,
      "loss": 1.2418,
      "step": 152650
    },
    {
      "epoch": 0.4759129456424513,
      "grad_norm": 0.5863161683082581,
      "learning_rate": 4.207097450473784e-05,
      "loss": 1.2827,
      "step": 152700
    },
    {
      "epoch": 0.476068778303107,
      "grad_norm": 0.4712691307067871,
      "learning_rate": 4.2068377293726905e-05,
      "loss": 1.1847,
      "step": 152750
    },
    {
      "epoch": 0.4762246109637627,
      "grad_norm": 0.7037965655326843,
      "learning_rate": 4.206578008271598e-05,
      "loss": 1.292,
      "step": 152800
    },
    {
      "epoch": 0.47638044362441834,
      "grad_norm": 0.48852500319480896,
      "learning_rate": 4.206318287170505e-05,
      "loss": 1.246,
      "step": 152850
    },
    {
      "epoch": 0.47653627628507406,
      "grad_norm": 0.5186313390731812,
      "learning_rate": 4.206058566069412e-05,
      "loss": 1.2237,
      "step": 152900
    },
    {
      "epoch": 0.4766921089457297,
      "grad_norm": 0.5118769407272339,
      "learning_rate": 4.2057988449683195e-05,
      "loss": 1.2999,
      "step": 152950
    },
    {
      "epoch": 0.4768479416063854,
      "grad_norm": 0.6286676526069641,
      "learning_rate": 4.205539123867227e-05,
      "loss": 1.2941,
      "step": 153000
    },
    {
      "epoch": 0.4770037742670411,
      "grad_norm": 0.7040165066719055,
      "learning_rate": 4.205279402766134e-05,
      "loss": 1.3055,
      "step": 153050
    },
    {
      "epoch": 0.47715960692769677,
      "grad_norm": 0.6214693188667297,
      "learning_rate": 4.205019681665041e-05,
      "loss": 1.3292,
      "step": 153100
    },
    {
      "epoch": 0.47731543958835243,
      "grad_norm": 0.6532495021820068,
      "learning_rate": 4.2047599605639485e-05,
      "loss": 1.2966,
      "step": 153150
    },
    {
      "epoch": 0.47747127224900815,
      "grad_norm": 0.7629511952400208,
      "learning_rate": 4.204500239462855e-05,
      "loss": 1.2935,
      "step": 153200
    },
    {
      "epoch": 0.4776271049096638,
      "grad_norm": 0.6519796848297119,
      "learning_rate": 4.204240518361763e-05,
      "loss": 1.2888,
      "step": 153250
    },
    {
      "epoch": 0.4777829375703195,
      "grad_norm": 0.5982452034950256,
      "learning_rate": 4.2039807972606696e-05,
      "loss": 1.2908,
      "step": 153300
    },
    {
      "epoch": 0.4779387702309752,
      "grad_norm": 0.6066778302192688,
      "learning_rate": 4.203721076159577e-05,
      "loss": 1.2507,
      "step": 153350
    },
    {
      "epoch": 0.47809460289163086,
      "grad_norm": 0.5508023500442505,
      "learning_rate": 4.203461355058484e-05,
      "loss": 1.2721,
      "step": 153400
    },
    {
      "epoch": 0.4782504355522865,
      "grad_norm": 0.6041975617408752,
      "learning_rate": 4.203201633957391e-05,
      "loss": 1.2625,
      "step": 153450
    },
    {
      "epoch": 0.4784062682129422,
      "grad_norm": 0.5963186025619507,
      "learning_rate": 4.2029419128562986e-05,
      "loss": 1.2798,
      "step": 153500
    },
    {
      "epoch": 0.4785621008735979,
      "grad_norm": 0.572507917881012,
      "learning_rate": 4.202682191755206e-05,
      "loss": 1.281,
      "step": 153550
    },
    {
      "epoch": 0.47871793353425357,
      "grad_norm": 0.7198031544685364,
      "learning_rate": 4.202422470654113e-05,
      "loss": 1.2656,
      "step": 153600
    },
    {
      "epoch": 0.47887376619490923,
      "grad_norm": 0.6590912342071533,
      "learning_rate": 4.20216274955302e-05,
      "loss": 1.2616,
      "step": 153650
    },
    {
      "epoch": 0.47902959885556495,
      "grad_norm": 0.5220698714256287,
      "learning_rate": 4.2019030284519276e-05,
      "loss": 1.2756,
      "step": 153700
    },
    {
      "epoch": 0.4791854315162206,
      "grad_norm": 0.650847315788269,
      "learning_rate": 4.201643307350834e-05,
      "loss": 1.2827,
      "step": 153750
    },
    {
      "epoch": 0.4793412641768763,
      "grad_norm": 0.6890001893043518,
      "learning_rate": 4.201383586249742e-05,
      "loss": 1.3087,
      "step": 153800
    },
    {
      "epoch": 0.479497096837532,
      "grad_norm": 0.6032544374465942,
      "learning_rate": 4.2011238651486487e-05,
      "loss": 1.2989,
      "step": 153850
    },
    {
      "epoch": 0.47965292949818766,
      "grad_norm": 0.5800248980522156,
      "learning_rate": 4.200864144047556e-05,
      "loss": 1.2466,
      "step": 153900
    },
    {
      "epoch": 0.4798087621588433,
      "grad_norm": 0.5825421810150146,
      "learning_rate": 4.200604422946464e-05,
      "loss": 1.2562,
      "step": 153950
    },
    {
      "epoch": 0.47996459481949905,
      "grad_norm": 0.727177619934082,
      "learning_rate": 4.2003447018453704e-05,
      "loss": 1.2436,
      "step": 154000
    },
    {
      "epoch": 0.4801204274801547,
      "grad_norm": 0.6770893335342407,
      "learning_rate": 4.2000849807442777e-05,
      "loss": 1.297,
      "step": 154050
    },
    {
      "epoch": 0.4802762601408104,
      "grad_norm": 0.6809379458427429,
      "learning_rate": 4.199825259643185e-05,
      "loss": 1.3194,
      "step": 154100
    },
    {
      "epoch": 0.4804320928014661,
      "grad_norm": 0.5813418626785278,
      "learning_rate": 4.199565538542092e-05,
      "loss": 1.2552,
      "step": 154150
    },
    {
      "epoch": 0.48058792546212176,
      "grad_norm": 0.6389602422714233,
      "learning_rate": 4.1993058174409994e-05,
      "loss": 1.2479,
      "step": 154200
    },
    {
      "epoch": 0.4807437581227774,
      "grad_norm": 0.6299837827682495,
      "learning_rate": 4.1990460963399067e-05,
      "loss": 1.3028,
      "step": 154250
    },
    {
      "epoch": 0.48089959078343314,
      "grad_norm": 0.575675904750824,
      "learning_rate": 4.198786375238814e-05,
      "loss": 1.2932,
      "step": 154300
    },
    {
      "epoch": 0.4810554234440888,
      "grad_norm": 0.41739413142204285,
      "learning_rate": 4.198526654137721e-05,
      "loss": 1.2812,
      "step": 154350
    },
    {
      "epoch": 0.48121125610474447,
      "grad_norm": 0.8225050568580627,
      "learning_rate": 4.1982669330366284e-05,
      "loss": 1.2562,
      "step": 154400
    },
    {
      "epoch": 0.4813670887654002,
      "grad_norm": 0.6264882683753967,
      "learning_rate": 4.198007211935535e-05,
      "loss": 1.3053,
      "step": 154450
    },
    {
      "epoch": 0.48152292142605585,
      "grad_norm": 0.697156548500061,
      "learning_rate": 4.197747490834443e-05,
      "loss": 1.2619,
      "step": 154500
    },
    {
      "epoch": 0.4816787540867115,
      "grad_norm": 0.644792914390564,
      "learning_rate": 4.1974877697333495e-05,
      "loss": 1.2772,
      "step": 154550
    },
    {
      "epoch": 0.48183458674736723,
      "grad_norm": 0.6950079202651978,
      "learning_rate": 4.197228048632257e-05,
      "loss": 1.3184,
      "step": 154600
    },
    {
      "epoch": 0.4819904194080229,
      "grad_norm": 0.5504694581031799,
      "learning_rate": 4.196968327531164e-05,
      "loss": 1.2817,
      "step": 154650
    },
    {
      "epoch": 0.48214625206867856,
      "grad_norm": 0.6105301976203918,
      "learning_rate": 4.196708606430071e-05,
      "loss": 1.2758,
      "step": 154700
    },
    {
      "epoch": 0.4823020847293343,
      "grad_norm": 0.39047184586524963,
      "learning_rate": 4.1964488853289785e-05,
      "loss": 1.2371,
      "step": 154750
    },
    {
      "epoch": 0.48245791738998994,
      "grad_norm": 0.742157518863678,
      "learning_rate": 4.196189164227886e-05,
      "loss": 1.216,
      "step": 154800
    },
    {
      "epoch": 0.4826137500506456,
      "grad_norm": 0.5120607018470764,
      "learning_rate": 4.195929443126793e-05,
      "loss": 1.285,
      "step": 154850
    },
    {
      "epoch": 0.48276958271130127,
      "grad_norm": 0.5877612829208374,
      "learning_rate": 4.1956697220257e-05,
      "loss": 1.3026,
      "step": 154900
    },
    {
      "epoch": 0.482925415371957,
      "grad_norm": 0.6351940631866455,
      "learning_rate": 4.1954100009246075e-05,
      "loss": 1.2985,
      "step": 154950
    },
    {
      "epoch": 0.48308124803261265,
      "grad_norm": 0.6294667720794678,
      "learning_rate": 4.195150279823514e-05,
      "loss": 1.2741,
      "step": 155000
    },
    {
      "epoch": 0.4832370806932683,
      "grad_norm": 0.6956526041030884,
      "learning_rate": 4.194890558722422e-05,
      "loss": 1.3396,
      "step": 155050
    },
    {
      "epoch": 0.48339291335392404,
      "grad_norm": 0.6768592000007629,
      "learning_rate": 4.194630837621329e-05,
      "loss": 1.2897,
      "step": 155100
    },
    {
      "epoch": 0.4835487460145797,
      "grad_norm": 0.5943355560302734,
      "learning_rate": 4.194371116520236e-05,
      "loss": 1.2817,
      "step": 155150
    },
    {
      "epoch": 0.48370457867523536,
      "grad_norm": 0.4201481342315674,
      "learning_rate": 4.194111395419144e-05,
      "loss": 1.2647,
      "step": 155200
    },
    {
      "epoch": 0.4838604113358911,
      "grad_norm": 0.6928107738494873,
      "learning_rate": 4.19385167431805e-05,
      "loss": 1.2544,
      "step": 155250
    },
    {
      "epoch": 0.48401624399654675,
      "grad_norm": 0.7017381191253662,
      "learning_rate": 4.1935919532169576e-05,
      "loss": 1.2482,
      "step": 155300
    },
    {
      "epoch": 0.4841720766572024,
      "grad_norm": 0.6909313201904297,
      "learning_rate": 4.193332232115865e-05,
      "loss": 1.2973,
      "step": 155350
    },
    {
      "epoch": 0.48432790931785813,
      "grad_norm": 0.6563231348991394,
      "learning_rate": 4.193072511014772e-05,
      "loss": 1.2904,
      "step": 155400
    },
    {
      "epoch": 0.4844837419785138,
      "grad_norm": 0.5109058022499084,
      "learning_rate": 4.192812789913679e-05,
      "loss": 1.2797,
      "step": 155450
    },
    {
      "epoch": 0.48463957463916946,
      "grad_norm": 0.5286438465118408,
      "learning_rate": 4.1925582632346085e-05,
      "loss": 1.2211,
      "step": 155500
    },
    {
      "epoch": 0.4847954072998252,
      "grad_norm": 0.6096494793891907,
      "learning_rate": 4.192298542133515e-05,
      "loss": 1.3294,
      "step": 155550
    },
    {
      "epoch": 0.48495123996048084,
      "grad_norm": 0.6185253858566284,
      "learning_rate": 4.192038821032423e-05,
      "loss": 1.3114,
      "step": 155600
    },
    {
      "epoch": 0.4851070726211365,
      "grad_norm": 0.5128195285797119,
      "learning_rate": 4.19177909993133e-05,
      "loss": 1.2879,
      "step": 155650
    },
    {
      "epoch": 0.4852629052817922,
      "grad_norm": 0.44154155254364014,
      "learning_rate": 4.191519378830237e-05,
      "loss": 1.3092,
      "step": 155700
    },
    {
      "epoch": 0.4854187379424479,
      "grad_norm": 0.5465865731239319,
      "learning_rate": 4.191259657729144e-05,
      "loss": 1.2889,
      "step": 155750
    },
    {
      "epoch": 0.48557457060310355,
      "grad_norm": 0.6269431114196777,
      "learning_rate": 4.1909999366280513e-05,
      "loss": 1.2919,
      "step": 155800
    },
    {
      "epoch": 0.48573040326375927,
      "grad_norm": 0.7134231328964233,
      "learning_rate": 4.1907402155269586e-05,
      "loss": 1.3153,
      "step": 155850
    },
    {
      "epoch": 0.48588623592441493,
      "grad_norm": 0.5901727080345154,
      "learning_rate": 4.190480494425866e-05,
      "loss": 1.2708,
      "step": 155900
    },
    {
      "epoch": 0.4860420685850706,
      "grad_norm": 0.6751509308815002,
      "learning_rate": 4.190220773324773e-05,
      "loss": 1.2635,
      "step": 155950
    },
    {
      "epoch": 0.4861979012457263,
      "grad_norm": 0.48710259795188904,
      "learning_rate": 4.1899610522236803e-05,
      "loss": 1.324,
      "step": 156000
    },
    {
      "epoch": 0.486353733906382,
      "grad_norm": 0.5814753174781799,
      "learning_rate": 4.1897013311225876e-05,
      "loss": 1.2726,
      "step": 156050
    },
    {
      "epoch": 0.48650956656703764,
      "grad_norm": 0.6888899803161621,
      "learning_rate": 4.189441610021494e-05,
      "loss": 1.221,
      "step": 156100
    },
    {
      "epoch": 0.4866653992276933,
      "grad_norm": 0.664042055606842,
      "learning_rate": 4.189181888920402e-05,
      "loss": 1.2729,
      "step": 156150
    },
    {
      "epoch": 0.486821231888349,
      "grad_norm": 0.48447707295417786,
      "learning_rate": 4.1889221678193093e-05,
      "loss": 1.2143,
      "step": 156200
    },
    {
      "epoch": 0.4869770645490047,
      "grad_norm": 0.6857041120529175,
      "learning_rate": 4.188662446718216e-05,
      "loss": 1.2173,
      "step": 156250
    },
    {
      "epoch": 0.48713289720966035,
      "grad_norm": 0.5956174731254578,
      "learning_rate": 4.188402725617124e-05,
      "loss": 1.3161,
      "step": 156300
    },
    {
      "epoch": 0.4872887298703161,
      "grad_norm": 0.680262565612793,
      "learning_rate": 4.188143004516031e-05,
      "loss": 1.2752,
      "step": 156350
    },
    {
      "epoch": 0.48744456253097174,
      "grad_norm": 0.6747142672538757,
      "learning_rate": 4.187883283414938e-05,
      "loss": 1.2759,
      "step": 156400
    },
    {
      "epoch": 0.4876003951916274,
      "grad_norm": 0.5383468270301819,
      "learning_rate": 4.187623562313845e-05,
      "loss": 1.2578,
      "step": 156450
    },
    {
      "epoch": 0.4877562278522831,
      "grad_norm": 0.5322409272193909,
      "learning_rate": 4.187363841212752e-05,
      "loss": 1.3009,
      "step": 156500
    },
    {
      "epoch": 0.4879120605129388,
      "grad_norm": 0.5413229465484619,
      "learning_rate": 4.1871041201116594e-05,
      "loss": 1.2984,
      "step": 156550
    },
    {
      "epoch": 0.48806789317359445,
      "grad_norm": 0.5460690259933472,
      "learning_rate": 4.186844399010567e-05,
      "loss": 1.2625,
      "step": 156600
    },
    {
      "epoch": 0.48822372583425017,
      "grad_norm": 0.6101374626159668,
      "learning_rate": 4.186584677909474e-05,
      "loss": 1.2412,
      "step": 156650
    },
    {
      "epoch": 0.48837955849490583,
      "grad_norm": 0.5169169306755066,
      "learning_rate": 4.186324956808381e-05,
      "loss": 1.2671,
      "step": 156700
    },
    {
      "epoch": 0.4885353911555615,
      "grad_norm": 0.6748039722442627,
      "learning_rate": 4.1860652357072884e-05,
      "loss": 1.2482,
      "step": 156750
    },
    {
      "epoch": 0.4886912238162172,
      "grad_norm": 0.7400112748146057,
      "learning_rate": 4.185805514606195e-05,
      "loss": 1.2728,
      "step": 156800
    },
    {
      "epoch": 0.4888470564768729,
      "grad_norm": 0.5997214913368225,
      "learning_rate": 4.185545793505103e-05,
      "loss": 1.2501,
      "step": 156850
    },
    {
      "epoch": 0.48900288913752854,
      "grad_norm": 0.6550659537315369,
      "learning_rate": 4.18528607240401e-05,
      "loss": 1.2429,
      "step": 156900
    },
    {
      "epoch": 0.48915872179818426,
      "grad_norm": 0.4764077961444855,
      "learning_rate": 4.185026351302917e-05,
      "loss": 1.3037,
      "step": 156950
    },
    {
      "epoch": 0.4893145544588399,
      "grad_norm": 0.6636883616447449,
      "learning_rate": 4.184766630201824e-05,
      "loss": 1.2615,
      "step": 157000
    },
    {
      "epoch": 0.4894703871194956,
      "grad_norm": 0.6376081109046936,
      "learning_rate": 4.184506909100732e-05,
      "loss": 1.2342,
      "step": 157050
    },
    {
      "epoch": 0.4896262197801513,
      "grad_norm": 0.5746507048606873,
      "learning_rate": 4.1842471879996385e-05,
      "loss": 1.238,
      "step": 157100
    },
    {
      "epoch": 0.48978205244080697,
      "grad_norm": 0.7253598570823669,
      "learning_rate": 4.183987466898546e-05,
      "loss": 1.3194,
      "step": 157150
    },
    {
      "epoch": 0.48993788510146263,
      "grad_norm": 0.5065282583236694,
      "learning_rate": 4.183727745797453e-05,
      "loss": 1.3024,
      "step": 157200
    },
    {
      "epoch": 0.49009371776211835,
      "grad_norm": 0.4739120304584503,
      "learning_rate": 4.18346802469636e-05,
      "loss": 1.2608,
      "step": 157250
    },
    {
      "epoch": 0.490249550422774,
      "grad_norm": 0.6674689650535583,
      "learning_rate": 4.1832083035952675e-05,
      "loss": 1.2955,
      "step": 157300
    },
    {
      "epoch": 0.4904053830834297,
      "grad_norm": 0.6422510743141174,
      "learning_rate": 4.182948582494175e-05,
      "loss": 1.3103,
      "step": 157350
    },
    {
      "epoch": 0.49056121574408534,
      "grad_norm": 0.5377429723739624,
      "learning_rate": 4.182688861393082e-05,
      "loss": 1.2836,
      "step": 157400
    },
    {
      "epoch": 0.49071704840474106,
      "grad_norm": 0.5767998099327087,
      "learning_rate": 4.182429140291989e-05,
      "loss": 1.2967,
      "step": 157450
    },
    {
      "epoch": 0.4908728810653967,
      "grad_norm": 0.8134852051734924,
      "learning_rate": 4.182174613612918e-05,
      "loss": 1.2883,
      "step": 157500
    },
    {
      "epoch": 0.4910287137260524,
      "grad_norm": 0.6279510855674744,
      "learning_rate": 4.181914892511825e-05,
      "loss": 1.2565,
      "step": 157550
    },
    {
      "epoch": 0.4911845463867081,
      "grad_norm": 0.5469830632209778,
      "learning_rate": 4.181655171410733e-05,
      "loss": 1.3118,
      "step": 157600
    },
    {
      "epoch": 0.4913403790473638,
      "grad_norm": 0.6305583715438843,
      "learning_rate": 4.1813954503096395e-05,
      "loss": 1.2751,
      "step": 157650
    },
    {
      "epoch": 0.49149621170801944,
      "grad_norm": 0.4886857569217682,
      "learning_rate": 4.181135729208547e-05,
      "loss": 1.1879,
      "step": 157700
    },
    {
      "epoch": 0.49165204436867516,
      "grad_norm": 0.6258983612060547,
      "learning_rate": 4.180876008107454e-05,
      "loss": 1.2136,
      "step": 157750
    },
    {
      "epoch": 0.4918078770293308,
      "grad_norm": 0.5934269428253174,
      "learning_rate": 4.180616287006361e-05,
      "loss": 1.3191,
      "step": 157800
    },
    {
      "epoch": 0.4919637096899865,
      "grad_norm": 0.5751442909240723,
      "learning_rate": 4.1803565659052685e-05,
      "loss": 1.3084,
      "step": 157850
    },
    {
      "epoch": 0.4921195423506422,
      "grad_norm": 0.5481546521186829,
      "learning_rate": 4.180096844804176e-05,
      "loss": 1.2308,
      "step": 157900
    },
    {
      "epoch": 0.49227537501129787,
      "grad_norm": 0.49354538321495056,
      "learning_rate": 4.179837123703083e-05,
      "loss": 1.3059,
      "step": 157950
    },
    {
      "epoch": 0.49243120767195353,
      "grad_norm": 0.6043608784675598,
      "learning_rate": 4.17957740260199e-05,
      "loss": 1.3034,
      "step": 158000
    },
    {
      "epoch": 0.49258704033260925,
      "grad_norm": 0.4316123127937317,
      "learning_rate": 4.179317681500897e-05,
      "loss": 1.279,
      "step": 158050
    },
    {
      "epoch": 0.4927428729932649,
      "grad_norm": 0.5747493505477905,
      "learning_rate": 4.179057960399804e-05,
      "loss": 1.256,
      "step": 158100
    },
    {
      "epoch": 0.4928987056539206,
      "grad_norm": 0.5526713132858276,
      "learning_rate": 4.178798239298712e-05,
      "loss": 1.2668,
      "step": 158150
    },
    {
      "epoch": 0.4930545383145763,
      "grad_norm": 0.5472176671028137,
      "learning_rate": 4.1785385181976186e-05,
      "loss": 1.2483,
      "step": 158200
    },
    {
      "epoch": 0.49321037097523196,
      "grad_norm": 0.593843936920166,
      "learning_rate": 4.178278797096526e-05,
      "loss": 1.2858,
      "step": 158250
    },
    {
      "epoch": 0.4933662036358876,
      "grad_norm": 0.6400632262229919,
      "learning_rate": 4.178019075995434e-05,
      "loss": 1.2546,
      "step": 158300
    },
    {
      "epoch": 0.49352203629654334,
      "grad_norm": 0.6220535039901733,
      "learning_rate": 4.1777593548943404e-05,
      "loss": 1.2488,
      "step": 158350
    },
    {
      "epoch": 0.493677868957199,
      "grad_norm": 0.605890154838562,
      "learning_rate": 4.1774996337932476e-05,
      "loss": 1.2848,
      "step": 158400
    },
    {
      "epoch": 0.49383370161785467,
      "grad_norm": 0.6356067657470703,
      "learning_rate": 4.177239912692155e-05,
      "loss": 1.2795,
      "step": 158450
    },
    {
      "epoch": 0.4939895342785104,
      "grad_norm": 0.5408151149749756,
      "learning_rate": 4.176980191591062e-05,
      "loss": 1.2879,
      "step": 158500
    },
    {
      "epoch": 0.49414536693916605,
      "grad_norm": 0.6549515724182129,
      "learning_rate": 4.1767204704899694e-05,
      "loss": 1.2788,
      "step": 158550
    },
    {
      "epoch": 0.4943011995998217,
      "grad_norm": 0.7384257912635803,
      "learning_rate": 4.1764607493888766e-05,
      "loss": 1.2682,
      "step": 158600
    },
    {
      "epoch": 0.49445703226047744,
      "grad_norm": 0.6326021552085876,
      "learning_rate": 4.176201028287784e-05,
      "loss": 1.2846,
      "step": 158650
    },
    {
      "epoch": 0.4946128649211331,
      "grad_norm": 0.7298886775970459,
      "learning_rate": 4.175941307186691e-05,
      "loss": 1.2633,
      "step": 158700
    },
    {
      "epoch": 0.49476869758178876,
      "grad_norm": 0.596337080001831,
      "learning_rate": 4.175681586085598e-05,
      "loss": 1.277,
      "step": 158750
    },
    {
      "epoch": 0.4949245302424444,
      "grad_norm": 0.7105053663253784,
      "learning_rate": 4.175421864984505e-05,
      "loss": 1.273,
      "step": 158800
    },
    {
      "epoch": 0.49508036290310015,
      "grad_norm": 0.6743444204330444,
      "learning_rate": 4.175162143883413e-05,
      "loss": 1.3094,
      "step": 158850
    },
    {
      "epoch": 0.4952361955637558,
      "grad_norm": 0.49192488193511963,
      "learning_rate": 4.1749024227823194e-05,
      "loss": 1.3129,
      "step": 158900
    },
    {
      "epoch": 0.4953920282244115,
      "grad_norm": 0.5442329049110413,
      "learning_rate": 4.174642701681227e-05,
      "loss": 1.2824,
      "step": 158950
    },
    {
      "epoch": 0.4955478608850672,
      "grad_norm": 0.6288788318634033,
      "learning_rate": 4.174382980580134e-05,
      "loss": 1.2767,
      "step": 159000
    },
    {
      "epoch": 0.49570369354572286,
      "grad_norm": 0.46112820506095886,
      "learning_rate": 4.174123259479041e-05,
      "loss": 1.3253,
      "step": 159050
    },
    {
      "epoch": 0.4958595262063785,
      "grad_norm": 0.6052106618881226,
      "learning_rate": 4.1738635383779484e-05,
      "loss": 1.2594,
      "step": 159100
    },
    {
      "epoch": 0.49601535886703424,
      "grad_norm": 0.5876401662826538,
      "learning_rate": 4.173603817276856e-05,
      "loss": 1.261,
      "step": 159150
    },
    {
      "epoch": 0.4961711915276899,
      "grad_norm": 0.5343034267425537,
      "learning_rate": 4.173344096175763e-05,
      "loss": 1.3141,
      "step": 159200
    },
    {
      "epoch": 0.49632702418834557,
      "grad_norm": 0.4880959093570709,
      "learning_rate": 4.17308437507467e-05,
      "loss": 1.2938,
      "step": 159250
    },
    {
      "epoch": 0.4964828568490013,
      "grad_norm": 0.590978741645813,
      "learning_rate": 4.1728246539735774e-05,
      "loss": 1.2574,
      "step": 159300
    },
    {
      "epoch": 0.49663868950965695,
      "grad_norm": 0.697171688079834,
      "learning_rate": 4.172564932872484e-05,
      "loss": 1.2697,
      "step": 159350
    },
    {
      "epoch": 0.4967945221703126,
      "grad_norm": 0.48719704151153564,
      "learning_rate": 4.172305211771392e-05,
      "loss": 1.2881,
      "step": 159400
    },
    {
      "epoch": 0.49695035483096833,
      "grad_norm": 0.5652236938476562,
      "learning_rate": 4.1720454906702985e-05,
      "loss": 1.3011,
      "step": 159450
    },
    {
      "epoch": 0.497106187491624,
      "grad_norm": 0.6203439235687256,
      "learning_rate": 4.171785769569206e-05,
      "loss": 1.2644,
      "step": 159500
    },
    {
      "epoch": 0.49726202015227966,
      "grad_norm": 0.6287343502044678,
      "learning_rate": 4.171526048468114e-05,
      "loss": 1.2522,
      "step": 159550
    },
    {
      "epoch": 0.4974178528129354,
      "grad_norm": 0.5995460748672485,
      "learning_rate": 4.17126632736702e-05,
      "loss": 1.2837,
      "step": 159600
    },
    {
      "epoch": 0.49757368547359104,
      "grad_norm": 0.7213717103004456,
      "learning_rate": 4.1710066062659275e-05,
      "loss": 1.2644,
      "step": 159650
    },
    {
      "epoch": 0.4977295181342467,
      "grad_norm": 0.5764607191085815,
      "learning_rate": 4.170746885164835e-05,
      "loss": 1.2773,
      "step": 159700
    },
    {
      "epoch": 0.4978853507949024,
      "grad_norm": 0.6368909478187561,
      "learning_rate": 4.170492358485763e-05,
      "loss": 1.33,
      "step": 159750
    },
    {
      "epoch": 0.4980411834555581,
      "grad_norm": 0.43627485632896423,
      "learning_rate": 4.170232637384671e-05,
      "loss": 1.2997,
      "step": 159800
    },
    {
      "epoch": 0.49819701611621375,
      "grad_norm": 0.5312307476997375,
      "learning_rate": 4.1699729162835785e-05,
      "loss": 1.2161,
      "step": 159850
    },
    {
      "epoch": 0.4983528487768695,
      "grad_norm": 0.5676446557044983,
      "learning_rate": 4.169713195182485e-05,
      "loss": 1.286,
      "step": 159900
    },
    {
      "epoch": 0.49850868143752514,
      "grad_norm": 0.5603193640708923,
      "learning_rate": 4.169453474081393e-05,
      "loss": 1.3298,
      "step": 159950
    },
    {
      "epoch": 0.4986645140981808,
      "grad_norm": 0.602201521396637,
      "learning_rate": 4.1691937529803e-05,
      "loss": 1.2887,
      "step": 160000
    },
    {
      "epoch": 0.49882034675883646,
      "grad_norm": 0.6851734519004822,
      "learning_rate": 4.168934031879207e-05,
      "loss": 1.3312,
      "step": 160050
    },
    {
      "epoch": 0.4989761794194922,
      "grad_norm": 0.6631762981414795,
      "learning_rate": 4.168674310778114e-05,
      "loss": 1.2907,
      "step": 160100
    },
    {
      "epoch": 0.49913201208014785,
      "grad_norm": 0.5929980278015137,
      "learning_rate": 4.168414589677021e-05,
      "loss": 1.2286,
      "step": 160150
    },
    {
      "epoch": 0.4992878447408035,
      "grad_norm": 0.5866820216178894,
      "learning_rate": 4.1681548685759286e-05,
      "loss": 1.2515,
      "step": 160200
    },
    {
      "epoch": 0.49944367740145923,
      "grad_norm": 0.5992395281791687,
      "learning_rate": 4.167895147474836e-05,
      "loss": 1.3036,
      "step": 160250
    },
    {
      "epoch": 0.4995995100621149,
      "grad_norm": 0.6288427710533142,
      "learning_rate": 4.167635426373743e-05,
      "loss": 1.3124,
      "step": 160300
    },
    {
      "epoch": 0.49975534272277056,
      "grad_norm": 0.6356044411659241,
      "learning_rate": 4.16737570527265e-05,
      "loss": 1.2894,
      "step": 160350
    },
    {
      "epoch": 0.4999111753834263,
      "grad_norm": 0.7295847535133362,
      "learning_rate": 4.1671159841715576e-05,
      "loss": 1.2536,
      "step": 160400
    },
    {
      "epoch": 0.500067008044082,
      "grad_norm": 0.5252774953842163,
      "learning_rate": 4.166856263070464e-05,
      "loss": 1.2919,
      "step": 160450
    },
    {
      "epoch": 0.5002228407047377,
      "grad_norm": 0.5898911952972412,
      "learning_rate": 4.166596541969372e-05,
      "loss": 1.2733,
      "step": 160500
    },
    {
      "epoch": 0.5003786733653933,
      "grad_norm": 0.5931973457336426,
      "learning_rate": 4.166336820868279e-05,
      "loss": 1.3115,
      "step": 160550
    },
    {
      "epoch": 0.500534506026049,
      "grad_norm": 0.5813238620758057,
      "learning_rate": 4.166077099767186e-05,
      "loss": 1.269,
      "step": 160600
    },
    {
      "epoch": 0.5006903386867047,
      "grad_norm": 0.5445960760116577,
      "learning_rate": 4.165817378666094e-05,
      "loss": 1.296,
      "step": 160650
    },
    {
      "epoch": 0.5008461713473603,
      "grad_norm": 0.6110395789146423,
      "learning_rate": 4.1655576575650004e-05,
      "loss": 1.2582,
      "step": 160700
    },
    {
      "epoch": 0.501002004008016,
      "grad_norm": 0.6724881529808044,
      "learning_rate": 4.1652979364639076e-05,
      "loss": 1.2921,
      "step": 160750
    },
    {
      "epoch": 0.5011578366686718,
      "grad_norm": 0.6815361976623535,
      "learning_rate": 4.165038215362815e-05,
      "loss": 1.3188,
      "step": 160800
    },
    {
      "epoch": 0.5013136693293274,
      "grad_norm": 0.6636915802955627,
      "learning_rate": 4.164778494261722e-05,
      "loss": 1.2739,
      "step": 160850
    },
    {
      "epoch": 0.5014695019899831,
      "grad_norm": 0.6765465140342712,
      "learning_rate": 4.1645187731606294e-05,
      "loss": 1.2518,
      "step": 160900
    },
    {
      "epoch": 0.5016253346506387,
      "grad_norm": 0.6579752564430237,
      "learning_rate": 4.1642590520595366e-05,
      "loss": 1.2525,
      "step": 160950
    },
    {
      "epoch": 0.5017811673112944,
      "grad_norm": 0.7135781645774841,
      "learning_rate": 4.163999330958443e-05,
      "loss": 1.2921,
      "step": 161000
    },
    {
      "epoch": 0.5019369999719501,
      "grad_norm": 0.5356391668319702,
      "learning_rate": 4.163739609857351e-05,
      "loss": 1.2465,
      "step": 161050
    },
    {
      "epoch": 0.5020928326326058,
      "grad_norm": 0.49942606687545776,
      "learning_rate": 4.1634798887562584e-05,
      "loss": 1.2795,
      "step": 161100
    },
    {
      "epoch": 0.5022486652932615,
      "grad_norm": 0.7994436025619507,
      "learning_rate": 4.163220167655165e-05,
      "loss": 1.3438,
      "step": 161150
    },
    {
      "epoch": 0.5024044979539172,
      "grad_norm": 0.4329412877559662,
      "learning_rate": 4.162960446554073e-05,
      "loss": 1.2914,
      "step": 161200
    },
    {
      "epoch": 0.5025603306145728,
      "grad_norm": 0.7412654757499695,
      "learning_rate": 4.16270072545298e-05,
      "loss": 1.2673,
      "step": 161250
    },
    {
      "epoch": 0.5027161632752285,
      "grad_norm": 0.5612020492553711,
      "learning_rate": 4.162441004351887e-05,
      "loss": 1.2969,
      "step": 161300
    },
    {
      "epoch": 0.5028719959358842,
      "grad_norm": 0.7150516510009766,
      "learning_rate": 4.162181283250794e-05,
      "loss": 1.2529,
      "step": 161350
    },
    {
      "epoch": 0.5030278285965399,
      "grad_norm": 0.5729686617851257,
      "learning_rate": 4.161921562149701e-05,
      "loss": 1.2496,
      "step": 161400
    },
    {
      "epoch": 0.5031836612571956,
      "grad_norm": 0.6263851523399353,
      "learning_rate": 4.1616618410486085e-05,
      "loss": 1.2758,
      "step": 161450
    },
    {
      "epoch": 0.5033394939178513,
      "grad_norm": 0.5635141134262085,
      "learning_rate": 4.161402119947516e-05,
      "loss": 1.2658,
      "step": 161500
    },
    {
      "epoch": 0.5034953265785069,
      "grad_norm": 0.767611563205719,
      "learning_rate": 4.161142398846423e-05,
      "loss": 1.2637,
      "step": 161550
    },
    {
      "epoch": 0.5036511592391626,
      "grad_norm": 0.6055853962898254,
      "learning_rate": 4.16088267774533e-05,
      "loss": 1.1986,
      "step": 161600
    },
    {
      "epoch": 0.5038069918998183,
      "grad_norm": 0.6484196186065674,
      "learning_rate": 4.1606229566442375e-05,
      "loss": 1.2936,
      "step": 161650
    },
    {
      "epoch": 0.503962824560474,
      "grad_norm": 0.5686272382736206,
      "learning_rate": 4.160363235543144e-05,
      "loss": 1.2527,
      "step": 161700
    },
    {
      "epoch": 0.5041186572211297,
      "grad_norm": 0.6436412930488586,
      "learning_rate": 4.160103514442052e-05,
      "loss": 1.2653,
      "step": 161750
    },
    {
      "epoch": 0.5042744898817854,
      "grad_norm": 0.6022602319717407,
      "learning_rate": 4.159843793340959e-05,
      "loss": 1.3006,
      "step": 161800
    },
    {
      "epoch": 0.504430322542441,
      "grad_norm": 0.4908212721347809,
      "learning_rate": 4.159584072239866e-05,
      "loss": 1.2558,
      "step": 161850
    },
    {
      "epoch": 0.5045861552030967,
      "grad_norm": 0.6022503972053528,
      "learning_rate": 4.159324351138774e-05,
      "loss": 1.2355,
      "step": 161900
    },
    {
      "epoch": 0.5047419878637524,
      "grad_norm": 0.6264612078666687,
      "learning_rate": 4.159064630037681e-05,
      "loss": 1.2111,
      "step": 161950
    },
    {
      "epoch": 0.504897820524408,
      "grad_norm": 0.61235111951828,
      "learning_rate": 4.1588049089365875e-05,
      "loss": 1.3066,
      "step": 162000
    },
    {
      "epoch": 0.5050536531850638,
      "grad_norm": 0.6951838731765747,
      "learning_rate": 4.158545187835495e-05,
      "loss": 1.2973,
      "step": 162050
    },
    {
      "epoch": 0.5052094858457195,
      "grad_norm": 0.6312398314476013,
      "learning_rate": 4.158285466734402e-05,
      "loss": 1.2612,
      "step": 162100
    },
    {
      "epoch": 0.5053653185063751,
      "grad_norm": 0.5998066067695618,
      "learning_rate": 4.158030940055331e-05,
      "loss": 1.2449,
      "step": 162150
    },
    {
      "epoch": 0.5055211511670308,
      "grad_norm": 0.6167109608650208,
      "learning_rate": 4.1577712189542385e-05,
      "loss": 1.2836,
      "step": 162200
    },
    {
      "epoch": 0.5056769838276864,
      "grad_norm": 0.6243129968643188,
      "learning_rate": 4.157511497853146e-05,
      "loss": 1.2495,
      "step": 162250
    },
    {
      "epoch": 0.5058328164883421,
      "grad_norm": 0.6831839084625244,
      "learning_rate": 4.157251776752053e-05,
      "loss": 1.2403,
      "step": 162300
    },
    {
      "epoch": 0.5059886491489979,
      "grad_norm": 0.6565070152282715,
      "learning_rate": 4.15699205565096e-05,
      "loss": 1.3201,
      "step": 162350
    },
    {
      "epoch": 0.5061444818096535,
      "grad_norm": 0.7596964836120605,
      "learning_rate": 4.156732334549867e-05,
      "loss": 1.3017,
      "step": 162400
    },
    {
      "epoch": 0.5063003144703092,
      "grad_norm": 0.3979642391204834,
      "learning_rate": 4.156472613448774e-05,
      "loss": 1.2998,
      "step": 162450
    },
    {
      "epoch": 0.5064561471309649,
      "grad_norm": 0.6216937303543091,
      "learning_rate": 4.156212892347682e-05,
      "loss": 1.2707,
      "step": 162500
    },
    {
      "epoch": 0.5066119797916205,
      "grad_norm": 0.7127414345741272,
      "learning_rate": 4.1559531712465886e-05,
      "loss": 1.2607,
      "step": 162550
    },
    {
      "epoch": 0.5067678124522762,
      "grad_norm": 0.591046929359436,
      "learning_rate": 4.155693450145496e-05,
      "loss": 1.2425,
      "step": 162600
    },
    {
      "epoch": 0.506923645112932,
      "grad_norm": 0.548032283782959,
      "learning_rate": 4.155433729044404e-05,
      "loss": 1.2141,
      "step": 162650
    },
    {
      "epoch": 0.5070794777735876,
      "grad_norm": 0.6495727300643921,
      "learning_rate": 4.15517400794331e-05,
      "loss": 1.2584,
      "step": 162700
    },
    {
      "epoch": 0.5072353104342433,
      "grad_norm": 0.5828865766525269,
      "learning_rate": 4.1549142868422176e-05,
      "loss": 1.3044,
      "step": 162750
    },
    {
      "epoch": 0.507391143094899,
      "grad_norm": 0.7445511221885681,
      "learning_rate": 4.154654565741125e-05,
      "loss": 1.2653,
      "step": 162800
    },
    {
      "epoch": 0.5075469757555546,
      "grad_norm": 0.7321845293045044,
      "learning_rate": 4.154394844640032e-05,
      "loss": 1.2565,
      "step": 162850
    },
    {
      "epoch": 0.5077028084162103,
      "grad_norm": 0.5692026019096375,
      "learning_rate": 4.154135123538939e-05,
      "loss": 1.2964,
      "step": 162900
    },
    {
      "epoch": 0.5078586410768661,
      "grad_norm": 0.7109234929084778,
      "learning_rate": 4.153875402437846e-05,
      "loss": 1.2259,
      "step": 162950
    },
    {
      "epoch": 0.5080144737375217,
      "grad_norm": 0.6557603478431702,
      "learning_rate": 4.153615681336753e-05,
      "loss": 1.2677,
      "step": 163000
    },
    {
      "epoch": 0.5081703063981774,
      "grad_norm": 0.7190135717391968,
      "learning_rate": 4.153355960235661e-05,
      "loss": 1.3436,
      "step": 163050
    },
    {
      "epoch": 0.5083261390588331,
      "grad_norm": 0.7249311208724976,
      "learning_rate": 4.1530962391345676e-05,
      "loss": 1.281,
      "step": 163100
    },
    {
      "epoch": 0.5084819717194887,
      "grad_norm": 0.6470355987548828,
      "learning_rate": 4.152836518033475e-05,
      "loss": 1.2792,
      "step": 163150
    },
    {
      "epoch": 0.5086378043801444,
      "grad_norm": 0.5382999181747437,
      "learning_rate": 4.152576796932383e-05,
      "loss": 1.2592,
      "step": 163200
    },
    {
      "epoch": 0.5087936370408002,
      "grad_norm": 0.5501776933670044,
      "learning_rate": 4.1523170758312894e-05,
      "loss": 1.2221,
      "step": 163250
    },
    {
      "epoch": 0.5089494697014558,
      "grad_norm": 0.6405466198921204,
      "learning_rate": 4.1520573547301966e-05,
      "loss": 1.3317,
      "step": 163300
    },
    {
      "epoch": 0.5091053023621115,
      "grad_norm": 0.8045985698699951,
      "learning_rate": 4.151797633629104e-05,
      "loss": 1.2731,
      "step": 163350
    },
    {
      "epoch": 0.5092611350227672,
      "grad_norm": 0.5496698021888733,
      "learning_rate": 4.151537912528011e-05,
      "loss": 1.3076,
      "step": 163400
    },
    {
      "epoch": 0.5094169676834228,
      "grad_norm": 0.6281981468200684,
      "learning_rate": 4.1512781914269184e-05,
      "loss": 1.324,
      "step": 163450
    },
    {
      "epoch": 0.5095728003440785,
      "grad_norm": 0.5798326730728149,
      "learning_rate": 4.1510184703258256e-05,
      "loss": 1.2575,
      "step": 163500
    },
    {
      "epoch": 0.5097286330047341,
      "grad_norm": 0.6405347585678101,
      "learning_rate": 4.150758749224733e-05,
      "loss": 1.3214,
      "step": 163550
    },
    {
      "epoch": 0.5098844656653899,
      "grad_norm": 0.5253356099128723,
      "learning_rate": 4.15049902812364e-05,
      "loss": 1.2953,
      "step": 163600
    },
    {
      "epoch": 0.5100402983260456,
      "grad_norm": 0.6139646172523499,
      "learning_rate": 4.150239307022547e-05,
      "loss": 1.2459,
      "step": 163650
    },
    {
      "epoch": 0.5101961309867012,
      "grad_norm": 0.5007148385047913,
      "learning_rate": 4.149979585921454e-05,
      "loss": 1.2386,
      "step": 163700
    },
    {
      "epoch": 0.5103519636473569,
      "grad_norm": 0.5405750870704651,
      "learning_rate": 4.149719864820362e-05,
      "loss": 1.2527,
      "step": 163750
    },
    {
      "epoch": 0.5105077963080126,
      "grad_norm": 0.5278601050376892,
      "learning_rate": 4.1494601437192685e-05,
      "loss": 1.301,
      "step": 163800
    },
    {
      "epoch": 0.5106636289686682,
      "grad_norm": 0.8121151924133301,
      "learning_rate": 4.149200422618176e-05,
      "loss": 1.2962,
      "step": 163850
    },
    {
      "epoch": 0.510819461629324,
      "grad_norm": 0.6390329599380493,
      "learning_rate": 4.1489407015170836e-05,
      "loss": 1.2764,
      "step": 163900
    },
    {
      "epoch": 0.5109752942899797,
      "grad_norm": 0.5926874279975891,
      "learning_rate": 4.14868098041599e-05,
      "loss": 1.3217,
      "step": 163950
    },
    {
      "epoch": 0.5111311269506353,
      "grad_norm": 0.5587507486343384,
      "learning_rate": 4.1484212593148975e-05,
      "loss": 1.2518,
      "step": 164000
    },
    {
      "epoch": 0.511286959611291,
      "grad_norm": 0.6821229457855225,
      "learning_rate": 4.148161538213805e-05,
      "loss": 1.2558,
      "step": 164050
    },
    {
      "epoch": 0.5114427922719467,
      "grad_norm": 0.5756598114967346,
      "learning_rate": 4.147901817112712e-05,
      "loss": 1.2287,
      "step": 164100
    },
    {
      "epoch": 0.5115986249326023,
      "grad_norm": 0.5985704660415649,
      "learning_rate": 4.147642096011619e-05,
      "loss": 1.261,
      "step": 164150
    },
    {
      "epoch": 0.5117544575932581,
      "grad_norm": 0.5024063587188721,
      "learning_rate": 4.1473823749105265e-05,
      "loss": 1.3218,
      "step": 164200
    },
    {
      "epoch": 0.5119102902539138,
      "grad_norm": 0.6152046322822571,
      "learning_rate": 4.147122653809433e-05,
      "loss": 1.2947,
      "step": 164250
    },
    {
      "epoch": 0.5120661229145694,
      "grad_norm": 0.7428183555603027,
      "learning_rate": 4.146868127130363e-05,
      "loss": 1.2848,
      "step": 164300
    },
    {
      "epoch": 0.5122219555752251,
      "grad_norm": 0.528507649898529,
      "learning_rate": 4.1466084060292695e-05,
      "loss": 1.2935,
      "step": 164350
    },
    {
      "epoch": 0.5123777882358808,
      "grad_norm": 0.6781556606292725,
      "learning_rate": 4.146348684928177e-05,
      "loss": 1.3085,
      "step": 164400
    },
    {
      "epoch": 0.5125336208965364,
      "grad_norm": 0.571526825428009,
      "learning_rate": 4.146088963827084e-05,
      "loss": 1.292,
      "step": 164450
    },
    {
      "epoch": 0.5126894535571922,
      "grad_norm": 0.5698826909065247,
      "learning_rate": 4.145829242725991e-05,
      "loss": 1.2275,
      "step": 164500
    },
    {
      "epoch": 0.5128452862178479,
      "grad_norm": 0.6730889081954956,
      "learning_rate": 4.1455695216248985e-05,
      "loss": 1.2541,
      "step": 164550
    },
    {
      "epoch": 0.5130011188785035,
      "grad_norm": 0.6388375759124756,
      "learning_rate": 4.145309800523806e-05,
      "loss": 1.2705,
      "step": 164600
    },
    {
      "epoch": 0.5131569515391592,
      "grad_norm": 0.5973089933395386,
      "learning_rate": 4.145050079422713e-05,
      "loss": 1.2533,
      "step": 164650
    },
    {
      "epoch": 0.5133127841998149,
      "grad_norm": 0.6185140013694763,
      "learning_rate": 4.14479035832162e-05,
      "loss": 1.2646,
      "step": 164700
    },
    {
      "epoch": 0.5134686168604705,
      "grad_norm": 0.721239447593689,
      "learning_rate": 4.1445306372205275e-05,
      "loss": 1.2975,
      "step": 164750
    },
    {
      "epoch": 0.5136244495211262,
      "grad_norm": 0.5734043121337891,
      "learning_rate": 4.144270916119434e-05,
      "loss": 1.2668,
      "step": 164800
    },
    {
      "epoch": 0.513780282181782,
      "grad_norm": 0.5793279409408569,
      "learning_rate": 4.144011195018342e-05,
      "loss": 1.2634,
      "step": 164850
    },
    {
      "epoch": 0.5139361148424376,
      "grad_norm": 0.5608803033828735,
      "learning_rate": 4.143751473917249e-05,
      "loss": 1.2542,
      "step": 164900
    },
    {
      "epoch": 0.5140919475030933,
      "grad_norm": 0.7357819676399231,
      "learning_rate": 4.143491752816156e-05,
      "loss": 1.2928,
      "step": 164950
    },
    {
      "epoch": 0.514247780163749,
      "grad_norm": 0.4830182194709778,
      "learning_rate": 4.143232031715063e-05,
      "loss": 1.2547,
      "step": 165000
    },
    {
      "epoch": 0.5144036128244046,
      "grad_norm": 0.5866001844406128,
      "learning_rate": 4.14297231061397e-05,
      "loss": 1.2324,
      "step": 165050
    },
    {
      "epoch": 0.5145594454850603,
      "grad_norm": 0.5470911860466003,
      "learning_rate": 4.1427125895128776e-05,
      "loss": 1.3093,
      "step": 165100
    },
    {
      "epoch": 0.514715278145716,
      "grad_norm": 0.7203946709632874,
      "learning_rate": 4.142452868411785e-05,
      "loss": 1.2519,
      "step": 165150
    },
    {
      "epoch": 0.5148711108063717,
      "grad_norm": 0.694197416305542,
      "learning_rate": 4.142193147310692e-05,
      "loss": 1.3437,
      "step": 165200
    },
    {
      "epoch": 0.5150269434670274,
      "grad_norm": 0.658517599105835,
      "learning_rate": 4.141933426209599e-05,
      "loss": 1.2628,
      "step": 165250
    },
    {
      "epoch": 0.515182776127683,
      "grad_norm": 0.5298516750335693,
      "learning_rate": 4.1416737051085066e-05,
      "loss": 1.2297,
      "step": 165300
    },
    {
      "epoch": 0.5153386087883387,
      "grad_norm": 0.5895594954490662,
      "learning_rate": 4.141413984007413e-05,
      "loss": 1.2481,
      "step": 165350
    },
    {
      "epoch": 0.5154944414489944,
      "grad_norm": 0.5714774131774902,
      "learning_rate": 4.141154262906321e-05,
      "loss": 1.2879,
      "step": 165400
    },
    {
      "epoch": 0.5156502741096501,
      "grad_norm": 0.6665360331535339,
      "learning_rate": 4.1408945418052283e-05,
      "loss": 1.2689,
      "step": 165450
    },
    {
      "epoch": 0.5158061067703058,
      "grad_norm": 0.6218756437301636,
      "learning_rate": 4.140634820704135e-05,
      "loss": 1.2928,
      "step": 165500
    },
    {
      "epoch": 0.5159619394309615,
      "grad_norm": 0.6031226515769958,
      "learning_rate": 4.140375099603043e-05,
      "loss": 1.2984,
      "step": 165550
    },
    {
      "epoch": 0.5161177720916171,
      "grad_norm": 0.7073838114738464,
      "learning_rate": 4.1401153785019494e-05,
      "loss": 1.3088,
      "step": 165600
    },
    {
      "epoch": 0.5162736047522728,
      "grad_norm": 0.47652798891067505,
      "learning_rate": 4.139855657400857e-05,
      "loss": 1.3213,
      "step": 165650
    },
    {
      "epoch": 0.5164294374129285,
      "grad_norm": 0.657321035861969,
      "learning_rate": 4.139595936299764e-05,
      "loss": 1.2941,
      "step": 165700
    },
    {
      "epoch": 0.5165852700735842,
      "grad_norm": 0.5272664427757263,
      "learning_rate": 4.139336215198671e-05,
      "loss": 1.3564,
      "step": 165750
    },
    {
      "epoch": 0.5167411027342399,
      "grad_norm": 0.718120276927948,
      "learning_rate": 4.1390764940975784e-05,
      "loss": 1.272,
      "step": 165800
    },
    {
      "epoch": 0.5168969353948956,
      "grad_norm": 0.6128067970275879,
      "learning_rate": 4.138816772996486e-05,
      "loss": 1.3063,
      "step": 165850
    },
    {
      "epoch": 0.5170527680555512,
      "grad_norm": 0.6125292181968689,
      "learning_rate": 4.138557051895393e-05,
      "loss": 1.2468,
      "step": 165900
    },
    {
      "epoch": 0.5172086007162069,
      "grad_norm": 0.7270992398262024,
      "learning_rate": 4.138302525216322e-05,
      "loss": 1.3319,
      "step": 165950
    },
    {
      "epoch": 0.5173644333768626,
      "grad_norm": 0.5578964948654175,
      "learning_rate": 4.1380428041152294e-05,
      "loss": 1.2879,
      "step": 166000
    },
    {
      "epoch": 0.5175202660375182,
      "grad_norm": 0.5442277193069458,
      "learning_rate": 4.137783083014136e-05,
      "loss": 1.2093,
      "step": 166050
    },
    {
      "epoch": 0.517676098698174,
      "grad_norm": 0.7688889503479004,
      "learning_rate": 4.137523361913043e-05,
      "loss": 1.242,
      "step": 166100
    },
    {
      "epoch": 0.5178319313588297,
      "grad_norm": 0.48583635687828064,
      "learning_rate": 4.137263640811951e-05,
      "loss": 1.275,
      "step": 166150
    },
    {
      "epoch": 0.5179877640194853,
      "grad_norm": 0.6507587432861328,
      "learning_rate": 4.137003919710858e-05,
      "loss": 1.2692,
      "step": 166200
    },
    {
      "epoch": 0.518143596680141,
      "grad_norm": 0.7351279854774475,
      "learning_rate": 4.136744198609765e-05,
      "loss": 1.2988,
      "step": 166250
    },
    {
      "epoch": 0.5182994293407966,
      "grad_norm": 0.5523515343666077,
      "learning_rate": 4.136484477508672e-05,
      "loss": 1.2516,
      "step": 166300
    },
    {
      "epoch": 0.5184552620014523,
      "grad_norm": 0.552539587020874,
      "learning_rate": 4.1362247564075795e-05,
      "loss": 1.3097,
      "step": 166350
    },
    {
      "epoch": 0.5186110946621081,
      "grad_norm": 0.6362524628639221,
      "learning_rate": 4.135965035306487e-05,
      "loss": 1.2929,
      "step": 166400
    },
    {
      "epoch": 0.5187669273227637,
      "grad_norm": 0.6736221313476562,
      "learning_rate": 4.135705314205394e-05,
      "loss": 1.2578,
      "step": 166450
    },
    {
      "epoch": 0.5189227599834194,
      "grad_norm": 0.46578145027160645,
      "learning_rate": 4.135445593104301e-05,
      "loss": 1.2784,
      "step": 166500
    },
    {
      "epoch": 0.5190785926440751,
      "grad_norm": 0.677216112613678,
      "learning_rate": 4.1351858720032085e-05,
      "loss": 1.2768,
      "step": 166550
    },
    {
      "epoch": 0.5192344253047307,
      "grad_norm": 0.5176962018013,
      "learning_rate": 4.134926150902115e-05,
      "loss": 1.3,
      "step": 166600
    },
    {
      "epoch": 0.5193902579653864,
      "grad_norm": 0.5837408900260925,
      "learning_rate": 4.134666429801023e-05,
      "loss": 1.2517,
      "step": 166650
    },
    {
      "epoch": 0.5195460906260422,
      "grad_norm": 0.6092562675476074,
      "learning_rate": 4.13440670869993e-05,
      "loss": 1.2663,
      "step": 166700
    },
    {
      "epoch": 0.5197019232866978,
      "grad_norm": 0.48578330874443054,
      "learning_rate": 4.134146987598837e-05,
      "loss": 1.274,
      "step": 166750
    },
    {
      "epoch": 0.5198577559473535,
      "grad_norm": 0.4692494869232178,
      "learning_rate": 4.133887266497744e-05,
      "loss": 1.2899,
      "step": 166800
    },
    {
      "epoch": 0.5200135886080092,
      "grad_norm": 0.7153960466384888,
      "learning_rate": 4.133627545396652e-05,
      "loss": 1.2506,
      "step": 166850
    },
    {
      "epoch": 0.5201694212686648,
      "grad_norm": 0.5184857249259949,
      "learning_rate": 4.1333678242955585e-05,
      "loss": 1.2408,
      "step": 166900
    },
    {
      "epoch": 0.5203252539293205,
      "grad_norm": 0.6317926645278931,
      "learning_rate": 4.133108103194466e-05,
      "loss": 1.3007,
      "step": 166950
    },
    {
      "epoch": 0.5204810865899763,
      "grad_norm": 0.6174653172492981,
      "learning_rate": 4.132848382093373e-05,
      "loss": 1.2629,
      "step": 167000
    },
    {
      "epoch": 0.5206369192506319,
      "grad_norm": 0.47697916626930237,
      "learning_rate": 4.13258866099228e-05,
      "loss": 1.2603,
      "step": 167050
    },
    {
      "epoch": 0.5207927519112876,
      "grad_norm": 0.5446128845214844,
      "learning_rate": 4.1323289398911875e-05,
      "loss": 1.2956,
      "step": 167100
    },
    {
      "epoch": 0.5209485845719433,
      "grad_norm": 0.5481482148170471,
      "learning_rate": 4.132069218790095e-05,
      "loss": 1.2844,
      "step": 167150
    },
    {
      "epoch": 0.5211044172325989,
      "grad_norm": 0.4313657283782959,
      "learning_rate": 4.131809497689002e-05,
      "loss": 1.2662,
      "step": 167200
    },
    {
      "epoch": 0.5212602498932546,
      "grad_norm": 0.7648991346359253,
      "learning_rate": 4.131549776587909e-05,
      "loss": 1.2952,
      "step": 167250
    },
    {
      "epoch": 0.5214160825539103,
      "grad_norm": 0.6576954126358032,
      "learning_rate": 4.131290055486816e-05,
      "loss": 1.3066,
      "step": 167300
    },
    {
      "epoch": 0.521571915214566,
      "grad_norm": 0.6692787408828735,
      "learning_rate": 4.131030334385723e-05,
      "loss": 1.2321,
      "step": 167350
    },
    {
      "epoch": 0.5217277478752217,
      "grad_norm": 0.6845353245735168,
      "learning_rate": 4.130770613284631e-05,
      "loss": 1.2768,
      "step": 167400
    },
    {
      "epoch": 0.5218835805358774,
      "grad_norm": 0.6252014636993408,
      "learning_rate": 4.1305108921835376e-05,
      "loss": 1.2741,
      "step": 167450
    },
    {
      "epoch": 0.522039413196533,
      "grad_norm": 0.6424674391746521,
      "learning_rate": 4.130251171082445e-05,
      "loss": 1.2626,
      "step": 167500
    },
    {
      "epoch": 0.5221952458571887,
      "grad_norm": 0.6931158900260925,
      "learning_rate": 4.129991449981353e-05,
      "loss": 1.2665,
      "step": 167550
    },
    {
      "epoch": 0.5223510785178443,
      "grad_norm": 0.4896167516708374,
      "learning_rate": 4.1297317288802594e-05,
      "loss": 1.2566,
      "step": 167600
    },
    {
      "epoch": 0.5225069111785001,
      "grad_norm": 0.554917573928833,
      "learning_rate": 4.1294720077791666e-05,
      "loss": 1.3176,
      "step": 167650
    },
    {
      "epoch": 0.5226627438391558,
      "grad_norm": 0.5102288722991943,
      "learning_rate": 4.129212286678074e-05,
      "loss": 1.246,
      "step": 167700
    },
    {
      "epoch": 0.5228185764998114,
      "grad_norm": 0.590537965297699,
      "learning_rate": 4.128952565576981e-05,
      "loss": 1.2736,
      "step": 167750
    },
    {
      "epoch": 0.5229744091604671,
      "grad_norm": 0.5537036061286926,
      "learning_rate": 4.1286928444758884e-05,
      "loss": 1.2803,
      "step": 167800
    },
    {
      "epoch": 0.5231302418211228,
      "grad_norm": 0.510501503944397,
      "learning_rate": 4.128433123374795e-05,
      "loss": 1.289,
      "step": 167850
    },
    {
      "epoch": 0.5232860744817784,
      "grad_norm": 0.5677136182785034,
      "learning_rate": 4.128173402273703e-05,
      "loss": 1.2624,
      "step": 167900
    },
    {
      "epoch": 0.5234419071424342,
      "grad_norm": 0.5636191964149475,
      "learning_rate": 4.12791368117261e-05,
      "loss": 1.2937,
      "step": 167950
    },
    {
      "epoch": 0.5235977398030899,
      "grad_norm": 0.5798371434211731,
      "learning_rate": 4.127653960071517e-05,
      "loss": 1.3492,
      "step": 168000
    },
    {
      "epoch": 0.5237535724637455,
      "grad_norm": 0.635289192199707,
      "learning_rate": 4.127394238970424e-05,
      "loss": 1.245,
      "step": 168050
    },
    {
      "epoch": 0.5239094051244012,
      "grad_norm": 0.6307967901229858,
      "learning_rate": 4.127134517869332e-05,
      "loss": 1.2965,
      "step": 168100
    },
    {
      "epoch": 0.5240652377850569,
      "grad_norm": 0.536463737487793,
      "learning_rate": 4.1268747967682384e-05,
      "loss": 1.278,
      "step": 168150
    },
    {
      "epoch": 0.5242210704457125,
      "grad_norm": 0.5606434941291809,
      "learning_rate": 4.126615075667146e-05,
      "loss": 1.2993,
      "step": 168200
    },
    {
      "epoch": 0.5243769031063683,
      "grad_norm": 0.5388838648796082,
      "learning_rate": 4.126355354566053e-05,
      "loss": 1.2387,
      "step": 168250
    },
    {
      "epoch": 0.524532735767024,
      "grad_norm": 0.5753293633460999,
      "learning_rate": 4.12609563346496e-05,
      "loss": 1.287,
      "step": 168300
    },
    {
      "epoch": 0.5246885684276796,
      "grad_norm": 0.6946455240249634,
      "learning_rate": 4.1258359123638674e-05,
      "loss": 1.2977,
      "step": 168350
    },
    {
      "epoch": 0.5248444010883353,
      "grad_norm": 0.6349738836288452,
      "learning_rate": 4.125576191262775e-05,
      "loss": 1.2493,
      "step": 168400
    },
    {
      "epoch": 0.525000233748991,
      "grad_norm": 0.5607850551605225,
      "learning_rate": 4.125316470161682e-05,
      "loss": 1.239,
      "step": 168450
    },
    {
      "epoch": 0.5251560664096466,
      "grad_norm": 0.6789815425872803,
      "learning_rate": 4.125056749060589e-05,
      "loss": 1.2881,
      "step": 168500
    },
    {
      "epoch": 0.5253118990703024,
      "grad_norm": 0.6088410019874573,
      "learning_rate": 4.124797027959496e-05,
      "loss": 1.2334,
      "step": 168550
    },
    {
      "epoch": 0.5254677317309581,
      "grad_norm": 0.5340653657913208,
      "learning_rate": 4.124537306858403e-05,
      "loss": 1.2795,
      "step": 168600
    },
    {
      "epoch": 0.5256235643916137,
      "grad_norm": 0.5939816236495972,
      "learning_rate": 4.124277585757311e-05,
      "loss": 1.2823,
      "step": 168650
    },
    {
      "epoch": 0.5257793970522694,
      "grad_norm": 0.5882434248924255,
      "learning_rate": 4.1240178646562175e-05,
      "loss": 1.3252,
      "step": 168700
    },
    {
      "epoch": 0.5259352297129251,
      "grad_norm": 0.5130549669265747,
      "learning_rate": 4.123758143555125e-05,
      "loss": 1.2969,
      "step": 168750
    },
    {
      "epoch": 0.5260910623735807,
      "grad_norm": 0.6405627727508545,
      "learning_rate": 4.123498422454033e-05,
      "loss": 1.2784,
      "step": 168800
    },
    {
      "epoch": 0.5262468950342364,
      "grad_norm": 0.6134641170501709,
      "learning_rate": 4.123238701352939e-05,
      "loss": 1.2768,
      "step": 168850
    },
    {
      "epoch": 0.5264027276948922,
      "grad_norm": 0.4907315969467163,
      "learning_rate": 4.1229789802518465e-05,
      "loss": 1.3093,
      "step": 168900
    },
    {
      "epoch": 0.5265585603555478,
      "grad_norm": 0.5261034965515137,
      "learning_rate": 4.122719259150754e-05,
      "loss": 1.2357,
      "step": 168950
    },
    {
      "epoch": 0.5267143930162035,
      "grad_norm": 0.6698751449584961,
      "learning_rate": 4.122459538049661e-05,
      "loss": 1.2989,
      "step": 169000
    },
    {
      "epoch": 0.5268702256768591,
      "grad_norm": 0.6378836035728455,
      "learning_rate": 4.122199816948568e-05,
      "loss": 1.2915,
      "step": 169050
    },
    {
      "epoch": 0.5270260583375148,
      "grad_norm": 0.6705827713012695,
      "learning_rate": 4.1219400958474755e-05,
      "loss": 1.2502,
      "step": 169100
    },
    {
      "epoch": 0.5271818909981705,
      "grad_norm": 0.5717329382896423,
      "learning_rate": 4.121680374746383e-05,
      "loss": 1.2713,
      "step": 169150
    },
    {
      "epoch": 0.5273377236588263,
      "grad_norm": 0.6741147637367249,
      "learning_rate": 4.12142065364529e-05,
      "loss": 1.3088,
      "step": 169200
    },
    {
      "epoch": 0.5274935563194819,
      "grad_norm": 0.4944573938846588,
      "learning_rate": 4.1211609325441966e-05,
      "loss": 1.247,
      "step": 169250
    },
    {
      "epoch": 0.5276493889801376,
      "grad_norm": 0.5735518336296082,
      "learning_rate": 4.120901211443104e-05,
      "loss": 1.2544,
      "step": 169300
    },
    {
      "epoch": 0.5278052216407932,
      "grad_norm": 0.49581509828567505,
      "learning_rate": 4.120641490342012e-05,
      "loss": 1.2536,
      "step": 169350
    },
    {
      "epoch": 0.5279610543014489,
      "grad_norm": 0.5224788188934326,
      "learning_rate": 4.120381769240918e-05,
      "loss": 1.2521,
      "step": 169400
    },
    {
      "epoch": 0.5281168869621046,
      "grad_norm": 0.6576712727546692,
      "learning_rate": 4.1201220481398256e-05,
      "loss": 1.2305,
      "step": 169450
    },
    {
      "epoch": 0.5282727196227603,
      "grad_norm": 0.5936416387557983,
      "learning_rate": 4.119862327038733e-05,
      "loss": 1.2822,
      "step": 169500
    },
    {
      "epoch": 0.528428552283416,
      "grad_norm": 0.6864123344421387,
      "learning_rate": 4.11960260593764e-05,
      "loss": 1.2397,
      "step": 169550
    },
    {
      "epoch": 0.5285843849440717,
      "grad_norm": 0.5955828428268433,
      "learning_rate": 4.119342884836547e-05,
      "loss": 1.2603,
      "step": 169600
    },
    {
      "epoch": 0.5287402176047273,
      "grad_norm": 0.5038768649101257,
      "learning_rate": 4.1190831637354546e-05,
      "loss": 1.2776,
      "step": 169650
    },
    {
      "epoch": 0.528896050265383,
      "grad_norm": 0.6684015393257141,
      "learning_rate": 4.118823442634362e-05,
      "loss": 1.2387,
      "step": 169700
    },
    {
      "epoch": 0.5290518829260387,
      "grad_norm": 0.7229540944099426,
      "learning_rate": 4.118563721533269e-05,
      "loss": 1.2657,
      "step": 169750
    },
    {
      "epoch": 0.5292077155866944,
      "grad_norm": 0.5757399797439575,
      "learning_rate": 4.118304000432176e-05,
      "loss": 1.3288,
      "step": 169800
    },
    {
      "epoch": 0.5293635482473501,
      "grad_norm": 0.6762748956680298,
      "learning_rate": 4.118044279331083e-05,
      "loss": 1.2558,
      "step": 169850
    },
    {
      "epoch": 0.5295193809080058,
      "grad_norm": 0.7595711350440979,
      "learning_rate": 4.117784558229991e-05,
      "loss": 1.3175,
      "step": 169900
    },
    {
      "epoch": 0.5296752135686614,
      "grad_norm": 0.542974591255188,
      "learning_rate": 4.1175248371288974e-05,
      "loss": 1.2993,
      "step": 169950
    },
    {
      "epoch": 0.5298310462293171,
      "grad_norm": 0.6051282286643982,
      "learning_rate": 4.1172651160278047e-05,
      "loss": 1.3302,
      "step": 170000
    },
    {
      "epoch": 0.5299868788899728,
      "grad_norm": 0.6216685175895691,
      "learning_rate": 4.1170053949267126e-05,
      "loss": 1.3275,
      "step": 170050
    },
    {
      "epoch": 0.5301427115506284,
      "grad_norm": 0.5558804273605347,
      "learning_rate": 4.116745673825619e-05,
      "loss": 1.2672,
      "step": 170100
    },
    {
      "epoch": 0.5302985442112842,
      "grad_norm": 0.6505751013755798,
      "learning_rate": 4.1164859527245264e-05,
      "loss": 1.3379,
      "step": 170150
    },
    {
      "epoch": 0.5304543768719399,
      "grad_norm": 0.7385095953941345,
      "learning_rate": 4.1162262316234337e-05,
      "loss": 1.3199,
      "step": 170200
    },
    {
      "epoch": 0.5306102095325955,
      "grad_norm": 0.7891319990158081,
      "learning_rate": 4.115966510522341e-05,
      "loss": 1.1788,
      "step": 170250
    },
    {
      "epoch": 0.5307660421932512,
      "grad_norm": 0.56773841381073,
      "learning_rate": 4.115706789421248e-05,
      "loss": 1.2766,
      "step": 170300
    },
    {
      "epoch": 0.5309218748539068,
      "grad_norm": 0.6511629819869995,
      "learning_rate": 4.1154470683201554e-05,
      "loss": 1.2891,
      "step": 170350
    },
    {
      "epoch": 0.5310777075145625,
      "grad_norm": 0.6079143285751343,
      "learning_rate": 4.1151873472190627e-05,
      "loss": 1.3103,
      "step": 170400
    },
    {
      "epoch": 0.5312335401752183,
      "grad_norm": 0.5453903675079346,
      "learning_rate": 4.11492762611797e-05,
      "loss": 1.2834,
      "step": 170450
    },
    {
      "epoch": 0.531389372835874,
      "grad_norm": 0.6206753849983215,
      "learning_rate": 4.114667905016877e-05,
      "loss": 1.287,
      "step": 170500
    },
    {
      "epoch": 0.5315452054965296,
      "grad_norm": 0.540416955947876,
      "learning_rate": 4.114408183915784e-05,
      "loss": 1.2695,
      "step": 170550
    },
    {
      "epoch": 0.5317010381571853,
      "grad_norm": 0.5392813086509705,
      "learning_rate": 4.114148462814692e-05,
      "loss": 1.229,
      "step": 170600
    },
    {
      "epoch": 0.5318568708178409,
      "grad_norm": 0.587963342666626,
      "learning_rate": 4.113888741713598e-05,
      "loss": 1.2583,
      "step": 170650
    },
    {
      "epoch": 0.5320127034784966,
      "grad_norm": 0.5195573568344116,
      "learning_rate": 4.1136290206125055e-05,
      "loss": 1.2936,
      "step": 170700
    },
    {
      "epoch": 0.5321685361391524,
      "grad_norm": 0.4953761398792267,
      "learning_rate": 4.113369299511413e-05,
      "loss": 1.2539,
      "step": 170750
    },
    {
      "epoch": 0.532324368799808,
      "grad_norm": 0.6383789777755737,
      "learning_rate": 4.11310957841032e-05,
      "loss": 1.2683,
      "step": 170800
    },
    {
      "epoch": 0.5324802014604637,
      "grad_norm": 0.5797767639160156,
      "learning_rate": 4.112849857309227e-05,
      "loss": 1.2668,
      "step": 170850
    },
    {
      "epoch": 0.5326360341211194,
      "grad_norm": 0.5448669195175171,
      "learning_rate": 4.1125953306301564e-05,
      "loss": 1.2805,
      "step": 170900
    },
    {
      "epoch": 0.532791866781775,
      "grad_norm": 0.6995815634727478,
      "learning_rate": 4.112335609529063e-05,
      "loss": 1.3088,
      "step": 170950
    },
    {
      "epoch": 0.5329476994424307,
      "grad_norm": 0.663447916507721,
      "learning_rate": 4.112075888427971e-05,
      "loss": 1.2293,
      "step": 171000
    },
    {
      "epoch": 0.5331035321030865,
      "grad_norm": 0.6455222368240356,
      "learning_rate": 4.111816167326878e-05,
      "loss": 1.2546,
      "step": 171050
    },
    {
      "epoch": 0.5332593647637421,
      "grad_norm": 0.5853185057640076,
      "learning_rate": 4.111556446225785e-05,
      "loss": 1.2718,
      "step": 171100
    },
    {
      "epoch": 0.5334151974243978,
      "grad_norm": 0.5080844759941101,
      "learning_rate": 4.111296725124693e-05,
      "loss": 1.2646,
      "step": 171150
    },
    {
      "epoch": 0.5335710300850535,
      "grad_norm": 0.7935272455215454,
      "learning_rate": 4.111037004023599e-05,
      "loss": 1.2321,
      "step": 171200
    },
    {
      "epoch": 0.5337268627457091,
      "grad_norm": 0.47332289814949036,
      "learning_rate": 4.1107772829225065e-05,
      "loss": 1.3027,
      "step": 171250
    },
    {
      "epoch": 0.5338826954063648,
      "grad_norm": 0.6131160259246826,
      "learning_rate": 4.110517561821414e-05,
      "loss": 1.2533,
      "step": 171300
    },
    {
      "epoch": 0.5340385280670205,
      "grad_norm": 0.5246301293373108,
      "learning_rate": 4.110257840720321e-05,
      "loss": 1.2885,
      "step": 171350
    },
    {
      "epoch": 0.5341943607276762,
      "grad_norm": 0.7368618845939636,
      "learning_rate": 4.109998119619228e-05,
      "loss": 1.2936,
      "step": 171400
    },
    {
      "epoch": 0.5343501933883319,
      "grad_norm": 0.48496925830841064,
      "learning_rate": 4.1097383985181355e-05,
      "loss": 1.2752,
      "step": 171450
    },
    {
      "epoch": 0.5345060260489876,
      "grad_norm": 0.6752015352249146,
      "learning_rate": 4.109478677417043e-05,
      "loss": 1.2541,
      "step": 171500
    },
    {
      "epoch": 0.5346618587096432,
      "grad_norm": 0.564626157283783,
      "learning_rate": 4.10921895631595e-05,
      "loss": 1.28,
      "step": 171550
    },
    {
      "epoch": 0.5348176913702989,
      "grad_norm": 0.5951848030090332,
      "learning_rate": 4.108959235214857e-05,
      "loss": 1.258,
      "step": 171600
    },
    {
      "epoch": 0.5349735240309546,
      "grad_norm": 0.622573733329773,
      "learning_rate": 4.108699514113764e-05,
      "loss": 1.3082,
      "step": 171650
    },
    {
      "epoch": 0.5351293566916103,
      "grad_norm": 0.6158699989318848,
      "learning_rate": 4.108439793012672e-05,
      "loss": 1.3073,
      "step": 171700
    },
    {
      "epoch": 0.535285189352266,
      "grad_norm": 0.5523108839988708,
      "learning_rate": 4.108180071911579e-05,
      "loss": 1.2788,
      "step": 171750
    },
    {
      "epoch": 0.5354410220129217,
      "grad_norm": 0.5180138945579529,
      "learning_rate": 4.1079203508104856e-05,
      "loss": 1.2613,
      "step": 171800
    },
    {
      "epoch": 0.5355968546735773,
      "grad_norm": 0.564331591129303,
      "learning_rate": 4.107660629709393e-05,
      "loss": 1.2474,
      "step": 171850
    },
    {
      "epoch": 0.535752687334233,
      "grad_norm": 0.6019526720046997,
      "learning_rate": 4.1074009086083e-05,
      "loss": 1.298,
      "step": 171900
    },
    {
      "epoch": 0.5359085199948886,
      "grad_norm": 0.5449563264846802,
      "learning_rate": 4.1071411875072074e-05,
      "loss": 1.2304,
      "step": 171950
    },
    {
      "epoch": 0.5360643526555444,
      "grad_norm": 0.7296083569526672,
      "learning_rate": 4.1068814664061146e-05,
      "loss": 1.2729,
      "step": 172000
    },
    {
      "epoch": 0.5362201853162001,
      "grad_norm": 0.5690126419067383,
      "learning_rate": 4.106621745305022e-05,
      "loss": 1.2363,
      "step": 172050
    },
    {
      "epoch": 0.5363760179768557,
      "grad_norm": 0.5507079362869263,
      "learning_rate": 4.106362024203929e-05,
      "loss": 1.2567,
      "step": 172100
    },
    {
      "epoch": 0.5365318506375114,
      "grad_norm": 0.8923865556716919,
      "learning_rate": 4.1061023031028364e-05,
      "loss": 1.2745,
      "step": 172150
    },
    {
      "epoch": 0.5366876832981671,
      "grad_norm": 0.5323068499565125,
      "learning_rate": 4.105842582001743e-05,
      "loss": 1.2933,
      "step": 172200
    },
    {
      "epoch": 0.5368435159588227,
      "grad_norm": 0.6599377989768982,
      "learning_rate": 4.105582860900651e-05,
      "loss": 1.2697,
      "step": 172250
    },
    {
      "epoch": 0.5369993486194785,
      "grad_norm": 0.6787304282188416,
      "learning_rate": 4.105323139799558e-05,
      "loss": 1.2586,
      "step": 172300
    },
    {
      "epoch": 0.5371551812801342,
      "grad_norm": 0.5673080086708069,
      "learning_rate": 4.105063418698465e-05,
      "loss": 1.3011,
      "step": 172350
    },
    {
      "epoch": 0.5373110139407898,
      "grad_norm": 0.6238276958465576,
      "learning_rate": 4.1048036975973726e-05,
      "loss": 1.2811,
      "step": 172400
    },
    {
      "epoch": 0.5374668466014455,
      "grad_norm": 0.5473044514656067,
      "learning_rate": 4.10454397649628e-05,
      "loss": 1.3123,
      "step": 172450
    },
    {
      "epoch": 0.5376226792621012,
      "grad_norm": 0.5921933054924011,
      "learning_rate": 4.1042842553951864e-05,
      "loss": 1.2914,
      "step": 172500
    },
    {
      "epoch": 0.5377785119227568,
      "grad_norm": 0.6484910249710083,
      "learning_rate": 4.104024534294094e-05,
      "loss": 1.2366,
      "step": 172550
    },
    {
      "epoch": 0.5379343445834125,
      "grad_norm": 0.6576825380325317,
      "learning_rate": 4.103764813193001e-05,
      "loss": 1.2389,
      "step": 172600
    },
    {
      "epoch": 0.5380901772440683,
      "grad_norm": 0.5033169984817505,
      "learning_rate": 4.103505092091908e-05,
      "loss": 1.2768,
      "step": 172650
    },
    {
      "epoch": 0.5382460099047239,
      "grad_norm": 0.5306479930877686,
      "learning_rate": 4.1032453709908154e-05,
      "loss": 1.2428,
      "step": 172700
    },
    {
      "epoch": 0.5384018425653796,
      "grad_norm": 0.6404086947441101,
      "learning_rate": 4.102985649889723e-05,
      "loss": 1.349,
      "step": 172750
    },
    {
      "epoch": 0.5385576752260353,
      "grad_norm": 0.6048862338066101,
      "learning_rate": 4.10272592878863e-05,
      "loss": 1.2552,
      "step": 172800
    },
    {
      "epoch": 0.5387135078866909,
      "grad_norm": 0.578732430934906,
      "learning_rate": 4.102466207687537e-05,
      "loss": 1.2961,
      "step": 172850
    },
    {
      "epoch": 0.5388693405473466,
      "grad_norm": 0.6631808280944824,
      "learning_rate": 4.102206486586444e-05,
      "loss": 1.3031,
      "step": 172900
    },
    {
      "epoch": 0.5390251732080024,
      "grad_norm": 0.45374390482902527,
      "learning_rate": 4.101946765485352e-05,
      "loss": 1.2794,
      "step": 172950
    },
    {
      "epoch": 0.539181005868658,
      "grad_norm": 0.7390823364257812,
      "learning_rate": 4.101687044384259e-05,
      "loss": 1.306,
      "step": 173000
    },
    {
      "epoch": 0.5393368385293137,
      "grad_norm": 0.5414720177650452,
      "learning_rate": 4.1014273232831655e-05,
      "loss": 1.2746,
      "step": 173050
    },
    {
      "epoch": 0.5394926711899694,
      "grad_norm": 0.6270699501037598,
      "learning_rate": 4.101167602182073e-05,
      "loss": 1.291,
      "step": 173100
    },
    {
      "epoch": 0.539648503850625,
      "grad_norm": 0.7518165111541748,
      "learning_rate": 4.10090788108098e-05,
      "loss": 1.2924,
      "step": 173150
    },
    {
      "epoch": 0.5398043365112807,
      "grad_norm": 0.5939826369285583,
      "learning_rate": 4.100648159979887e-05,
      "loss": 1.2763,
      "step": 173200
    },
    {
      "epoch": 0.5399601691719365,
      "grad_norm": 0.5897543430328369,
      "learning_rate": 4.1003884388787945e-05,
      "loss": 1.3104,
      "step": 173250
    },
    {
      "epoch": 0.5401160018325921,
      "grad_norm": 0.5911385416984558,
      "learning_rate": 4.100128717777702e-05,
      "loss": 1.2588,
      "step": 173300
    },
    {
      "epoch": 0.5402718344932478,
      "grad_norm": 0.5795450210571289,
      "learning_rate": 4.099868996676609e-05,
      "loss": 1.2609,
      "step": 173350
    },
    {
      "epoch": 0.5404276671539034,
      "grad_norm": 0.6127112507820129,
      "learning_rate": 4.099609275575516e-05,
      "loss": 1.2303,
      "step": 173400
    },
    {
      "epoch": 0.5405834998145591,
      "grad_norm": 0.5779236555099487,
      "learning_rate": 4.099349554474423e-05,
      "loss": 1.2939,
      "step": 173450
    },
    {
      "epoch": 0.5407393324752148,
      "grad_norm": 0.662371814250946,
      "learning_rate": 4.099089833373331e-05,
      "loss": 1.3079,
      "step": 173500
    },
    {
      "epoch": 0.5408951651358705,
      "grad_norm": 0.6047078967094421,
      "learning_rate": 4.098830112272238e-05,
      "loss": 1.2669,
      "step": 173550
    },
    {
      "epoch": 0.5410509977965262,
      "grad_norm": 0.4750766456127167,
      "learning_rate": 4.0985703911711446e-05,
      "loss": 1.2757,
      "step": 173600
    },
    {
      "epoch": 0.5412068304571819,
      "grad_norm": 0.5140831470489502,
      "learning_rate": 4.0983106700700525e-05,
      "loss": 1.3097,
      "step": 173650
    },
    {
      "epoch": 0.5413626631178375,
      "grad_norm": 0.5859358906745911,
      "learning_rate": 4.09805094896896e-05,
      "loss": 1.3503,
      "step": 173700
    },
    {
      "epoch": 0.5415184957784932,
      "grad_norm": 0.5339497923851013,
      "learning_rate": 4.097791227867866e-05,
      "loss": 1.3077,
      "step": 173750
    },
    {
      "epoch": 0.5416743284391489,
      "grad_norm": 0.625855028629303,
      "learning_rate": 4.0975315067667736e-05,
      "loss": 1.298,
      "step": 173800
    },
    {
      "epoch": 0.5418301610998045,
      "grad_norm": 0.6461637020111084,
      "learning_rate": 4.097271785665681e-05,
      "loss": 1.2854,
      "step": 173850
    },
    {
      "epoch": 0.5419859937604603,
      "grad_norm": 0.5039582848548889,
      "learning_rate": 4.097012064564588e-05,
      "loss": 1.2469,
      "step": 173900
    },
    {
      "epoch": 0.542141826421116,
      "grad_norm": 0.6699569225311279,
      "learning_rate": 4.096752343463495e-05,
      "loss": 1.2478,
      "step": 173950
    },
    {
      "epoch": 0.5422976590817716,
      "grad_norm": 0.5747435092926025,
      "learning_rate": 4.0964926223624026e-05,
      "loss": 1.2711,
      "step": 174000
    },
    {
      "epoch": 0.5424534917424273,
      "grad_norm": 0.47627103328704834,
      "learning_rate": 4.09623290126131e-05,
      "loss": 1.2566,
      "step": 174050
    },
    {
      "epoch": 0.542609324403083,
      "grad_norm": 0.56363445520401,
      "learning_rate": 4.095973180160217e-05,
      "loss": 1.2602,
      "step": 174100
    },
    {
      "epoch": 0.5427651570637386,
      "grad_norm": 0.5369840264320374,
      "learning_rate": 4.0957186534811456e-05,
      "loss": 1.2665,
      "step": 174150
    },
    {
      "epoch": 0.5429209897243944,
      "grad_norm": 0.5496580600738525,
      "learning_rate": 4.095458932380053e-05,
      "loss": 1.2241,
      "step": 174200
    },
    {
      "epoch": 0.5430768223850501,
      "grad_norm": 0.6152626872062683,
      "learning_rate": 4.095199211278961e-05,
      "loss": 1.2411,
      "step": 174250
    },
    {
      "epoch": 0.5432326550457057,
      "grad_norm": 0.5437959432601929,
      "learning_rate": 4.0949394901778674e-05,
      "loss": 1.3166,
      "step": 174300
    },
    {
      "epoch": 0.5433884877063614,
      "grad_norm": 0.5399641990661621,
      "learning_rate": 4.0946797690767746e-05,
      "loss": 1.2659,
      "step": 174350
    },
    {
      "epoch": 0.543544320367017,
      "grad_norm": 0.4515671730041504,
      "learning_rate": 4.0944200479756825e-05,
      "loss": 1.2521,
      "step": 174400
    },
    {
      "epoch": 0.5437001530276727,
      "grad_norm": 0.673549234867096,
      "learning_rate": 4.094160326874589e-05,
      "loss": 1.2923,
      "step": 174450
    },
    {
      "epoch": 0.5438559856883285,
      "grad_norm": 0.5939921140670776,
      "learning_rate": 4.0939006057734964e-05,
      "loss": 1.3008,
      "step": 174500
    },
    {
      "epoch": 0.5440118183489842,
      "grad_norm": 0.4799743592739105,
      "learning_rate": 4.0936408846724036e-05,
      "loss": 1.2244,
      "step": 174550
    },
    {
      "epoch": 0.5441676510096398,
      "grad_norm": 0.5063397288322449,
      "learning_rate": 4.093381163571311e-05,
      "loss": 1.2605,
      "step": 174600
    },
    {
      "epoch": 0.5443234836702955,
      "grad_norm": 0.6195188164710999,
      "learning_rate": 4.093121442470218e-05,
      "loss": 1.2936,
      "step": 174650
    },
    {
      "epoch": 0.5444793163309511,
      "grad_norm": 0.516301691532135,
      "learning_rate": 4.0928617213691254e-05,
      "loss": 1.2583,
      "step": 174700
    },
    {
      "epoch": 0.5446351489916068,
      "grad_norm": 0.6033408045768738,
      "learning_rate": 4.0926020002680326e-05,
      "loss": 1.2566,
      "step": 174750
    },
    {
      "epoch": 0.5447909816522626,
      "grad_norm": 0.5322497487068176,
      "learning_rate": 4.09234227916694e-05,
      "loss": 1.2364,
      "step": 174800
    },
    {
      "epoch": 0.5449468143129182,
      "grad_norm": 0.6106345653533936,
      "learning_rate": 4.0920825580658464e-05,
      "loss": 1.2412,
      "step": 174850
    },
    {
      "epoch": 0.5451026469735739,
      "grad_norm": 0.6567584276199341,
      "learning_rate": 4.091822836964754e-05,
      "loss": 1.2273,
      "step": 174900
    },
    {
      "epoch": 0.5452584796342296,
      "grad_norm": 0.4736187756061554,
      "learning_rate": 4.0915631158636616e-05,
      "loss": 1.2611,
      "step": 174950
    },
    {
      "epoch": 0.5454143122948852,
      "grad_norm": 0.5834989547729492,
      "learning_rate": 4.091303394762568e-05,
      "loss": 1.2922,
      "step": 175000
    },
    {
      "epoch": 0.5455701449555409,
      "grad_norm": 0.6445411443710327,
      "learning_rate": 4.0910436736614754e-05,
      "loss": 1.2779,
      "step": 175050
    },
    {
      "epoch": 0.5457259776161967,
      "grad_norm": 0.5959978699684143,
      "learning_rate": 4.090783952560383e-05,
      "loss": 1.3214,
      "step": 175100
    },
    {
      "epoch": 0.5458818102768523,
      "grad_norm": 0.4829780161380768,
      "learning_rate": 4.09052423145929e-05,
      "loss": 1.2471,
      "step": 175150
    },
    {
      "epoch": 0.546037642937508,
      "grad_norm": 0.765328586101532,
      "learning_rate": 4.090264510358197e-05,
      "loss": 1.2822,
      "step": 175200
    },
    {
      "epoch": 0.5461934755981637,
      "grad_norm": 0.622166633605957,
      "learning_rate": 4.0900047892571044e-05,
      "loss": 1.2298,
      "step": 175250
    },
    {
      "epoch": 0.5463493082588193,
      "grad_norm": 0.5805361270904541,
      "learning_rate": 4.089745068156012e-05,
      "loss": 1.257,
      "step": 175300
    },
    {
      "epoch": 0.546505140919475,
      "grad_norm": 0.5318669080734253,
      "learning_rate": 4.089485347054919e-05,
      "loss": 1.2822,
      "step": 175350
    },
    {
      "epoch": 0.5466609735801307,
      "grad_norm": 0.6136809587478638,
      "learning_rate": 4.0892256259538255e-05,
      "loss": 1.2481,
      "step": 175400
    },
    {
      "epoch": 0.5468168062407864,
      "grad_norm": 0.5910487174987793,
      "learning_rate": 4.088965904852733e-05,
      "loss": 1.2702,
      "step": 175450
    },
    {
      "epoch": 0.5469726389014421,
      "grad_norm": 0.59513258934021,
      "learning_rate": 4.088706183751641e-05,
      "loss": 1.2644,
      "step": 175500
    },
    {
      "epoch": 0.5471284715620978,
      "grad_norm": 0.6807076930999756,
      "learning_rate": 4.088446462650547e-05,
      "loss": 1.3673,
      "step": 175550
    },
    {
      "epoch": 0.5472843042227534,
      "grad_norm": 0.5656909942626953,
      "learning_rate": 4.0881867415494545e-05,
      "loss": 1.2885,
      "step": 175600
    },
    {
      "epoch": 0.5474401368834091,
      "grad_norm": 0.6127826571464539,
      "learning_rate": 4.0879270204483624e-05,
      "loss": 1.2801,
      "step": 175650
    },
    {
      "epoch": 0.5475959695440648,
      "grad_norm": 0.7611042857170105,
      "learning_rate": 4.087667299347269e-05,
      "loss": 1.2989,
      "step": 175700
    },
    {
      "epoch": 0.5477518022047205,
      "grad_norm": 0.61456698179245,
      "learning_rate": 4.087407578246176e-05,
      "loss": 1.2953,
      "step": 175750
    },
    {
      "epoch": 0.5479076348653762,
      "grad_norm": 0.5833728313446045,
      "learning_rate": 4.0871478571450835e-05,
      "loss": 1.2789,
      "step": 175800
    },
    {
      "epoch": 0.5480634675260319,
      "grad_norm": 0.5156880021095276,
      "learning_rate": 4.086888136043991e-05,
      "loss": 1.2946,
      "step": 175850
    },
    {
      "epoch": 0.5482193001866875,
      "grad_norm": 0.6841842532157898,
      "learning_rate": 4.086628414942898e-05,
      "loss": 1.3049,
      "step": 175900
    },
    {
      "epoch": 0.5483751328473432,
      "grad_norm": 0.6870470643043518,
      "learning_rate": 4.086368693841805e-05,
      "loss": 1.3044,
      "step": 175950
    },
    {
      "epoch": 0.5485309655079988,
      "grad_norm": 0.5975657105445862,
      "learning_rate": 4.0861089727407125e-05,
      "loss": 1.2873,
      "step": 176000
    },
    {
      "epoch": 0.5486867981686546,
      "grad_norm": 0.40677163004875183,
      "learning_rate": 4.08584925163962e-05,
      "loss": 1.2737,
      "step": 176050
    },
    {
      "epoch": 0.5488426308293103,
      "grad_norm": 0.5652812123298645,
      "learning_rate": 4.0855895305385263e-05,
      "loss": 1.266,
      "step": 176100
    },
    {
      "epoch": 0.548998463489966,
      "grad_norm": 0.7117477655410767,
      "learning_rate": 4.0853350038594556e-05,
      "loss": 1.2817,
      "step": 176150
    },
    {
      "epoch": 0.5491542961506216,
      "grad_norm": 0.6019474864006042,
      "learning_rate": 4.085075282758363e-05,
      "loss": 1.2326,
      "step": 176200
    },
    {
      "epoch": 0.5493101288112773,
      "grad_norm": 0.5338082313537598,
      "learning_rate": 4.08481556165727e-05,
      "loss": 1.2399,
      "step": 176250
    },
    {
      "epoch": 0.5494659614719329,
      "grad_norm": 0.5781909823417664,
      "learning_rate": 4.084555840556177e-05,
      "loss": 1.2613,
      "step": 176300
    },
    {
      "epoch": 0.5496217941325887,
      "grad_norm": 0.5041137337684631,
      "learning_rate": 4.0842961194550846e-05,
      "loss": 1.2517,
      "step": 176350
    },
    {
      "epoch": 0.5497776267932444,
      "grad_norm": 0.5760359764099121,
      "learning_rate": 4.084036398353992e-05,
      "loss": 1.2589,
      "step": 176400
    },
    {
      "epoch": 0.5499334594539,
      "grad_norm": 0.8062566518783569,
      "learning_rate": 4.083776677252899e-05,
      "loss": 1.2011,
      "step": 176450
    },
    {
      "epoch": 0.5500892921145557,
      "grad_norm": 0.6183164119720459,
      "learning_rate": 4.083516956151806e-05,
      "loss": 1.2468,
      "step": 176500
    },
    {
      "epoch": 0.5502451247752114,
      "grad_norm": 0.5674371123313904,
      "learning_rate": 4.083257235050713e-05,
      "loss": 1.2479,
      "step": 176550
    },
    {
      "epoch": 0.550400957435867,
      "grad_norm": 0.6589226126670837,
      "learning_rate": 4.082997513949621e-05,
      "loss": 1.2804,
      "step": 176600
    },
    {
      "epoch": 0.5505567900965227,
      "grad_norm": 0.5946899652481079,
      "learning_rate": 4.082737792848528e-05,
      "loss": 1.2748,
      "step": 176650
    },
    {
      "epoch": 0.5507126227571785,
      "grad_norm": 0.5811276435852051,
      "learning_rate": 4.0824780717474346e-05,
      "loss": 1.2951,
      "step": 176700
    },
    {
      "epoch": 0.5508684554178341,
      "grad_norm": 0.832680881023407,
      "learning_rate": 4.0822183506463426e-05,
      "loss": 1.2853,
      "step": 176750
    },
    {
      "epoch": 0.5510242880784898,
      "grad_norm": 0.5295873284339905,
      "learning_rate": 4.081958629545249e-05,
      "loss": 1.2712,
      "step": 176800
    },
    {
      "epoch": 0.5511801207391455,
      "grad_norm": 0.4883500933647156,
      "learning_rate": 4.0816989084441564e-05,
      "loss": 1.267,
      "step": 176850
    },
    {
      "epoch": 0.5513359533998011,
      "grad_norm": 0.7471120357513428,
      "learning_rate": 4.0814391873430636e-05,
      "loss": 1.2726,
      "step": 176900
    },
    {
      "epoch": 0.5514917860604568,
      "grad_norm": 0.6622915863990784,
      "learning_rate": 4.081179466241971e-05,
      "loss": 1.3043,
      "step": 176950
    },
    {
      "epoch": 0.5516476187211126,
      "grad_norm": 0.5425915718078613,
      "learning_rate": 4.080919745140878e-05,
      "loss": 1.2444,
      "step": 177000
    },
    {
      "epoch": 0.5518034513817682,
      "grad_norm": 0.6209478378295898,
      "learning_rate": 4.0806600240397854e-05,
      "loss": 1.3113,
      "step": 177050
    },
    {
      "epoch": 0.5519592840424239,
      "grad_norm": 0.6689556241035461,
      "learning_rate": 4.080400302938692e-05,
      "loss": 1.2868,
      "step": 177100
    },
    {
      "epoch": 0.5521151167030796,
      "grad_norm": 0.5079534649848938,
      "learning_rate": 4.0801405818376e-05,
      "loss": 1.2684,
      "step": 177150
    },
    {
      "epoch": 0.5522709493637352,
      "grad_norm": 0.46262624859809875,
      "learning_rate": 4.079880860736507e-05,
      "loss": 1.2699,
      "step": 177200
    },
    {
      "epoch": 0.5524267820243909,
      "grad_norm": 0.5957790613174438,
      "learning_rate": 4.079621139635414e-05,
      "loss": 1.2705,
      "step": 177250
    },
    {
      "epoch": 0.5525826146850467,
      "grad_norm": 0.5408950448036194,
      "learning_rate": 4.0793614185343216e-05,
      "loss": 1.2679,
      "step": 177300
    },
    {
      "epoch": 0.5527384473457023,
      "grad_norm": 0.5200523138046265,
      "learning_rate": 4.079101697433229e-05,
      "loss": 1.2492,
      "step": 177350
    },
    {
      "epoch": 0.552894280006358,
      "grad_norm": 0.5333212614059448,
      "learning_rate": 4.0788419763321355e-05,
      "loss": 1.3021,
      "step": 177400
    },
    {
      "epoch": 0.5530501126670136,
      "grad_norm": 0.595081090927124,
      "learning_rate": 4.078582255231043e-05,
      "loss": 1.2247,
      "step": 177450
    },
    {
      "epoch": 0.5532059453276693,
      "grad_norm": 0.6159462928771973,
      "learning_rate": 4.07832253412995e-05,
      "loss": 1.3012,
      "step": 177500
    },
    {
      "epoch": 0.553361777988325,
      "grad_norm": 0.5413480401039124,
      "learning_rate": 4.078062813028857e-05,
      "loss": 1.2909,
      "step": 177550
    },
    {
      "epoch": 0.5535176106489808,
      "grad_norm": 0.6011275053024292,
      "learning_rate": 4.0778030919277645e-05,
      "loss": 1.2838,
      "step": 177600
    },
    {
      "epoch": 0.5536734433096364,
      "grad_norm": 0.6469451189041138,
      "learning_rate": 4.077543370826672e-05,
      "loss": 1.2463,
      "step": 177650
    },
    {
      "epoch": 0.5538292759702921,
      "grad_norm": 0.6626255512237549,
      "learning_rate": 4.077283649725579e-05,
      "loss": 1.2854,
      "step": 177700
    },
    {
      "epoch": 0.5539851086309477,
      "grad_norm": 0.5291823148727417,
      "learning_rate": 4.077023928624486e-05,
      "loss": 1.2659,
      "step": 177750
    },
    {
      "epoch": 0.5541409412916034,
      "grad_norm": 0.6690221428871155,
      "learning_rate": 4.076764207523393e-05,
      "loss": 1.256,
      "step": 177800
    },
    {
      "epoch": 0.5542967739522591,
      "grad_norm": 0.6615973711013794,
      "learning_rate": 4.076504486422301e-05,
      "loss": 1.267,
      "step": 177850
    },
    {
      "epoch": 0.5544526066129147,
      "grad_norm": 0.6402056217193604,
      "learning_rate": 4.076244765321208e-05,
      "loss": 1.2623,
      "step": 177900
    },
    {
      "epoch": 0.5546084392735705,
      "grad_norm": 0.6669626832008362,
      "learning_rate": 4.0759850442201145e-05,
      "loss": 1.2717,
      "step": 177950
    },
    {
      "epoch": 0.5547642719342262,
      "grad_norm": 0.6820542812347412,
      "learning_rate": 4.0757253231190225e-05,
      "loss": 1.2758,
      "step": 178000
    },
    {
      "epoch": 0.5549201045948818,
      "grad_norm": 0.6333132386207581,
      "learning_rate": 4.075465602017929e-05,
      "loss": 1.1741,
      "step": 178050
    },
    {
      "epoch": 0.5550759372555375,
      "grad_norm": 0.5222553014755249,
      "learning_rate": 4.075205880916836e-05,
      "loss": 1.2324,
      "step": 178100
    },
    {
      "epoch": 0.5552317699161932,
      "grad_norm": 0.5528483986854553,
      "learning_rate": 4.0749461598157435e-05,
      "loss": 1.2211,
      "step": 178150
    },
    {
      "epoch": 0.5553876025768488,
      "grad_norm": 0.6279786229133606,
      "learning_rate": 4.074686438714651e-05,
      "loss": 1.2964,
      "step": 178200
    },
    {
      "epoch": 0.5555434352375046,
      "grad_norm": 0.6180035471916199,
      "learning_rate": 4.074426717613558e-05,
      "loss": 1.2992,
      "step": 178250
    },
    {
      "epoch": 0.5556992678981603,
      "grad_norm": 0.4526415169239044,
      "learning_rate": 4.074166996512465e-05,
      "loss": 1.2945,
      "step": 178300
    },
    {
      "epoch": 0.5558551005588159,
      "grad_norm": 0.6582341194152832,
      "learning_rate": 4.073912469833394e-05,
      "loss": 1.2629,
      "step": 178350
    },
    {
      "epoch": 0.5560109332194716,
      "grad_norm": 0.5564736723899841,
      "learning_rate": 4.073652748732302e-05,
      "loss": 1.2686,
      "step": 178400
    },
    {
      "epoch": 0.5561667658801273,
      "grad_norm": 0.5970975756645203,
      "learning_rate": 4.073393027631209e-05,
      "loss": 1.309,
      "step": 178450
    },
    {
      "epoch": 0.5563225985407829,
      "grad_norm": 0.6329913139343262,
      "learning_rate": 4.0731333065301156e-05,
      "loss": 1.2797,
      "step": 178500
    },
    {
      "epoch": 0.5564784312014387,
      "grad_norm": 0.6111862659454346,
      "learning_rate": 4.072873585429023e-05,
      "loss": 1.2764,
      "step": 178550
    },
    {
      "epoch": 0.5566342638620944,
      "grad_norm": 0.5756760835647583,
      "learning_rate": 4.072613864327931e-05,
      "loss": 1.323,
      "step": 178600
    },
    {
      "epoch": 0.55679009652275,
      "grad_norm": 0.6942640542984009,
      "learning_rate": 4.072354143226837e-05,
      "loss": 1.2967,
      "step": 178650
    },
    {
      "epoch": 0.5569459291834057,
      "grad_norm": 0.6315968632698059,
      "learning_rate": 4.0720944221257446e-05,
      "loss": 1.2653,
      "step": 178700
    },
    {
      "epoch": 0.5571017618440613,
      "grad_norm": 0.5126413702964783,
      "learning_rate": 4.071834701024652e-05,
      "loss": 1.2793,
      "step": 178750
    },
    {
      "epoch": 0.557257594504717,
      "grad_norm": 0.6123092770576477,
      "learning_rate": 4.071574979923559e-05,
      "loss": 1.2717,
      "step": 178800
    },
    {
      "epoch": 0.5574134271653728,
      "grad_norm": 0.5482190251350403,
      "learning_rate": 4.071315258822466e-05,
      "loss": 1.2148,
      "step": 178850
    },
    {
      "epoch": 0.5575692598260285,
      "grad_norm": 0.6752222180366516,
      "learning_rate": 4.0710555377213736e-05,
      "loss": 1.2291,
      "step": 178900
    },
    {
      "epoch": 0.5577250924866841,
      "grad_norm": 0.4458039700984955,
      "learning_rate": 4.070795816620281e-05,
      "loss": 1.2213,
      "step": 178950
    },
    {
      "epoch": 0.5578809251473398,
      "grad_norm": 0.7929094433784485,
      "learning_rate": 4.070536095519188e-05,
      "loss": 1.2428,
      "step": 179000
    },
    {
      "epoch": 0.5580367578079954,
      "grad_norm": 0.591804563999176,
      "learning_rate": 4.0702763744180947e-05,
      "loss": 1.2497,
      "step": 179050
    },
    {
      "epoch": 0.5581925904686511,
      "grad_norm": 0.6722413301467896,
      "learning_rate": 4.070016653317002e-05,
      "loss": 1.3043,
      "step": 179100
    },
    {
      "epoch": 0.5583484231293068,
      "grad_norm": 0.4788733124732971,
      "learning_rate": 4.06975693221591e-05,
      "loss": 1.2829,
      "step": 179150
    },
    {
      "epoch": 0.5585042557899625,
      "grad_norm": 0.5714565515518188,
      "learning_rate": 4.0694972111148164e-05,
      "loss": 1.2616,
      "step": 179200
    },
    {
      "epoch": 0.5586600884506182,
      "grad_norm": 0.571962833404541,
      "learning_rate": 4.0692374900137237e-05,
      "loss": 1.271,
      "step": 179250
    },
    {
      "epoch": 0.5588159211112739,
      "grad_norm": 0.7160214781761169,
      "learning_rate": 4.0689777689126316e-05,
      "loss": 1.2497,
      "step": 179300
    },
    {
      "epoch": 0.5589717537719295,
      "grad_norm": 0.6492081880569458,
      "learning_rate": 4.068718047811538e-05,
      "loss": 1.2271,
      "step": 179350
    },
    {
      "epoch": 0.5591275864325852,
      "grad_norm": 0.562067985534668,
      "learning_rate": 4.0684583267104454e-05,
      "loss": 1.2431,
      "step": 179400
    },
    {
      "epoch": 0.5592834190932409,
      "grad_norm": 0.5651232004165649,
      "learning_rate": 4.0681986056093527e-05,
      "loss": 1.2338,
      "step": 179450
    },
    {
      "epoch": 0.5594392517538966,
      "grad_norm": 0.61620032787323,
      "learning_rate": 4.06793888450826e-05,
      "loss": 1.2976,
      "step": 179500
    },
    {
      "epoch": 0.5595950844145523,
      "grad_norm": 0.6805077195167542,
      "learning_rate": 4.067679163407167e-05,
      "loss": 1.2772,
      "step": 179550
    },
    {
      "epoch": 0.559750917075208,
      "grad_norm": 0.5997222661972046,
      "learning_rate": 4.0674194423060744e-05,
      "loss": 1.305,
      "step": 179600
    },
    {
      "epoch": 0.5599067497358636,
      "grad_norm": 0.7011411190032959,
      "learning_rate": 4.0671597212049817e-05,
      "loss": 1.2482,
      "step": 179650
    },
    {
      "epoch": 0.5600625823965193,
      "grad_norm": 0.46774813532829285,
      "learning_rate": 4.066900000103889e-05,
      "loss": 1.2873,
      "step": 179700
    },
    {
      "epoch": 0.560218415057175,
      "grad_norm": 0.5289072394371033,
      "learning_rate": 4.0666402790027955e-05,
      "loss": 1.2926,
      "step": 179750
    },
    {
      "epoch": 0.5603742477178307,
      "grad_norm": 0.5104095339775085,
      "learning_rate": 4.066380557901703e-05,
      "loss": 1.262,
      "step": 179800
    },
    {
      "epoch": 0.5605300803784864,
      "grad_norm": 0.5411126613616943,
      "learning_rate": 4.0661208368006107e-05,
      "loss": 1.2709,
      "step": 179850
    },
    {
      "epoch": 0.5606859130391421,
      "grad_norm": 0.5910367965698242,
      "learning_rate": 4.065861115699517e-05,
      "loss": 1.3024,
      "step": 179900
    },
    {
      "epoch": 0.5608417456997977,
      "grad_norm": 0.732734203338623,
      "learning_rate": 4.0656013945984245e-05,
      "loss": 1.348,
      "step": 179950
    },
    {
      "epoch": 0.5609975783604534,
      "grad_norm": 0.5737102031707764,
      "learning_rate": 4.0653416734973324e-05,
      "loss": 1.292,
      "step": 180000
    },
    {
      "epoch": 0.561153411021109,
      "grad_norm": 0.5838018655776978,
      "learning_rate": 4.065081952396239e-05,
      "loss": 1.2678,
      "step": 180050
    },
    {
      "epoch": 0.5613092436817648,
      "grad_norm": 0.5390734076499939,
      "learning_rate": 4.064822231295146e-05,
      "loss": 1.267,
      "step": 180100
    },
    {
      "epoch": 0.5614650763424205,
      "grad_norm": 0.5479850769042969,
      "learning_rate": 4.0645625101940535e-05,
      "loss": 1.3057,
      "step": 180150
    },
    {
      "epoch": 0.5616209090030762,
      "grad_norm": 0.5703291296958923,
      "learning_rate": 4.064302789092961e-05,
      "loss": 1.2937,
      "step": 180200
    },
    {
      "epoch": 0.5617767416637318,
      "grad_norm": 0.5735237002372742,
      "learning_rate": 4.064043067991868e-05,
      "loss": 1.2565,
      "step": 180250
    },
    {
      "epoch": 0.5619325743243875,
      "grad_norm": 0.6156452894210815,
      "learning_rate": 4.0637833468907746e-05,
      "loss": 1.2469,
      "step": 180300
    },
    {
      "epoch": 0.5620884069850431,
      "grad_norm": 0.5310657620429993,
      "learning_rate": 4.063523625789682e-05,
      "loss": 1.3034,
      "step": 180350
    },
    {
      "epoch": 0.5622442396456989,
      "grad_norm": 0.6023800373077393,
      "learning_rate": 4.063269099110612e-05,
      "loss": 1.3158,
      "step": 180400
    },
    {
      "epoch": 0.5624000723063546,
      "grad_norm": 0.5529705882072449,
      "learning_rate": 4.063009378009518e-05,
      "loss": 1.2693,
      "step": 180450
    },
    {
      "epoch": 0.5625559049670102,
      "grad_norm": 0.5254725813865662,
      "learning_rate": 4.0627496569084255e-05,
      "loss": 1.2968,
      "step": 180500
    },
    {
      "epoch": 0.5627117376276659,
      "grad_norm": 0.5240069627761841,
      "learning_rate": 4.062489935807333e-05,
      "loss": 1.261,
      "step": 180550
    },
    {
      "epoch": 0.5628675702883216,
      "grad_norm": 0.5929487943649292,
      "learning_rate": 4.06223021470624e-05,
      "loss": 1.2545,
      "step": 180600
    },
    {
      "epoch": 0.5630234029489772,
      "grad_norm": 0.6573295593261719,
      "learning_rate": 4.061970493605147e-05,
      "loss": 1.2786,
      "step": 180650
    },
    {
      "epoch": 0.5631792356096329,
      "grad_norm": 0.5742493271827698,
      "learning_rate": 4.0617107725040545e-05,
      "loss": 1.3042,
      "step": 180700
    },
    {
      "epoch": 0.5633350682702887,
      "grad_norm": 0.6176324486732483,
      "learning_rate": 4.061451051402962e-05,
      "loss": 1.2378,
      "step": 180750
    },
    {
      "epoch": 0.5634909009309443,
      "grad_norm": 0.6091394424438477,
      "learning_rate": 4.061191330301869e-05,
      "loss": 1.2448,
      "step": 180800
    },
    {
      "epoch": 0.5636467335916,
      "grad_norm": 0.6489340662956238,
      "learning_rate": 4.060931609200776e-05,
      "loss": 1.1902,
      "step": 180850
    },
    {
      "epoch": 0.5638025662522557,
      "grad_norm": 0.6590124368667603,
      "learning_rate": 4.060671888099683e-05,
      "loss": 1.2363,
      "step": 180900
    },
    {
      "epoch": 0.5639583989129113,
      "grad_norm": 0.5429449081420898,
      "learning_rate": 4.060412166998591e-05,
      "loss": 1.3149,
      "step": 180950
    },
    {
      "epoch": 0.564114231573567,
      "grad_norm": 0.6634132266044617,
      "learning_rate": 4.0601524458974973e-05,
      "loss": 1.2755,
      "step": 181000
    },
    {
      "epoch": 0.5642700642342228,
      "grad_norm": 0.48343902826309204,
      "learning_rate": 4.0598927247964046e-05,
      "loss": 1.2888,
      "step": 181050
    },
    {
      "epoch": 0.5644258968948784,
      "grad_norm": 0.5582841038703918,
      "learning_rate": 4.0596330036953125e-05,
      "loss": 1.2593,
      "step": 181100
    },
    {
      "epoch": 0.5645817295555341,
      "grad_norm": 0.5561085343360901,
      "learning_rate": 4.059373282594219e-05,
      "loss": 1.2885,
      "step": 181150
    },
    {
      "epoch": 0.5647375622161898,
      "grad_norm": 0.7157278656959534,
      "learning_rate": 4.0591135614931263e-05,
      "loss": 1.2492,
      "step": 181200
    },
    {
      "epoch": 0.5648933948768454,
      "grad_norm": 0.6179695129394531,
      "learning_rate": 4.0588538403920336e-05,
      "loss": 1.2898,
      "step": 181250
    },
    {
      "epoch": 0.5650492275375011,
      "grad_norm": 0.6473724842071533,
      "learning_rate": 4.058594119290941e-05,
      "loss": 1.3198,
      "step": 181300
    },
    {
      "epoch": 0.5652050601981569,
      "grad_norm": 0.7383731007575989,
      "learning_rate": 4.058334398189848e-05,
      "loss": 1.2188,
      "step": 181350
    },
    {
      "epoch": 0.5653608928588125,
      "grad_norm": 0.5021049976348877,
      "learning_rate": 4.0580746770887553e-05,
      "loss": 1.3313,
      "step": 181400
    },
    {
      "epoch": 0.5655167255194682,
      "grad_norm": 0.5287771224975586,
      "learning_rate": 4.057814955987662e-05,
      "loss": 1.3019,
      "step": 181450
    },
    {
      "epoch": 0.5656725581801239,
      "grad_norm": 0.5376695990562439,
      "learning_rate": 4.05755523488657e-05,
      "loss": 1.2617,
      "step": 181500
    },
    {
      "epoch": 0.5658283908407795,
      "grad_norm": 0.4607307016849518,
      "learning_rate": 4.057295513785477e-05,
      "loss": 1.285,
      "step": 181550
    },
    {
      "epoch": 0.5659842235014352,
      "grad_norm": 0.5392764806747437,
      "learning_rate": 4.057035792684384e-05,
      "loss": 1.2745,
      "step": 181600
    },
    {
      "epoch": 0.566140056162091,
      "grad_norm": 0.60886150598526,
      "learning_rate": 4.0567760715832916e-05,
      "loss": 1.2644,
      "step": 181650
    },
    {
      "epoch": 0.5662958888227466,
      "grad_norm": 0.5543106198310852,
      "learning_rate": 4.056516350482198e-05,
      "loss": 1.2106,
      "step": 181700
    },
    {
      "epoch": 0.5664517214834023,
      "grad_norm": 0.5962131023406982,
      "learning_rate": 4.0562566293811054e-05,
      "loss": 1.2718,
      "step": 181750
    },
    {
      "epoch": 0.566607554144058,
      "grad_norm": 0.5450668931007385,
      "learning_rate": 4.055996908280013e-05,
      "loss": 1.2849,
      "step": 181800
    },
    {
      "epoch": 0.5667633868047136,
      "grad_norm": 0.5656518340110779,
      "learning_rate": 4.05573718717892e-05,
      "loss": 1.255,
      "step": 181850
    },
    {
      "epoch": 0.5669192194653693,
      "grad_norm": 0.6741383671760559,
      "learning_rate": 4.055477466077827e-05,
      "loss": 1.2345,
      "step": 181900
    },
    {
      "epoch": 0.5670750521260249,
      "grad_norm": 0.5571132898330688,
      "learning_rate": 4.0552177449767344e-05,
      "loss": 1.2473,
      "step": 181950
    },
    {
      "epoch": 0.5672308847866807,
      "grad_norm": 0.7113386988639832,
      "learning_rate": 4.054958023875642e-05,
      "loss": 1.3165,
      "step": 182000
    },
    {
      "epoch": 0.5673867174473364,
      "grad_norm": 0.5587770938873291,
      "learning_rate": 4.054698302774549e-05,
      "loss": 1.2347,
      "step": 182050
    },
    {
      "epoch": 0.567542550107992,
      "grad_norm": 0.6505225300788879,
      "learning_rate": 4.054438581673456e-05,
      "loss": 1.322,
      "step": 182100
    },
    {
      "epoch": 0.5676983827686477,
      "grad_norm": 0.5281860828399658,
      "learning_rate": 4.054178860572363e-05,
      "loss": 1.2716,
      "step": 182150
    },
    {
      "epoch": 0.5678542154293034,
      "grad_norm": 0.5254988074302673,
      "learning_rate": 4.053919139471271e-05,
      "loss": 1.2882,
      "step": 182200
    },
    {
      "epoch": 0.568010048089959,
      "grad_norm": 0.633094310760498,
      "learning_rate": 4.053659418370178e-05,
      "loss": 1.2833,
      "step": 182250
    },
    {
      "epoch": 0.5681658807506148,
      "grad_norm": 0.5869540572166443,
      "learning_rate": 4.0533996972690845e-05,
      "loss": 1.1855,
      "step": 182300
    },
    {
      "epoch": 0.5683217134112705,
      "grad_norm": 0.7051056027412415,
      "learning_rate": 4.053139976167992e-05,
      "loss": 1.2818,
      "step": 182350
    },
    {
      "epoch": 0.5684775460719261,
      "grad_norm": 0.621597409248352,
      "learning_rate": 4.052880255066899e-05,
      "loss": 1.2662,
      "step": 182400
    },
    {
      "epoch": 0.5686333787325818,
      "grad_norm": 0.647009015083313,
      "learning_rate": 4.052620533965806e-05,
      "loss": 1.23,
      "step": 182450
    },
    {
      "epoch": 0.5687892113932375,
      "grad_norm": 0.533658504486084,
      "learning_rate": 4.0523608128647135e-05,
      "loss": 1.2656,
      "step": 182500
    },
    {
      "epoch": 0.5689450440538931,
      "grad_norm": 0.5126413106918335,
      "learning_rate": 4.052101091763621e-05,
      "loss": 1.2353,
      "step": 182550
    },
    {
      "epoch": 0.5691008767145489,
      "grad_norm": 0.4938739240169525,
      "learning_rate": 4.051841370662528e-05,
      "loss": 1.2763,
      "step": 182600
    },
    {
      "epoch": 0.5692567093752046,
      "grad_norm": 0.6169564127922058,
      "learning_rate": 4.051581649561435e-05,
      "loss": 1.3146,
      "step": 182650
    },
    {
      "epoch": 0.5694125420358602,
      "grad_norm": 0.5534818172454834,
      "learning_rate": 4.051321928460342e-05,
      "loss": 1.2503,
      "step": 182700
    },
    {
      "epoch": 0.5695683746965159,
      "grad_norm": 0.5370913743972778,
      "learning_rate": 4.05106220735925e-05,
      "loss": 1.261,
      "step": 182750
    },
    {
      "epoch": 0.5697242073571716,
      "grad_norm": 0.6136302947998047,
      "learning_rate": 4.050802486258157e-05,
      "loss": 1.2238,
      "step": 182800
    },
    {
      "epoch": 0.5698800400178272,
      "grad_norm": 0.812391996383667,
      "learning_rate": 4.0505427651570636e-05,
      "loss": 1.3107,
      "step": 182850
    },
    {
      "epoch": 0.570035872678483,
      "grad_norm": 0.7501991987228394,
      "learning_rate": 4.0502830440559715e-05,
      "loss": 1.2447,
      "step": 182900
    },
    {
      "epoch": 0.5701917053391387,
      "grad_norm": 0.6411494612693787,
      "learning_rate": 4.050023322954878e-05,
      "loss": 1.2603,
      "step": 182950
    },
    {
      "epoch": 0.5703475379997943,
      "grad_norm": 0.8309946656227112,
      "learning_rate": 4.049763601853785e-05,
      "loss": 1.2246,
      "step": 183000
    },
    {
      "epoch": 0.57050337066045,
      "grad_norm": 0.6086750626564026,
      "learning_rate": 4.0495038807526926e-05,
      "loss": 1.2846,
      "step": 183050
    },
    {
      "epoch": 0.5706592033211056,
      "grad_norm": 0.6104145646095276,
      "learning_rate": 4.0492441596516e-05,
      "loss": 1.2607,
      "step": 183100
    },
    {
      "epoch": 0.5708150359817613,
      "grad_norm": 0.7583717107772827,
      "learning_rate": 4.048984438550507e-05,
      "loss": 1.3357,
      "step": 183150
    },
    {
      "epoch": 0.570970868642417,
      "grad_norm": 0.40760675072669983,
      "learning_rate": 4.048729911871436e-05,
      "loss": 1.2621,
      "step": 183200
    },
    {
      "epoch": 0.5711267013030727,
      "grad_norm": 0.7420857548713684,
      "learning_rate": 4.048470190770343e-05,
      "loss": 1.2692,
      "step": 183250
    },
    {
      "epoch": 0.5712825339637284,
      "grad_norm": 0.5854899287223816,
      "learning_rate": 4.048210469669251e-05,
      "loss": 1.2747,
      "step": 183300
    },
    {
      "epoch": 0.5714383666243841,
      "grad_norm": 0.5707488059997559,
      "learning_rate": 4.047950748568158e-05,
      "loss": 1.2582,
      "step": 183350
    },
    {
      "epoch": 0.5715941992850397,
      "grad_norm": 0.5392256379127502,
      "learning_rate": 4.0476910274670646e-05,
      "loss": 1.2696,
      "step": 183400
    },
    {
      "epoch": 0.5717500319456954,
      "grad_norm": 0.6328666806221008,
      "learning_rate": 4.047431306365972e-05,
      "loss": 1.278,
      "step": 183450
    },
    {
      "epoch": 0.5719058646063511,
      "grad_norm": 0.5289963483810425,
      "learning_rate": 4.04717158526488e-05,
      "loss": 1.291,
      "step": 183500
    },
    {
      "epoch": 0.5720616972670068,
      "grad_norm": 0.5732439756393433,
      "learning_rate": 4.0469118641637864e-05,
      "loss": 1.2609,
      "step": 183550
    },
    {
      "epoch": 0.5722175299276625,
      "grad_norm": 0.7020410299301147,
      "learning_rate": 4.0466521430626936e-05,
      "loss": 1.2777,
      "step": 183600
    },
    {
      "epoch": 0.5723733625883182,
      "grad_norm": 0.6297352910041809,
      "learning_rate": 4.046392421961601e-05,
      "loss": 1.3214,
      "step": 183650
    },
    {
      "epoch": 0.5725291952489738,
      "grad_norm": 0.679639458656311,
      "learning_rate": 4.046132700860508e-05,
      "loss": 1.3206,
      "step": 183700
    },
    {
      "epoch": 0.5726850279096295,
      "grad_norm": 0.5909444689750671,
      "learning_rate": 4.0458729797594154e-05,
      "loss": 1.3157,
      "step": 183750
    },
    {
      "epoch": 0.5728408605702852,
      "grad_norm": 0.6820036172866821,
      "learning_rate": 4.0456132586583226e-05,
      "loss": 1.3178,
      "step": 183800
    },
    {
      "epoch": 0.5729966932309409,
      "grad_norm": 0.5543551445007324,
      "learning_rate": 4.04535353755723e-05,
      "loss": 1.2998,
      "step": 183850
    },
    {
      "epoch": 0.5731525258915966,
      "grad_norm": 0.6251958608627319,
      "learning_rate": 4.045093816456137e-05,
      "loss": 1.2157,
      "step": 183900
    },
    {
      "epoch": 0.5733083585522523,
      "grad_norm": 0.6566923260688782,
      "learning_rate": 4.044834095355044e-05,
      "loss": 1.2656,
      "step": 183950
    },
    {
      "epoch": 0.5734641912129079,
      "grad_norm": 0.5258402228355408,
      "learning_rate": 4.0445743742539516e-05,
      "loss": 1.291,
      "step": 184000
    },
    {
      "epoch": 0.5736200238735636,
      "grad_norm": 0.5669825077056885,
      "learning_rate": 4.044314653152859e-05,
      "loss": 1.2864,
      "step": 184050
    },
    {
      "epoch": 0.5737758565342193,
      "grad_norm": 0.6418685913085938,
      "learning_rate": 4.0440549320517654e-05,
      "loss": 1.3503,
      "step": 184100
    },
    {
      "epoch": 0.573931689194875,
      "grad_norm": 0.5066685676574707,
      "learning_rate": 4.043795210950673e-05,
      "loss": 1.2927,
      "step": 184150
    },
    {
      "epoch": 0.5740875218555307,
      "grad_norm": 0.5332129597663879,
      "learning_rate": 4.0435354898495806e-05,
      "loss": 1.2438,
      "step": 184200
    },
    {
      "epoch": 0.5742433545161864,
      "grad_norm": 0.5449172258377075,
      "learning_rate": 4.043275768748487e-05,
      "loss": 1.235,
      "step": 184250
    },
    {
      "epoch": 0.574399187176842,
      "grad_norm": 0.6508942246437073,
      "learning_rate": 4.0430160476473944e-05,
      "loss": 1.2867,
      "step": 184300
    },
    {
      "epoch": 0.5745550198374977,
      "grad_norm": 0.5086479187011719,
      "learning_rate": 4.042756326546302e-05,
      "loss": 1.2438,
      "step": 184350
    },
    {
      "epoch": 0.5747108524981533,
      "grad_norm": 0.6898919939994812,
      "learning_rate": 4.042496605445209e-05,
      "loss": 1.2549,
      "step": 184400
    },
    {
      "epoch": 0.574866685158809,
      "grad_norm": 0.7687966823577881,
      "learning_rate": 4.042236884344116e-05,
      "loss": 1.275,
      "step": 184450
    },
    {
      "epoch": 0.5750225178194648,
      "grad_norm": 0.7031933665275574,
      "learning_rate": 4.0419771632430234e-05,
      "loss": 1.288,
      "step": 184500
    },
    {
      "epoch": 0.5751783504801204,
      "grad_norm": 0.674678385257721,
      "learning_rate": 4.041717442141931e-05,
      "loss": 1.2608,
      "step": 184550
    },
    {
      "epoch": 0.5753341831407761,
      "grad_norm": 0.6099633574485779,
      "learning_rate": 4.041457721040838e-05,
      "loss": 1.2979,
      "step": 184600
    },
    {
      "epoch": 0.5754900158014318,
      "grad_norm": 0.5747697949409485,
      "learning_rate": 4.0411979999397445e-05,
      "loss": 1.2661,
      "step": 184650
    },
    {
      "epoch": 0.5756458484620874,
      "grad_norm": 0.6862455606460571,
      "learning_rate": 4.040938278838652e-05,
      "loss": 1.2562,
      "step": 184700
    },
    {
      "epoch": 0.5758016811227431,
      "grad_norm": 0.5204344391822815,
      "learning_rate": 4.04067855773756e-05,
      "loss": 1.2677,
      "step": 184750
    },
    {
      "epoch": 0.5759575137833989,
      "grad_norm": 0.5929754376411438,
      "learning_rate": 4.040418836636466e-05,
      "loss": 1.3093,
      "step": 184800
    },
    {
      "epoch": 0.5761133464440545,
      "grad_norm": 0.6242352724075317,
      "learning_rate": 4.0401591155353735e-05,
      "loss": 1.2673,
      "step": 184850
    },
    {
      "epoch": 0.5762691791047102,
      "grad_norm": 0.6884002089500427,
      "learning_rate": 4.0398993944342814e-05,
      "loss": 1.2402,
      "step": 184900
    },
    {
      "epoch": 0.5764250117653659,
      "grad_norm": 0.5844721794128418,
      "learning_rate": 4.039639673333188e-05,
      "loss": 1.3236,
      "step": 184950
    },
    {
      "epoch": 0.5765808444260215,
      "grad_norm": 0.5018792152404785,
      "learning_rate": 4.039379952232095e-05,
      "loss": 1.2303,
      "step": 185000
    },
    {
      "epoch": 0.5767366770866772,
      "grad_norm": 0.6252804398536682,
      "learning_rate": 4.0391202311310025e-05,
      "loss": 1.2503,
      "step": 185050
    },
    {
      "epoch": 0.576892509747333,
      "grad_norm": 0.5743984580039978,
      "learning_rate": 4.03886051002991e-05,
      "loss": 1.2923,
      "step": 185100
    },
    {
      "epoch": 0.5770483424079886,
      "grad_norm": 0.6573079824447632,
      "learning_rate": 4.038600788928817e-05,
      "loss": 1.3381,
      "step": 185150
    },
    {
      "epoch": 0.5772041750686443,
      "grad_norm": 0.6577433943748474,
      "learning_rate": 4.0383410678277236e-05,
      "loss": 1.288,
      "step": 185200
    },
    {
      "epoch": 0.5773600077293,
      "grad_norm": 0.6353569626808167,
      "learning_rate": 4.0380813467266315e-05,
      "loss": 1.2516,
      "step": 185250
    },
    {
      "epoch": 0.5775158403899556,
      "grad_norm": 0.727366030216217,
      "learning_rate": 4.037821625625539e-05,
      "loss": 1.2915,
      "step": 185300
    },
    {
      "epoch": 0.5776716730506113,
      "grad_norm": 0.49317052960395813,
      "learning_rate": 4.037561904524445e-05,
      "loss": 1.3081,
      "step": 185350
    },
    {
      "epoch": 0.5778275057112671,
      "grad_norm": 0.616112470626831,
      "learning_rate": 4.0373021834233526e-05,
      "loss": 1.2854,
      "step": 185400
    },
    {
      "epoch": 0.5779833383719227,
      "grad_norm": 0.4698598384857178,
      "learning_rate": 4.0370424623222605e-05,
      "loss": 1.2404,
      "step": 185450
    },
    {
      "epoch": 0.5781391710325784,
      "grad_norm": 0.586218535900116,
      "learning_rate": 4.036782741221167e-05,
      "loss": 1.3238,
      "step": 185500
    },
    {
      "epoch": 0.578295003693234,
      "grad_norm": 0.5831766128540039,
      "learning_rate": 4.0365230201200743e-05,
      "loss": 1.275,
      "step": 185550
    },
    {
      "epoch": 0.5784508363538897,
      "grad_norm": 0.5700907707214355,
      "learning_rate": 4.0362632990189816e-05,
      "loss": 1.2617,
      "step": 185600
    },
    {
      "epoch": 0.5786066690145454,
      "grad_norm": 0.6462106704711914,
      "learning_rate": 4.036003577917889e-05,
      "loss": 1.2644,
      "step": 185650
    },
    {
      "epoch": 0.5787625016752012,
      "grad_norm": 0.552490234375,
      "learning_rate": 4.035743856816796e-05,
      "loss": 1.2789,
      "step": 185700
    },
    {
      "epoch": 0.5789183343358568,
      "grad_norm": 0.5498912334442139,
      "learning_rate": 4.0354841357157033e-05,
      "loss": 1.2836,
      "step": 185750
    },
    {
      "epoch": 0.5790741669965125,
      "grad_norm": 0.4228047728538513,
      "learning_rate": 4.0352244146146106e-05,
      "loss": 1.268,
      "step": 185800
    },
    {
      "epoch": 0.5792299996571681,
      "grad_norm": 0.5930004119873047,
      "learning_rate": 4.034964693513518e-05,
      "loss": 1.2563,
      "step": 185850
    },
    {
      "epoch": 0.5793858323178238,
      "grad_norm": 0.6168150901794434,
      "learning_rate": 4.0347049724124244e-05,
      "loss": 1.2573,
      "step": 185900
    },
    {
      "epoch": 0.5795416649784795,
      "grad_norm": 0.6052978038787842,
      "learning_rate": 4.034445251311332e-05,
      "loss": 1.3347,
      "step": 185950
    },
    {
      "epoch": 0.5796974976391351,
      "grad_norm": 0.6417331695556641,
      "learning_rate": 4.0341855302102396e-05,
      "loss": 1.2941,
      "step": 186000
    },
    {
      "epoch": 0.5798533302997909,
      "grad_norm": 0.7134687304496765,
      "learning_rate": 4.033925809109146e-05,
      "loss": 1.2843,
      "step": 186050
    },
    {
      "epoch": 0.5800091629604466,
      "grad_norm": 0.5504260659217834,
      "learning_rate": 4.0336660880080534e-05,
      "loss": 1.2406,
      "step": 186100
    },
    {
      "epoch": 0.5801649956211022,
      "grad_norm": 0.7019891738891602,
      "learning_rate": 4.0334063669069613e-05,
      "loss": 1.2898,
      "step": 186150
    },
    {
      "epoch": 0.5803208282817579,
      "grad_norm": 0.48003554344177246,
      "learning_rate": 4.033146645805868e-05,
      "loss": 1.297,
      "step": 186200
    },
    {
      "epoch": 0.5804766609424136,
      "grad_norm": 0.7451887726783752,
      "learning_rate": 4.032886924704775e-05,
      "loss": 1.2517,
      "step": 186250
    },
    {
      "epoch": 0.5806324936030692,
      "grad_norm": 0.5721503496170044,
      "learning_rate": 4.0326272036036824e-05,
      "loss": 1.2669,
      "step": 186300
    },
    {
      "epoch": 0.580788326263725,
      "grad_norm": 0.6827594637870789,
      "learning_rate": 4.03236748250259e-05,
      "loss": 1.2942,
      "step": 186350
    },
    {
      "epoch": 0.5809441589243807,
      "grad_norm": 0.5774710178375244,
      "learning_rate": 4.032107761401497e-05,
      "loss": 1.2342,
      "step": 186400
    },
    {
      "epoch": 0.5810999915850363,
      "grad_norm": 0.5175358057022095,
      "learning_rate": 4.031848040300404e-05,
      "loss": 1.2585,
      "step": 186450
    },
    {
      "epoch": 0.581255824245692,
      "grad_norm": 0.62489253282547,
      "learning_rate": 4.0315883191993114e-05,
      "loss": 1.2663,
      "step": 186500
    },
    {
      "epoch": 0.5814116569063477,
      "grad_norm": 0.4326961040496826,
      "learning_rate": 4.031328598098219e-05,
      "loss": 1.2628,
      "step": 186550
    },
    {
      "epoch": 0.5815674895670033,
      "grad_norm": 0.6330729126930237,
      "learning_rate": 4.031068876997125e-05,
      "loss": 1.3084,
      "step": 186600
    },
    {
      "epoch": 0.5817233222276591,
      "grad_norm": 0.5943904519081116,
      "learning_rate": 4.0308091558960325e-05,
      "loss": 1.307,
      "step": 186650
    },
    {
      "epoch": 0.5818791548883148,
      "grad_norm": 0.5628844499588013,
      "learning_rate": 4.0305494347949404e-05,
      "loss": 1.2134,
      "step": 186700
    },
    {
      "epoch": 0.5820349875489704,
      "grad_norm": 0.4989558458328247,
      "learning_rate": 4.030289713693847e-05,
      "loss": 1.2681,
      "step": 186750
    },
    {
      "epoch": 0.5821908202096261,
      "grad_norm": 0.6097825765609741,
      "learning_rate": 4.030029992592754e-05,
      "loss": 1.2551,
      "step": 186800
    },
    {
      "epoch": 0.5823466528702818,
      "grad_norm": 0.5287413001060486,
      "learning_rate": 4.029770271491662e-05,
      "loss": 1.213,
      "step": 186850
    },
    {
      "epoch": 0.5825024855309374,
      "grad_norm": 0.611534059047699,
      "learning_rate": 4.029510550390569e-05,
      "loss": 1.2478,
      "step": 186900
    },
    {
      "epoch": 0.5826583181915932,
      "grad_norm": 0.5528048872947693,
      "learning_rate": 4.029250829289476e-05,
      "loss": 1.2647,
      "step": 186950
    },
    {
      "epoch": 0.5828141508522489,
      "grad_norm": 0.5153080224990845,
      "learning_rate": 4.028991108188383e-05,
      "loss": 1.3103,
      "step": 187000
    },
    {
      "epoch": 0.5829699835129045,
      "grad_norm": 0.5968290567398071,
      "learning_rate": 4.0287313870872905e-05,
      "loss": 1.2626,
      "step": 187050
    },
    {
      "epoch": 0.5831258161735602,
      "grad_norm": 0.6097970604896545,
      "learning_rate": 4.028471665986198e-05,
      "loss": 1.2939,
      "step": 187100
    },
    {
      "epoch": 0.5832816488342158,
      "grad_norm": 0.5490946173667908,
      "learning_rate": 4.028211944885105e-05,
      "loss": 1.2986,
      "step": 187150
    },
    {
      "epoch": 0.5834374814948715,
      "grad_norm": 0.6026803851127625,
      "learning_rate": 4.0279574182060335e-05,
      "loss": 1.2461,
      "step": 187200
    },
    {
      "epoch": 0.5835933141555272,
      "grad_norm": 0.7438319325447083,
      "learning_rate": 4.0276976971049415e-05,
      "loss": 1.3319,
      "step": 187250
    },
    {
      "epoch": 0.583749146816183,
      "grad_norm": 0.4204316735267639,
      "learning_rate": 4.027437976003848e-05,
      "loss": 1.274,
      "step": 187300
    },
    {
      "epoch": 0.5839049794768386,
      "grad_norm": 0.6214482188224792,
      "learning_rate": 4.027183449324777e-05,
      "loss": 1.2692,
      "step": 187350
    },
    {
      "epoch": 0.5840608121374943,
      "grad_norm": 0.6411049962043762,
      "learning_rate": 4.0269237282236845e-05,
      "loss": 1.3488,
      "step": 187400
    },
    {
      "epoch": 0.5842166447981499,
      "grad_norm": 0.5976201891899109,
      "learning_rate": 4.026664007122591e-05,
      "loss": 1.2779,
      "step": 187450
    },
    {
      "epoch": 0.5843724774588056,
      "grad_norm": 0.4596599340438843,
      "learning_rate": 4.026404286021499e-05,
      "loss": 1.2536,
      "step": 187500
    },
    {
      "epoch": 0.5845283101194613,
      "grad_norm": 0.6152087450027466,
      "learning_rate": 4.026144564920406e-05,
      "loss": 1.2096,
      "step": 187550
    },
    {
      "epoch": 0.584684142780117,
      "grad_norm": 0.6244821548461914,
      "learning_rate": 4.025884843819313e-05,
      "loss": 1.2448,
      "step": 187600
    },
    {
      "epoch": 0.5848399754407727,
      "grad_norm": 0.6020123958587646,
      "learning_rate": 4.025625122718221e-05,
      "loss": 1.2546,
      "step": 187650
    },
    {
      "epoch": 0.5849958081014284,
      "grad_norm": 0.589783787727356,
      "learning_rate": 4.025365401617128e-05,
      "loss": 1.2217,
      "step": 187700
    },
    {
      "epoch": 0.585151640762084,
      "grad_norm": 0.5460779666900635,
      "learning_rate": 4.0251056805160346e-05,
      "loss": 1.2574,
      "step": 187750
    },
    {
      "epoch": 0.5853074734227397,
      "grad_norm": 0.6094915866851807,
      "learning_rate": 4.024845959414942e-05,
      "loss": 1.3258,
      "step": 187800
    },
    {
      "epoch": 0.5854633060833954,
      "grad_norm": 0.6719242334365845,
      "learning_rate": 4.024586238313849e-05,
      "loss": 1.2834,
      "step": 187850
    },
    {
      "epoch": 0.5856191387440511,
      "grad_norm": 0.5842164754867554,
      "learning_rate": 4.024326517212756e-05,
      "loss": 1.3131,
      "step": 187900
    },
    {
      "epoch": 0.5857749714047068,
      "grad_norm": 0.6131096482276917,
      "learning_rate": 4.0240667961116636e-05,
      "loss": 1.2685,
      "step": 187950
    },
    {
      "epoch": 0.5859308040653625,
      "grad_norm": 0.6065405011177063,
      "learning_rate": 4.023807075010571e-05,
      "loss": 1.2906,
      "step": 188000
    },
    {
      "epoch": 0.5860866367260181,
      "grad_norm": 0.7229238748550415,
      "learning_rate": 4.023547353909478e-05,
      "loss": 1.2285,
      "step": 188050
    },
    {
      "epoch": 0.5862424693866738,
      "grad_norm": 0.5797953605651855,
      "learning_rate": 4.023287632808385e-05,
      "loss": 1.2662,
      "step": 188100
    },
    {
      "epoch": 0.5863983020473295,
      "grad_norm": 0.4747297465801239,
      "learning_rate": 4.023027911707292e-05,
      "loss": 1.3132,
      "step": 188150
    },
    {
      "epoch": 0.5865541347079852,
      "grad_norm": 0.6258133053779602,
      "learning_rate": 4.0227681906062e-05,
      "loss": 1.3069,
      "step": 188200
    },
    {
      "epoch": 0.5867099673686409,
      "grad_norm": 0.5575190782546997,
      "learning_rate": 4.022508469505107e-05,
      "loss": 1.3354,
      "step": 188250
    },
    {
      "epoch": 0.5868658000292966,
      "grad_norm": 0.5141082406044006,
      "learning_rate": 4.0222487484040136e-05,
      "loss": 1.2511,
      "step": 188300
    },
    {
      "epoch": 0.5870216326899522,
      "grad_norm": 0.6335542798042297,
      "learning_rate": 4.0219890273029216e-05,
      "loss": 1.2708,
      "step": 188350
    },
    {
      "epoch": 0.5871774653506079,
      "grad_norm": 0.5154556632041931,
      "learning_rate": 4.021729306201829e-05,
      "loss": 1.3062,
      "step": 188400
    },
    {
      "epoch": 0.5873332980112635,
      "grad_norm": 0.7102959752082825,
      "learning_rate": 4.0214695851007354e-05,
      "loss": 1.2647,
      "step": 188450
    },
    {
      "epoch": 0.5874891306719192,
      "grad_norm": 0.7610617280006409,
      "learning_rate": 4.0212098639996426e-05,
      "loss": 1.2356,
      "step": 188500
    },
    {
      "epoch": 0.587644963332575,
      "grad_norm": 0.6641021966934204,
      "learning_rate": 4.02095014289855e-05,
      "loss": 1.2975,
      "step": 188550
    },
    {
      "epoch": 0.5878007959932307,
      "grad_norm": 0.6582244038581848,
      "learning_rate": 4.020690421797457e-05,
      "loss": 1.2544,
      "step": 188600
    },
    {
      "epoch": 0.5879566286538863,
      "grad_norm": 0.8173955678939819,
      "learning_rate": 4.0204307006963644e-05,
      "loss": 1.2739,
      "step": 188650
    },
    {
      "epoch": 0.588112461314542,
      "grad_norm": 0.79898601770401,
      "learning_rate": 4.0201709795952716e-05,
      "loss": 1.2651,
      "step": 188700
    },
    {
      "epoch": 0.5882682939751976,
      "grad_norm": 0.5126559138298035,
      "learning_rate": 4.019911258494179e-05,
      "loss": 1.2799,
      "step": 188750
    },
    {
      "epoch": 0.5884241266358533,
      "grad_norm": 0.604981541633606,
      "learning_rate": 4.019651537393086e-05,
      "loss": 1.2933,
      "step": 188800
    },
    {
      "epoch": 0.5885799592965091,
      "grad_norm": 0.5711925625801086,
      "learning_rate": 4.019391816291993e-05,
      "loss": 1.2726,
      "step": 188850
    },
    {
      "epoch": 0.5887357919571647,
      "grad_norm": 0.5102505683898926,
      "learning_rate": 4.0191320951909006e-05,
      "loss": 1.3223,
      "step": 188900
    },
    {
      "epoch": 0.5888916246178204,
      "grad_norm": 0.5588755011558533,
      "learning_rate": 4.018872374089808e-05,
      "loss": 1.2994,
      "step": 188950
    },
    {
      "epoch": 0.5890474572784761,
      "grad_norm": 0.6671631336212158,
      "learning_rate": 4.0186126529887145e-05,
      "loss": 1.2817,
      "step": 189000
    },
    {
      "epoch": 0.5892032899391317,
      "grad_norm": 0.7591487765312195,
      "learning_rate": 4.018352931887622e-05,
      "loss": 1.3164,
      "step": 189050
    },
    {
      "epoch": 0.5893591225997874,
      "grad_norm": 0.6187503337860107,
      "learning_rate": 4.0180932107865296e-05,
      "loss": 1.2874,
      "step": 189100
    },
    {
      "epoch": 0.5895149552604432,
      "grad_norm": 0.6121912002563477,
      "learning_rate": 4.017833489685436e-05,
      "loss": 1.2873,
      "step": 189150
    },
    {
      "epoch": 0.5896707879210988,
      "grad_norm": 0.5866326689720154,
      "learning_rate": 4.0175737685843435e-05,
      "loss": 1.2994,
      "step": 189200
    },
    {
      "epoch": 0.5898266205817545,
      "grad_norm": 0.6104822158813477,
      "learning_rate": 4.017314047483251e-05,
      "loss": 1.3038,
      "step": 189250
    },
    {
      "epoch": 0.5899824532424102,
      "grad_norm": 0.5459830164909363,
      "learning_rate": 4.017054326382158e-05,
      "loss": 1.2234,
      "step": 189300
    },
    {
      "epoch": 0.5901382859030658,
      "grad_norm": 0.43959707021713257,
      "learning_rate": 4.016794605281065e-05,
      "loss": 1.2462,
      "step": 189350
    },
    {
      "epoch": 0.5902941185637215,
      "grad_norm": 0.5962198972702026,
      "learning_rate": 4.0165348841799725e-05,
      "loss": 1.3146,
      "step": 189400
    },
    {
      "epoch": 0.5904499512243773,
      "grad_norm": 0.6529532074928284,
      "learning_rate": 4.01627516307888e-05,
      "loss": 1.268,
      "step": 189450
    },
    {
      "epoch": 0.5906057838850329,
      "grad_norm": 0.612642228603363,
      "learning_rate": 4.016015441977787e-05,
      "loss": 1.2832,
      "step": 189500
    },
    {
      "epoch": 0.5907616165456886,
      "grad_norm": 0.5401579141616821,
      "learning_rate": 4.0157557208766935e-05,
      "loss": 1.2411,
      "step": 189550
    },
    {
      "epoch": 0.5909174492063443,
      "grad_norm": 0.4451392590999603,
      "learning_rate": 4.0154959997756015e-05,
      "loss": 1.2543,
      "step": 189600
    },
    {
      "epoch": 0.5910732818669999,
      "grad_norm": 0.6831021904945374,
      "learning_rate": 4.015236278674509e-05,
      "loss": 1.2626,
      "step": 189650
    },
    {
      "epoch": 0.5912291145276556,
      "grad_norm": 0.696334958076477,
      "learning_rate": 4.014976557573415e-05,
      "loss": 1.2729,
      "step": 189700
    },
    {
      "epoch": 0.5913849471883112,
      "grad_norm": 0.5802069306373596,
      "learning_rate": 4.0147168364723225e-05,
      "loss": 1.2391,
      "step": 189750
    },
    {
      "epoch": 0.591540779848967,
      "grad_norm": 0.6311849355697632,
      "learning_rate": 4.0144571153712305e-05,
      "loss": 1.3088,
      "step": 189800
    },
    {
      "epoch": 0.5916966125096227,
      "grad_norm": 0.6442586183547974,
      "learning_rate": 4.014197394270137e-05,
      "loss": 1.2556,
      "step": 189850
    },
    {
      "epoch": 0.5918524451702784,
      "grad_norm": 0.5074144601821899,
      "learning_rate": 4.013937673169044e-05,
      "loss": 1.2839,
      "step": 189900
    },
    {
      "epoch": 0.592008277830934,
      "grad_norm": 0.4851645231246948,
      "learning_rate": 4.0136779520679515e-05,
      "loss": 1.351,
      "step": 189950
    },
    {
      "epoch": 0.5921641104915897,
      "grad_norm": 0.4325522482395172,
      "learning_rate": 4.013418230966859e-05,
      "loss": 1.2658,
      "step": 190000
    },
    {
      "epoch": 0.5923199431522453,
      "grad_norm": 0.583758533000946,
      "learning_rate": 4.013158509865766e-05,
      "loss": 1.309,
      "step": 190050
    },
    {
      "epoch": 0.5924757758129011,
      "grad_norm": 0.40339845418930054,
      "learning_rate": 4.0128987887646726e-05,
      "loss": 1.2972,
      "step": 190100
    },
    {
      "epoch": 0.5926316084735568,
      "grad_norm": 0.5673566460609436,
      "learning_rate": 4.0126390676635805e-05,
      "loss": 1.2427,
      "step": 190150
    },
    {
      "epoch": 0.5927874411342124,
      "grad_norm": 0.5905590057373047,
      "learning_rate": 4.012379346562488e-05,
      "loss": 1.3123,
      "step": 190200
    },
    {
      "epoch": 0.5929432737948681,
      "grad_norm": 0.4685143828392029,
      "learning_rate": 4.0121196254613944e-05,
      "loss": 1.2772,
      "step": 190250
    },
    {
      "epoch": 0.5930991064555238,
      "grad_norm": 0.6149799227714539,
      "learning_rate": 4.0118599043603016e-05,
      "loss": 1.2686,
      "step": 190300
    },
    {
      "epoch": 0.5932549391161794,
      "grad_norm": 0.5574237704277039,
      "learning_rate": 4.0116001832592096e-05,
      "loss": 1.2703,
      "step": 190350
    },
    {
      "epoch": 0.5934107717768352,
      "grad_norm": 0.5475642085075378,
      "learning_rate": 4.011340462158116e-05,
      "loss": 1.247,
      "step": 190400
    },
    {
      "epoch": 0.5935666044374909,
      "grad_norm": 0.6475778222084045,
      "learning_rate": 4.0110807410570234e-05,
      "loss": 1.28,
      "step": 190450
    },
    {
      "epoch": 0.5937224370981465,
      "grad_norm": 0.5904285311698914,
      "learning_rate": 4.0108210199559306e-05,
      "loss": 1.2458,
      "step": 190500
    },
    {
      "epoch": 0.5938782697588022,
      "grad_norm": 0.6672846674919128,
      "learning_rate": 4.010561298854838e-05,
      "loss": 1.3217,
      "step": 190550
    },
    {
      "epoch": 0.5940341024194579,
      "grad_norm": 0.7026747465133667,
      "learning_rate": 4.010301577753745e-05,
      "loss": 1.2865,
      "step": 190600
    },
    {
      "epoch": 0.5941899350801135,
      "grad_norm": 0.6235398650169373,
      "learning_rate": 4.0100418566526524e-05,
      "loss": 1.2989,
      "step": 190650
    },
    {
      "epoch": 0.5943457677407693,
      "grad_norm": 0.5278922915458679,
      "learning_rate": 4.0097821355515596e-05,
      "loss": 1.2539,
      "step": 190700
    },
    {
      "epoch": 0.594501600401425,
      "grad_norm": 0.54766845703125,
      "learning_rate": 4.009522414450467e-05,
      "loss": 1.265,
      "step": 190750
    },
    {
      "epoch": 0.5946574330620806,
      "grad_norm": 0.5001869201660156,
      "learning_rate": 4.0092626933493734e-05,
      "loss": 1.2339,
      "step": 190800
    },
    {
      "epoch": 0.5948132657227363,
      "grad_norm": 0.5224082469940186,
      "learning_rate": 4.0090029722482814e-05,
      "loss": 1.2858,
      "step": 190850
    },
    {
      "epoch": 0.594969098383392,
      "grad_norm": 0.5158872008323669,
      "learning_rate": 4.0087432511471886e-05,
      "loss": 1.2785,
      "step": 190900
    },
    {
      "epoch": 0.5951249310440476,
      "grad_norm": 0.7357028126716614,
      "learning_rate": 4.008483530046095e-05,
      "loss": 1.3021,
      "step": 190950
    },
    {
      "epoch": 0.5952807637047033,
      "grad_norm": 0.610054612159729,
      "learning_rate": 4.0082238089450024e-05,
      "loss": 1.2887,
      "step": 191000
    },
    {
      "epoch": 0.5954365963653591,
      "grad_norm": 0.672198474407196,
      "learning_rate": 4.0079640878439104e-05,
      "loss": 1.2728,
      "step": 191050
    },
    {
      "epoch": 0.5955924290260147,
      "grad_norm": 0.5578683614730835,
      "learning_rate": 4.007704366742817e-05,
      "loss": 1.2441,
      "step": 191100
    },
    {
      "epoch": 0.5957482616866704,
      "grad_norm": 0.5697489976882935,
      "learning_rate": 4.007444645641724e-05,
      "loss": 1.3043,
      "step": 191150
    },
    {
      "epoch": 0.595904094347326,
      "grad_norm": 0.4741530418395996,
      "learning_rate": 4.0071849245406315e-05,
      "loss": 1.2834,
      "step": 191200
    },
    {
      "epoch": 0.5960599270079817,
      "grad_norm": 0.5781334638595581,
      "learning_rate": 4.006930397861561e-05,
      "loss": 1.2372,
      "step": 191250
    },
    {
      "epoch": 0.5962157596686374,
      "grad_norm": 0.5475937128067017,
      "learning_rate": 4.006670676760468e-05,
      "loss": 1.2254,
      "step": 191300
    },
    {
      "epoch": 0.5963715923292932,
      "grad_norm": 0.6365494728088379,
      "learning_rate": 4.006410955659375e-05,
      "loss": 1.2734,
      "step": 191350
    },
    {
      "epoch": 0.5965274249899488,
      "grad_norm": 0.7165797352790833,
      "learning_rate": 4.006151234558282e-05,
      "loss": 1.3172,
      "step": 191400
    },
    {
      "epoch": 0.5966832576506045,
      "grad_norm": 0.6329336166381836,
      "learning_rate": 4.00589151345719e-05,
      "loss": 1.302,
      "step": 191450
    },
    {
      "epoch": 0.5968390903112601,
      "grad_norm": 0.6242782473564148,
      "learning_rate": 4.005631792356096e-05,
      "loss": 1.3169,
      "step": 191500
    },
    {
      "epoch": 0.5969949229719158,
      "grad_norm": 0.6729143261909485,
      "learning_rate": 4.0053720712550035e-05,
      "loss": 1.2571,
      "step": 191550
    },
    {
      "epoch": 0.5971507556325715,
      "grad_norm": 0.5326117277145386,
      "learning_rate": 4.0051123501539114e-05,
      "loss": 1.3228,
      "step": 191600
    },
    {
      "epoch": 0.5973065882932272,
      "grad_norm": 0.6622045040130615,
      "learning_rate": 4.004852629052818e-05,
      "loss": 1.309,
      "step": 191650
    },
    {
      "epoch": 0.5974624209538829,
      "grad_norm": 0.7261117100715637,
      "learning_rate": 4.004592907951725e-05,
      "loss": 1.3128,
      "step": 191700
    },
    {
      "epoch": 0.5976182536145386,
      "grad_norm": 0.5932720899581909,
      "learning_rate": 4.0043331868506325e-05,
      "loss": 1.2666,
      "step": 191750
    },
    {
      "epoch": 0.5977740862751942,
      "grad_norm": 0.4464029371738434,
      "learning_rate": 4.00407346574954e-05,
      "loss": 1.2569,
      "step": 191800
    },
    {
      "epoch": 0.5979299189358499,
      "grad_norm": 0.7106451988220215,
      "learning_rate": 4.003813744648447e-05,
      "loss": 1.2641,
      "step": 191850
    },
    {
      "epoch": 0.5980857515965056,
      "grad_norm": 0.6170849800109863,
      "learning_rate": 4.003554023547354e-05,
      "loss": 1.3335,
      "step": 191900
    },
    {
      "epoch": 0.5982415842571613,
      "grad_norm": 0.5022038817405701,
      "learning_rate": 4.003294302446261e-05,
      "loss": 1.2573,
      "step": 191950
    },
    {
      "epoch": 0.598397416917817,
      "grad_norm": 0.5240921974182129,
      "learning_rate": 4.003034581345169e-05,
      "loss": 1.2541,
      "step": 192000
    },
    {
      "epoch": 0.5985532495784727,
      "grad_norm": 0.7125792503356934,
      "learning_rate": 4.002774860244076e-05,
      "loss": 1.2672,
      "step": 192050
    },
    {
      "epoch": 0.5987090822391283,
      "grad_norm": 0.600852906703949,
      "learning_rate": 4.0025151391429826e-05,
      "loss": 1.2734,
      "step": 192100
    },
    {
      "epoch": 0.598864914899784,
      "grad_norm": 0.5958154797554016,
      "learning_rate": 4.0022554180418905e-05,
      "loss": 1.2848,
      "step": 192150
    },
    {
      "epoch": 0.5990207475604397,
      "grad_norm": 0.5797392725944519,
      "learning_rate": 4.001995696940797e-05,
      "loss": 1.2827,
      "step": 192200
    },
    {
      "epoch": 0.5991765802210954,
      "grad_norm": 0.6304718852043152,
      "learning_rate": 4.001735975839704e-05,
      "loss": 1.2747,
      "step": 192250
    },
    {
      "epoch": 0.5993324128817511,
      "grad_norm": 0.5963767766952515,
      "learning_rate": 4.0014762547386116e-05,
      "loss": 1.2416,
      "step": 192300
    },
    {
      "epoch": 0.5994882455424068,
      "grad_norm": 0.5884056687355042,
      "learning_rate": 4.001216533637519e-05,
      "loss": 1.3301,
      "step": 192350
    },
    {
      "epoch": 0.5996440782030624,
      "grad_norm": 0.5959275960922241,
      "learning_rate": 4.000956812536426e-05,
      "loss": 1.2497,
      "step": 192400
    },
    {
      "epoch": 0.5997999108637181,
      "grad_norm": 0.7010215520858765,
      "learning_rate": 4.000697091435333e-05,
      "loss": 1.3155,
      "step": 192450
    },
    {
      "epoch": 0.5999557435243738,
      "grad_norm": 0.5429340600967407,
      "learning_rate": 4.0004373703342406e-05,
      "loss": 1.2764,
      "step": 192500
    },
    {
      "epoch": 0.6001115761850294,
      "grad_norm": 0.55997633934021,
      "learning_rate": 4.000177649233148e-05,
      "loss": 1.2701,
      "step": 192550
    },
    {
      "epoch": 0.6002674088456852,
      "grad_norm": 0.582821786403656,
      "learning_rate": 3.999917928132055e-05,
      "loss": 1.2742,
      "step": 192600
    },
    {
      "epoch": 0.6004232415063409,
      "grad_norm": 0.6318413019180298,
      "learning_rate": 3.9996582070309616e-05,
      "loss": 1.2919,
      "step": 192650
    },
    {
      "epoch": 0.6005790741669965,
      "grad_norm": 0.69875568151474,
      "learning_rate": 3.9994036803518915e-05,
      "loss": 1.2776,
      "step": 192700
    },
    {
      "epoch": 0.6007349068276522,
      "grad_norm": 0.615453839302063,
      "learning_rate": 3.999143959250798e-05,
      "loss": 1.2347,
      "step": 192750
    },
    {
      "epoch": 0.6008907394883078,
      "grad_norm": 0.729686439037323,
      "learning_rate": 3.9988842381497054e-05,
      "loss": 1.2915,
      "step": 192800
    },
    {
      "epoch": 0.6010465721489635,
      "grad_norm": 0.719240128993988,
      "learning_rate": 3.9986245170486126e-05,
      "loss": 1.1979,
      "step": 192850
    },
    {
      "epoch": 0.6012024048096193,
      "grad_norm": 0.7427785992622375,
      "learning_rate": 3.99836479594752e-05,
      "loss": 1.322,
      "step": 192900
    },
    {
      "epoch": 0.601358237470275,
      "grad_norm": 0.7322011590003967,
      "learning_rate": 3.998105074846427e-05,
      "loss": 1.2104,
      "step": 192950
    },
    {
      "epoch": 0.6015140701309306,
      "grad_norm": 0.8104621767997742,
      "learning_rate": 3.9978453537453344e-05,
      "loss": 1.2517,
      "step": 193000
    },
    {
      "epoch": 0.6016699027915863,
      "grad_norm": 0.6985605359077454,
      "learning_rate": 3.997585632644241e-05,
      "loss": 1.2805,
      "step": 193050
    },
    {
      "epoch": 0.6018257354522419,
      "grad_norm": 0.7817198634147644,
      "learning_rate": 3.997325911543149e-05,
      "loss": 1.2846,
      "step": 193100
    },
    {
      "epoch": 0.6019815681128976,
      "grad_norm": 0.696296751499176,
      "learning_rate": 3.997066190442056e-05,
      "loss": 1.2897,
      "step": 193150
    },
    {
      "epoch": 0.6021374007735534,
      "grad_norm": 0.6673757433891296,
      "learning_rate": 3.996806469340963e-05,
      "loss": 1.2589,
      "step": 193200
    },
    {
      "epoch": 0.602293233434209,
      "grad_norm": 0.6177590489387512,
      "learning_rate": 3.9965467482398706e-05,
      "loss": 1.2939,
      "step": 193250
    },
    {
      "epoch": 0.6024490660948647,
      "grad_norm": 0.6968129873275757,
      "learning_rate": 3.996287027138778e-05,
      "loss": 1.2899,
      "step": 193300
    },
    {
      "epoch": 0.6026048987555204,
      "grad_norm": 0.8044605851173401,
      "learning_rate": 3.9960273060376844e-05,
      "loss": 1.3095,
      "step": 193350
    },
    {
      "epoch": 0.602760731416176,
      "grad_norm": 0.6582750678062439,
      "learning_rate": 3.995767584936592e-05,
      "loss": 1.3097,
      "step": 193400
    },
    {
      "epoch": 0.6029165640768317,
      "grad_norm": 0.588542640209198,
      "learning_rate": 3.995507863835499e-05,
      "loss": 1.2076,
      "step": 193450
    },
    {
      "epoch": 0.6030723967374875,
      "grad_norm": 0.5945594906806946,
      "learning_rate": 3.995248142734406e-05,
      "loss": 1.2588,
      "step": 193500
    },
    {
      "epoch": 0.6032282293981431,
      "grad_norm": 0.5937480926513672,
      "learning_rate": 3.9949884216333134e-05,
      "loss": 1.2917,
      "step": 193550
    },
    {
      "epoch": 0.6033840620587988,
      "grad_norm": 0.659422755241394,
      "learning_rate": 3.994728700532221e-05,
      "loss": 1.3082,
      "step": 193600
    },
    {
      "epoch": 0.6035398947194545,
      "grad_norm": 0.7127808928489685,
      "learning_rate": 3.994468979431128e-05,
      "loss": 1.261,
      "step": 193650
    },
    {
      "epoch": 0.6036957273801101,
      "grad_norm": 0.7674804925918579,
      "learning_rate": 3.994209258330035e-05,
      "loss": 1.2369,
      "step": 193700
    },
    {
      "epoch": 0.6038515600407658,
      "grad_norm": 0.5352988243103027,
      "learning_rate": 3.993949537228942e-05,
      "loss": 1.2694,
      "step": 193750
    },
    {
      "epoch": 0.6040073927014215,
      "grad_norm": 0.579515278339386,
      "learning_rate": 3.99368981612785e-05,
      "loss": 1.2889,
      "step": 193800
    },
    {
      "epoch": 0.6041632253620772,
      "grad_norm": 0.489926278591156,
      "learning_rate": 3.993430095026757e-05,
      "loss": 1.2573,
      "step": 193850
    },
    {
      "epoch": 0.6043190580227329,
      "grad_norm": 0.6527678370475769,
      "learning_rate": 3.9931703739256635e-05,
      "loss": 1.2685,
      "step": 193900
    },
    {
      "epoch": 0.6044748906833886,
      "grad_norm": 0.4696420729160309,
      "learning_rate": 3.9929106528245714e-05,
      "loss": 1.2908,
      "step": 193950
    },
    {
      "epoch": 0.6046307233440442,
      "grad_norm": 0.687336802482605,
      "learning_rate": 3.992650931723479e-05,
      "loss": 1.2754,
      "step": 194000
    },
    {
      "epoch": 0.6047865560046999,
      "grad_norm": 0.4670640826225281,
      "learning_rate": 3.992391210622385e-05,
      "loss": 1.2933,
      "step": 194050
    },
    {
      "epoch": 0.6049423886653555,
      "grad_norm": 0.6690991520881653,
      "learning_rate": 3.9921314895212925e-05,
      "loss": 1.2779,
      "step": 194100
    },
    {
      "epoch": 0.6050982213260113,
      "grad_norm": 0.4984358549118042,
      "learning_rate": 3.9918717684202e-05,
      "loss": 1.2714,
      "step": 194150
    },
    {
      "epoch": 0.605254053986667,
      "grad_norm": 0.6085771918296814,
      "learning_rate": 3.991612047319107e-05,
      "loss": 1.268,
      "step": 194200
    },
    {
      "epoch": 0.6054098866473226,
      "grad_norm": 0.5481331944465637,
      "learning_rate": 3.991352326218014e-05,
      "loss": 1.2968,
      "step": 194250
    },
    {
      "epoch": 0.6055657193079783,
      "grad_norm": 0.5420446991920471,
      "learning_rate": 3.9910926051169215e-05,
      "loss": 1.2565,
      "step": 194300
    },
    {
      "epoch": 0.605721551968634,
      "grad_norm": 0.640873908996582,
      "learning_rate": 3.990832884015829e-05,
      "loss": 1.3306,
      "step": 194350
    },
    {
      "epoch": 0.6058773846292896,
      "grad_norm": 0.6317607760429382,
      "learning_rate": 3.990573162914736e-05,
      "loss": 1.2865,
      "step": 194400
    },
    {
      "epoch": 0.6060332172899454,
      "grad_norm": 0.6379064321517944,
      "learning_rate": 3.9903134418136426e-05,
      "loss": 1.2774,
      "step": 194450
    },
    {
      "epoch": 0.6061890499506011,
      "grad_norm": 0.5375171303749084,
      "learning_rate": 3.9900537207125505e-05,
      "loss": 1.2946,
      "step": 194500
    },
    {
      "epoch": 0.6063448826112567,
      "grad_norm": 0.6895569562911987,
      "learning_rate": 3.989793999611458e-05,
      "loss": 1.2322,
      "step": 194550
    },
    {
      "epoch": 0.6065007152719124,
      "grad_norm": 0.666262149810791,
      "learning_rate": 3.989534278510364e-05,
      "loss": 1.2696,
      "step": 194600
    },
    {
      "epoch": 0.6066565479325681,
      "grad_norm": 0.5849983096122742,
      "learning_rate": 3.9892745574092716e-05,
      "loss": 1.2612,
      "step": 194650
    },
    {
      "epoch": 0.6068123805932237,
      "grad_norm": 0.6733120083808899,
      "learning_rate": 3.9890148363081795e-05,
      "loss": 1.3077,
      "step": 194700
    },
    {
      "epoch": 0.6069682132538795,
      "grad_norm": 0.716981053352356,
      "learning_rate": 3.988755115207086e-05,
      "loss": 1.2964,
      "step": 194750
    },
    {
      "epoch": 0.6071240459145352,
      "grad_norm": 0.48011183738708496,
      "learning_rate": 3.988495394105993e-05,
      "loss": 1.2274,
      "step": 194800
    },
    {
      "epoch": 0.6072798785751908,
      "grad_norm": 0.6460979580879211,
      "learning_rate": 3.9882356730049006e-05,
      "loss": 1.291,
      "step": 194850
    },
    {
      "epoch": 0.6074357112358465,
      "grad_norm": 0.5877129435539246,
      "learning_rate": 3.987975951903808e-05,
      "loss": 1.236,
      "step": 194900
    },
    {
      "epoch": 0.6075915438965022,
      "grad_norm": 0.5914149880409241,
      "learning_rate": 3.987716230802715e-05,
      "loss": 1.244,
      "step": 194950
    },
    {
      "epoch": 0.6077473765571578,
      "grad_norm": 0.6472278833389282,
      "learning_rate": 3.9874565097016217e-05,
      "loss": 1.2922,
      "step": 195000
    },
    {
      "epoch": 0.6079032092178135,
      "grad_norm": 0.6135692596435547,
      "learning_rate": 3.987201983022551e-05,
      "loss": 1.3065,
      "step": 195050
    },
    {
      "epoch": 0.6080590418784693,
      "grad_norm": 0.5444263815879822,
      "learning_rate": 3.986942261921459e-05,
      "loss": 1.2933,
      "step": 195100
    },
    {
      "epoch": 0.6082148745391249,
      "grad_norm": 0.548632800579071,
      "learning_rate": 3.9866825408203654e-05,
      "loss": 1.2668,
      "step": 195150
    },
    {
      "epoch": 0.6083707071997806,
      "grad_norm": 0.7282531261444092,
      "learning_rate": 3.9864228197192726e-05,
      "loss": 1.2889,
      "step": 195200
    },
    {
      "epoch": 0.6085265398604363,
      "grad_norm": 0.703594446182251,
      "learning_rate": 3.9861630986181805e-05,
      "loss": 1.2565,
      "step": 195250
    },
    {
      "epoch": 0.6086823725210919,
      "grad_norm": 0.6189613938331604,
      "learning_rate": 3.985903377517087e-05,
      "loss": 1.262,
      "step": 195300
    },
    {
      "epoch": 0.6088382051817476,
      "grad_norm": 0.5095120668411255,
      "learning_rate": 3.9856436564159944e-05,
      "loss": 1.2225,
      "step": 195350
    },
    {
      "epoch": 0.6089940378424034,
      "grad_norm": 0.6382741332054138,
      "learning_rate": 3.9853839353149016e-05,
      "loss": 1.2933,
      "step": 195400
    },
    {
      "epoch": 0.609149870503059,
      "grad_norm": 0.6147310137748718,
      "learning_rate": 3.985124214213809e-05,
      "loss": 1.2983,
      "step": 195450
    },
    {
      "epoch": 0.6093057031637147,
      "grad_norm": 0.6585391163825989,
      "learning_rate": 3.984864493112716e-05,
      "loss": 1.2362,
      "step": 195500
    },
    {
      "epoch": 0.6094615358243703,
      "grad_norm": 0.6285418272018433,
      "learning_rate": 3.9846047720116234e-05,
      "loss": 1.2794,
      "step": 195550
    },
    {
      "epoch": 0.609617368485026,
      "grad_norm": 0.5061032772064209,
      "learning_rate": 3.9843450509105306e-05,
      "loss": 1.2432,
      "step": 195600
    },
    {
      "epoch": 0.6097732011456817,
      "grad_norm": 0.63034588098526,
      "learning_rate": 3.984085329809438e-05,
      "loss": 1.2617,
      "step": 195650
    },
    {
      "epoch": 0.6099290338063375,
      "grad_norm": 0.6120051145553589,
      "learning_rate": 3.9838256087083444e-05,
      "loss": 1.2435,
      "step": 195700
    },
    {
      "epoch": 0.6100848664669931,
      "grad_norm": 0.9020148515701294,
      "learning_rate": 3.983565887607252e-05,
      "loss": 1.2714,
      "step": 195750
    },
    {
      "epoch": 0.6102406991276488,
      "grad_norm": 0.6779379844665527,
      "learning_rate": 3.9833061665061596e-05,
      "loss": 1.3178,
      "step": 195800
    },
    {
      "epoch": 0.6103965317883044,
      "grad_norm": 0.49714523553848267,
      "learning_rate": 3.983046445405066e-05,
      "loss": 1.2794,
      "step": 195850
    },
    {
      "epoch": 0.6105523644489601,
      "grad_norm": 0.5706663727760315,
      "learning_rate": 3.9827867243039734e-05,
      "loss": 1.2116,
      "step": 195900
    },
    {
      "epoch": 0.6107081971096158,
      "grad_norm": 0.6292635202407837,
      "learning_rate": 3.9825270032028814e-05,
      "loss": 1.2983,
      "step": 195950
    },
    {
      "epoch": 0.6108640297702715,
      "grad_norm": 0.5225825905799866,
      "learning_rate": 3.982267282101788e-05,
      "loss": 1.3252,
      "step": 196000
    },
    {
      "epoch": 0.6110198624309272,
      "grad_norm": 0.5334610939025879,
      "learning_rate": 3.982007561000695e-05,
      "loss": 1.2479,
      "step": 196050
    },
    {
      "epoch": 0.6111756950915829,
      "grad_norm": 0.5535922050476074,
      "learning_rate": 3.9817478398996024e-05,
      "loss": 1.2871,
      "step": 196100
    },
    {
      "epoch": 0.6113315277522385,
      "grad_norm": 0.6549745798110962,
      "learning_rate": 3.98148811879851e-05,
      "loss": 1.2886,
      "step": 196150
    },
    {
      "epoch": 0.6114873604128942,
      "grad_norm": 0.6993521451950073,
      "learning_rate": 3.981228397697417e-05,
      "loss": 1.3039,
      "step": 196200
    },
    {
      "epoch": 0.6116431930735499,
      "grad_norm": 0.6559540629386902,
      "learning_rate": 3.980968676596324e-05,
      "loss": 1.3045,
      "step": 196250
    },
    {
      "epoch": 0.6117990257342055,
      "grad_norm": 0.5605999231338501,
      "learning_rate": 3.980708955495231e-05,
      "loss": 1.234,
      "step": 196300
    },
    {
      "epoch": 0.6119548583948613,
      "grad_norm": 0.6776569485664368,
      "learning_rate": 3.980449234394139e-05,
      "loss": 1.2805,
      "step": 196350
    },
    {
      "epoch": 0.612110691055517,
      "grad_norm": 0.6402667164802551,
      "learning_rate": 3.980189513293045e-05,
      "loss": 1.2733,
      "step": 196400
    },
    {
      "epoch": 0.6122665237161726,
      "grad_norm": 0.585288941860199,
      "learning_rate": 3.9799297921919525e-05,
      "loss": 1.242,
      "step": 196450
    },
    {
      "epoch": 0.6124223563768283,
      "grad_norm": 0.6322082281112671,
      "learning_rate": 3.9796700710908605e-05,
      "loss": 1.288,
      "step": 196500
    },
    {
      "epoch": 0.612578189037484,
      "grad_norm": 0.4800749719142914,
      "learning_rate": 3.979410349989767e-05,
      "loss": 1.2575,
      "step": 196550
    },
    {
      "epoch": 0.6127340216981396,
      "grad_norm": 0.5955564975738525,
      "learning_rate": 3.979150628888674e-05,
      "loss": 1.2528,
      "step": 196600
    },
    {
      "epoch": 0.6128898543587954,
      "grad_norm": 0.5725911855697632,
      "learning_rate": 3.9788909077875815e-05,
      "loss": 1.2735,
      "step": 196650
    },
    {
      "epoch": 0.6130456870194511,
      "grad_norm": 1.138827919960022,
      "learning_rate": 3.978631186686489e-05,
      "loss": 1.3112,
      "step": 196700
    },
    {
      "epoch": 0.6132015196801067,
      "grad_norm": 0.4907785952091217,
      "learning_rate": 3.978371465585396e-05,
      "loss": 1.2529,
      "step": 196750
    },
    {
      "epoch": 0.6133573523407624,
      "grad_norm": 0.5208542346954346,
      "learning_rate": 3.978111744484303e-05,
      "loss": 1.2714,
      "step": 196800
    },
    {
      "epoch": 0.613513185001418,
      "grad_norm": 0.5714495182037354,
      "learning_rate": 3.9778520233832105e-05,
      "loss": 1.2979,
      "step": 196850
    },
    {
      "epoch": 0.6136690176620737,
      "grad_norm": 0.7126333117485046,
      "learning_rate": 3.977592302282118e-05,
      "loss": 1.2749,
      "step": 196900
    },
    {
      "epoch": 0.6138248503227295,
      "grad_norm": 0.6271315813064575,
      "learning_rate": 3.977332581181025e-05,
      "loss": 1.3416,
      "step": 196950
    },
    {
      "epoch": 0.6139806829833852,
      "grad_norm": 0.6214141249656677,
      "learning_rate": 3.9770728600799316e-05,
      "loss": 1.2619,
      "step": 197000
    },
    {
      "epoch": 0.6141365156440408,
      "grad_norm": 0.5653635263442993,
      "learning_rate": 3.9768131389788395e-05,
      "loss": 1.3301,
      "step": 197050
    },
    {
      "epoch": 0.6142923483046965,
      "grad_norm": 0.5944128632545471,
      "learning_rate": 3.976553417877746e-05,
      "loss": 1.2717,
      "step": 197100
    },
    {
      "epoch": 0.6144481809653521,
      "grad_norm": 0.5502679347991943,
      "learning_rate": 3.9762936967766533e-05,
      "loss": 1.2925,
      "step": 197150
    },
    {
      "epoch": 0.6146040136260078,
      "grad_norm": 0.6565899848937988,
      "learning_rate": 3.976033975675561e-05,
      "loss": 1.248,
      "step": 197200
    },
    {
      "epoch": 0.6147598462866636,
      "grad_norm": 0.6076186895370483,
      "learning_rate": 3.975774254574468e-05,
      "loss": 1.2412,
      "step": 197250
    },
    {
      "epoch": 0.6149156789473192,
      "grad_norm": 0.5418685674667358,
      "learning_rate": 3.975514533473375e-05,
      "loss": 1.2785,
      "step": 197300
    },
    {
      "epoch": 0.6150715116079749,
      "grad_norm": 0.6551962494850159,
      "learning_rate": 3.9752548123722824e-05,
      "loss": 1.3324,
      "step": 197350
    },
    {
      "epoch": 0.6152273442686306,
      "grad_norm": 0.5562427043914795,
      "learning_rate": 3.9749950912711896e-05,
      "loss": 1.2708,
      "step": 197400
    },
    {
      "epoch": 0.6153831769292862,
      "grad_norm": 0.7165648937225342,
      "learning_rate": 3.974735370170097e-05,
      "loss": 1.2529,
      "step": 197450
    },
    {
      "epoch": 0.6155390095899419,
      "grad_norm": 0.6154099702835083,
      "learning_rate": 3.974475649069004e-05,
      "loss": 1.2813,
      "step": 197500
    },
    {
      "epoch": 0.6156948422505977,
      "grad_norm": 0.6536914110183716,
      "learning_rate": 3.974215927967911e-05,
      "loss": 1.2723,
      "step": 197550
    },
    {
      "epoch": 0.6158506749112533,
      "grad_norm": 0.5702958106994629,
      "learning_rate": 3.9739562068668186e-05,
      "loss": 1.293,
      "step": 197600
    },
    {
      "epoch": 0.616006507571909,
      "grad_norm": 0.6317130327224731,
      "learning_rate": 3.973696485765725e-05,
      "loss": 1.2948,
      "step": 197650
    },
    {
      "epoch": 0.6161623402325647,
      "grad_norm": 0.585908055305481,
      "learning_rate": 3.9734367646646324e-05,
      "loss": 1.2968,
      "step": 197700
    },
    {
      "epoch": 0.6163181728932203,
      "grad_norm": 0.5058935284614563,
      "learning_rate": 3.9731770435635404e-05,
      "loss": 1.2325,
      "step": 197750
    },
    {
      "epoch": 0.616474005553876,
      "grad_norm": 0.5959802269935608,
      "learning_rate": 3.972917322462447e-05,
      "loss": 1.3105,
      "step": 197800
    },
    {
      "epoch": 0.6166298382145317,
      "grad_norm": 0.5236337184906006,
      "learning_rate": 3.972657601361354e-05,
      "loss": 1.2586,
      "step": 197850
    },
    {
      "epoch": 0.6167856708751874,
      "grad_norm": 0.5830507278442383,
      "learning_rate": 3.9723978802602614e-05,
      "loss": 1.2358,
      "step": 197900
    },
    {
      "epoch": 0.6169415035358431,
      "grad_norm": 0.6140191555023193,
      "learning_rate": 3.972138159159169e-05,
      "loss": 1.2374,
      "step": 197950
    },
    {
      "epoch": 0.6170973361964988,
      "grad_norm": 0.6700617074966431,
      "learning_rate": 3.971878438058076e-05,
      "loss": 1.3252,
      "step": 198000
    },
    {
      "epoch": 0.6172531688571544,
      "grad_norm": 0.5042281746864319,
      "learning_rate": 3.971618716956983e-05,
      "loss": 1.2503,
      "step": 198050
    },
    {
      "epoch": 0.6174090015178101,
      "grad_norm": 0.6348397731781006,
      "learning_rate": 3.9713589958558904e-05,
      "loss": 1.2875,
      "step": 198100
    },
    {
      "epoch": 0.6175648341784657,
      "grad_norm": 0.5349622368812561,
      "learning_rate": 3.971099274754798e-05,
      "loss": 1.2844,
      "step": 198150
    },
    {
      "epoch": 0.6177206668391215,
      "grad_norm": 0.6752257347106934,
      "learning_rate": 3.970839553653705e-05,
      "loss": 1.2929,
      "step": 198200
    },
    {
      "epoch": 0.6178764994997772,
      "grad_norm": 0.6174190044403076,
      "learning_rate": 3.9705798325526115e-05,
      "loss": 1.3099,
      "step": 198250
    },
    {
      "epoch": 0.6180323321604329,
      "grad_norm": 0.6333006620407104,
      "learning_rate": 3.9703201114515194e-05,
      "loss": 1.2786,
      "step": 198300
    },
    {
      "epoch": 0.6181881648210885,
      "grad_norm": 0.5088238716125488,
      "learning_rate": 3.970060390350426e-05,
      "loss": 1.3067,
      "step": 198350
    },
    {
      "epoch": 0.6183439974817442,
      "grad_norm": 0.6264280676841736,
      "learning_rate": 3.969800669249333e-05,
      "loss": 1.282,
      "step": 198400
    },
    {
      "epoch": 0.6184998301423998,
      "grad_norm": 0.5627291798591614,
      "learning_rate": 3.969540948148241e-05,
      "loss": 1.2352,
      "step": 198450
    },
    {
      "epoch": 0.6186556628030556,
      "grad_norm": 0.6426908373832703,
      "learning_rate": 3.969281227047148e-05,
      "loss": 1.3159,
      "step": 198500
    },
    {
      "epoch": 0.6188114954637113,
      "grad_norm": 0.5678166151046753,
      "learning_rate": 3.969021505946055e-05,
      "loss": 1.2621,
      "step": 198550
    },
    {
      "epoch": 0.6189673281243669,
      "grad_norm": 0.7099789381027222,
      "learning_rate": 3.968761784844962e-05,
      "loss": 1.235,
      "step": 198600
    },
    {
      "epoch": 0.6191231607850226,
      "grad_norm": 0.63734370470047,
      "learning_rate": 3.9685020637438695e-05,
      "loss": 1.3058,
      "step": 198650
    },
    {
      "epoch": 0.6192789934456783,
      "grad_norm": 0.6252632141113281,
      "learning_rate": 3.968242342642777e-05,
      "loss": 1.2679,
      "step": 198700
    },
    {
      "epoch": 0.6194348261063339,
      "grad_norm": 0.45675596594810486,
      "learning_rate": 3.967982621541684e-05,
      "loss": 1.2837,
      "step": 198750
    },
    {
      "epoch": 0.6195906587669897,
      "grad_norm": 0.5438093543052673,
      "learning_rate": 3.9677229004405906e-05,
      "loss": 1.2462,
      "step": 198800
    },
    {
      "epoch": 0.6197464914276454,
      "grad_norm": 0.6062216758728027,
      "learning_rate": 3.9674631793394985e-05,
      "loss": 1.2579,
      "step": 198850
    },
    {
      "epoch": 0.619902324088301,
      "grad_norm": 0.6211130619049072,
      "learning_rate": 3.967203458238406e-05,
      "loss": 1.2231,
      "step": 198900
    },
    {
      "epoch": 0.6200581567489567,
      "grad_norm": 0.6646173000335693,
      "learning_rate": 3.966943737137312e-05,
      "loss": 1.2733,
      "step": 198950
    },
    {
      "epoch": 0.6202139894096124,
      "grad_norm": 0.5136886835098267,
      "learning_rate": 3.96668401603622e-05,
      "loss": 1.2367,
      "step": 199000
    },
    {
      "epoch": 0.620369822070268,
      "grad_norm": 0.7157886028289795,
      "learning_rate": 3.966424294935127e-05,
      "loss": 1.2878,
      "step": 199050
    },
    {
      "epoch": 0.6205256547309237,
      "grad_norm": 0.5566164255142212,
      "learning_rate": 3.966164573834034e-05,
      "loss": 1.2613,
      "step": 199100
    },
    {
      "epoch": 0.6206814873915795,
      "grad_norm": 0.828605055809021,
      "learning_rate": 3.965904852732941e-05,
      "loss": 1.3129,
      "step": 199150
    },
    {
      "epoch": 0.6208373200522351,
      "grad_norm": 0.5712503790855408,
      "learning_rate": 3.9656451316318486e-05,
      "loss": 1.2646,
      "step": 199200
    },
    {
      "epoch": 0.6209931527128908,
      "grad_norm": 0.6140094995498657,
      "learning_rate": 3.965385410530756e-05,
      "loss": 1.23,
      "step": 199250
    },
    {
      "epoch": 0.6211489853735465,
      "grad_norm": 0.6232287883758545,
      "learning_rate": 3.965125689429663e-05,
      "loss": 1.2798,
      "step": 199300
    },
    {
      "epoch": 0.6213048180342021,
      "grad_norm": 0.5183936953544617,
      "learning_rate": 3.96486596832857e-05,
      "loss": 1.3031,
      "step": 199350
    },
    {
      "epoch": 0.6214606506948578,
      "grad_norm": 0.6251088976860046,
      "learning_rate": 3.9646062472274776e-05,
      "loss": 1.2483,
      "step": 199400
    },
    {
      "epoch": 0.6216164833555136,
      "grad_norm": 0.5780421495437622,
      "learning_rate": 3.964351720548407e-05,
      "loss": 1.2305,
      "step": 199450
    },
    {
      "epoch": 0.6217723160161692,
      "grad_norm": 0.6667070388793945,
      "learning_rate": 3.9640919994473134e-05,
      "loss": 1.3044,
      "step": 199500
    },
    {
      "epoch": 0.6219281486768249,
      "grad_norm": 0.5743460655212402,
      "learning_rate": 3.9638322783462206e-05,
      "loss": 1.2735,
      "step": 199550
    },
    {
      "epoch": 0.6220839813374806,
      "grad_norm": 0.5573583841323853,
      "learning_rate": 3.9635725572451285e-05,
      "loss": 1.2528,
      "step": 199600
    },
    {
      "epoch": 0.6222398139981362,
      "grad_norm": 0.6483930945396423,
      "learning_rate": 3.963312836144035e-05,
      "loss": 1.2103,
      "step": 199650
    },
    {
      "epoch": 0.6223956466587919,
      "grad_norm": 0.46142882108688354,
      "learning_rate": 3.9630531150429424e-05,
      "loss": 1.319,
      "step": 199700
    },
    {
      "epoch": 0.6225514793194477,
      "grad_norm": 0.6746532320976257,
      "learning_rate": 3.9627933939418496e-05,
      "loss": 1.2601,
      "step": 199750
    },
    {
      "epoch": 0.6227073119801033,
      "grad_norm": 0.6180123090744019,
      "learning_rate": 3.962533672840757e-05,
      "loss": 1.2713,
      "step": 199800
    },
    {
      "epoch": 0.622863144640759,
      "grad_norm": 0.555161714553833,
      "learning_rate": 3.962273951739664e-05,
      "loss": 1.2739,
      "step": 199850
    },
    {
      "epoch": 0.6230189773014146,
      "grad_norm": 0.49862220883369446,
      "learning_rate": 3.962014230638571e-05,
      "loss": 1.2685,
      "step": 199900
    },
    {
      "epoch": 0.6231748099620703,
      "grad_norm": 0.6597772240638733,
      "learning_rate": 3.9617545095374786e-05,
      "loss": 1.2577,
      "step": 199950
    },
    {
      "epoch": 0.623330642622726,
      "grad_norm": 0.5325880646705627,
      "learning_rate": 3.961494788436386e-05,
      "loss": 1.2403,
      "step": 200000
    },
    {
      "epoch": 0.6234864752833817,
      "grad_norm": 0.6043215990066528,
      "learning_rate": 3.9612350673352924e-05,
      "loss": 1.2694,
      "step": 200050
    },
    {
      "epoch": 0.6236423079440374,
      "grad_norm": 0.5508672595024109,
      "learning_rate": 3.9609753462342004e-05,
      "loss": 1.2536,
      "step": 200100
    },
    {
      "epoch": 0.6237981406046931,
      "grad_norm": 0.6790350675582886,
      "learning_rate": 3.9607156251331076e-05,
      "loss": 1.27,
      "step": 200150
    },
    {
      "epoch": 0.6239539732653487,
      "grad_norm": 0.5107483267784119,
      "learning_rate": 3.960455904032014e-05,
      "loss": 1.2652,
      "step": 200200
    },
    {
      "epoch": 0.6241098059260044,
      "grad_norm": 0.6092932224273682,
      "learning_rate": 3.9601961829309214e-05,
      "loss": 1.2383,
      "step": 200250
    },
    {
      "epoch": 0.6242656385866601,
      "grad_norm": 0.690166175365448,
      "learning_rate": 3.959936461829829e-05,
      "loss": 1.3175,
      "step": 200300
    },
    {
      "epoch": 0.6244214712473157,
      "grad_norm": 0.6870782971382141,
      "learning_rate": 3.959676740728736e-05,
      "loss": 1.3289,
      "step": 200350
    },
    {
      "epoch": 0.6245773039079715,
      "grad_norm": 0.6624361276626587,
      "learning_rate": 3.959417019627643e-05,
      "loss": 1.252,
      "step": 200400
    },
    {
      "epoch": 0.6247331365686272,
      "grad_norm": 0.5276787281036377,
      "learning_rate": 3.9591572985265504e-05,
      "loss": 1.2638,
      "step": 200450
    },
    {
      "epoch": 0.6248889692292828,
      "grad_norm": 0.4799824059009552,
      "learning_rate": 3.958897577425458e-05,
      "loss": 1.2638,
      "step": 200500
    },
    {
      "epoch": 0.6250448018899385,
      "grad_norm": 0.5762313008308411,
      "learning_rate": 3.958637856324365e-05,
      "loss": 1.2531,
      "step": 200550
    },
    {
      "epoch": 0.6252006345505942,
      "grad_norm": 0.6287556886672974,
      "learning_rate": 3.9583781352232715e-05,
      "loss": 1.2951,
      "step": 200600
    },
    {
      "epoch": 0.6253564672112498,
      "grad_norm": 0.4663707911968231,
      "learning_rate": 3.9581184141221794e-05,
      "loss": 1.2527,
      "step": 200650
    },
    {
      "epoch": 0.6255122998719056,
      "grad_norm": 0.48191192746162415,
      "learning_rate": 3.957858693021087e-05,
      "loss": 1.3019,
      "step": 200700
    },
    {
      "epoch": 0.6256681325325613,
      "grad_norm": 0.5542537569999695,
      "learning_rate": 3.957598971919993e-05,
      "loss": 1.2668,
      "step": 200750
    },
    {
      "epoch": 0.6258239651932169,
      "grad_norm": 0.6086219549179077,
      "learning_rate": 3.9573392508189005e-05,
      "loss": 1.3064,
      "step": 200800
    },
    {
      "epoch": 0.6259797978538726,
      "grad_norm": 0.4936094582080841,
      "learning_rate": 3.9570795297178084e-05,
      "loss": 1.2389,
      "step": 200850
    },
    {
      "epoch": 0.6261356305145283,
      "grad_norm": 0.729391872882843,
      "learning_rate": 3.956819808616715e-05,
      "loss": 1.2264,
      "step": 200900
    },
    {
      "epoch": 0.6262914631751839,
      "grad_norm": 0.67300945520401,
      "learning_rate": 3.956560087515622e-05,
      "loss": 1.2887,
      "step": 200950
    },
    {
      "epoch": 0.6264472958358397,
      "grad_norm": 0.5072035789489746,
      "learning_rate": 3.9563003664145295e-05,
      "loss": 1.3146,
      "step": 201000
    },
    {
      "epoch": 0.6266031284964954,
      "grad_norm": 0.7310717701911926,
      "learning_rate": 3.956040645313437e-05,
      "loss": 1.3281,
      "step": 201050
    },
    {
      "epoch": 0.626758961157151,
      "grad_norm": 0.5221379995346069,
      "learning_rate": 3.955780924212344e-05,
      "loss": 1.316,
      "step": 201100
    },
    {
      "epoch": 0.6269147938178067,
      "grad_norm": 0.6292473673820496,
      "learning_rate": 3.955521203111251e-05,
      "loss": 1.294,
      "step": 201150
    },
    {
      "epoch": 0.6270706264784623,
      "grad_norm": 0.6689508557319641,
      "learning_rate": 3.9552614820101585e-05,
      "loss": 1.2547,
      "step": 201200
    },
    {
      "epoch": 0.627226459139118,
      "grad_norm": 0.5942477583885193,
      "learning_rate": 3.955001760909066e-05,
      "loss": 1.2919,
      "step": 201250
    },
    {
      "epoch": 0.6273822917997738,
      "grad_norm": 0.6301725506782532,
      "learning_rate": 3.9547420398079723e-05,
      "loss": 1.2818,
      "step": 201300
    },
    {
      "epoch": 0.6275381244604294,
      "grad_norm": 0.5978614687919617,
      "learning_rate": 3.95448231870688e-05,
      "loss": 1.2621,
      "step": 201350
    },
    {
      "epoch": 0.6276939571210851,
      "grad_norm": 0.6071933507919312,
      "learning_rate": 3.9542225976057875e-05,
      "loss": 1.2909,
      "step": 201400
    },
    {
      "epoch": 0.6278497897817408,
      "grad_norm": 0.632492184638977,
      "learning_rate": 3.953962876504694e-05,
      "loss": 1.2389,
      "step": 201450
    },
    {
      "epoch": 0.6280056224423964,
      "grad_norm": 0.5972146987915039,
      "learning_rate": 3.9537031554036013e-05,
      "loss": 1.2532,
      "step": 201500
    },
    {
      "epoch": 0.6281614551030521,
      "grad_norm": 0.5186383724212646,
      "learning_rate": 3.953443434302509e-05,
      "loss": 1.2918,
      "step": 201550
    },
    {
      "epoch": 0.6283172877637078,
      "grad_norm": 0.5597660541534424,
      "learning_rate": 3.953183713201416e-05,
      "loss": 1.2475,
      "step": 201600
    },
    {
      "epoch": 0.6284731204243635,
      "grad_norm": 0.6767000555992126,
      "learning_rate": 3.952923992100323e-05,
      "loss": 1.1947,
      "step": 201650
    },
    {
      "epoch": 0.6286289530850192,
      "grad_norm": 0.6047422885894775,
      "learning_rate": 3.9526642709992303e-05,
      "loss": 1.222,
      "step": 201700
    },
    {
      "epoch": 0.6287847857456749,
      "grad_norm": 0.5772911906242371,
      "learning_rate": 3.9524045498981376e-05,
      "loss": 1.2414,
      "step": 201750
    },
    {
      "epoch": 0.6289406184063305,
      "grad_norm": 0.4911171793937683,
      "learning_rate": 3.952144828797045e-05,
      "loss": 1.2705,
      "step": 201800
    },
    {
      "epoch": 0.6290964510669862,
      "grad_norm": 0.5704007148742676,
      "learning_rate": 3.951885107695952e-05,
      "loss": 1.2721,
      "step": 201850
    },
    {
      "epoch": 0.6292522837276419,
      "grad_norm": 0.551872193813324,
      "learning_rate": 3.9516253865948593e-05,
      "loss": 1.2976,
      "step": 201900
    },
    {
      "epoch": 0.6294081163882976,
      "grad_norm": 0.5389552712440491,
      "learning_rate": 3.9513656654937666e-05,
      "loss": 1.2723,
      "step": 201950
    },
    {
      "epoch": 0.6295639490489533,
      "grad_norm": 0.6242775321006775,
      "learning_rate": 3.951105944392673e-05,
      "loss": 1.2697,
      "step": 202000
    },
    {
      "epoch": 0.629719781709609,
      "grad_norm": 0.6233567595481873,
      "learning_rate": 3.9508462232915804e-05,
      "loss": 1.3168,
      "step": 202050
    },
    {
      "epoch": 0.6298756143702646,
      "grad_norm": 0.6395180225372314,
      "learning_rate": 3.9505865021904883e-05,
      "loss": 1.2071,
      "step": 202100
    },
    {
      "epoch": 0.6300314470309203,
      "grad_norm": 0.5201868414878845,
      "learning_rate": 3.950326781089395e-05,
      "loss": 1.3049,
      "step": 202150
    },
    {
      "epoch": 0.630187279691576,
      "grad_norm": 0.6126246452331543,
      "learning_rate": 3.950067059988302e-05,
      "loss": 1.2247,
      "step": 202200
    },
    {
      "epoch": 0.6303431123522317,
      "grad_norm": 0.6137568354606628,
      "learning_rate": 3.94980733888721e-05,
      "loss": 1.289,
      "step": 202250
    },
    {
      "epoch": 0.6304989450128874,
      "grad_norm": 0.5840105414390564,
      "learning_rate": 3.949547617786117e-05,
      "loss": 1.2806,
      "step": 202300
    },
    {
      "epoch": 0.630654777673543,
      "grad_norm": 0.5868103504180908,
      "learning_rate": 3.949287896685024e-05,
      "loss": 1.2819,
      "step": 202350
    },
    {
      "epoch": 0.6308106103341987,
      "grad_norm": 0.5996014475822449,
      "learning_rate": 3.949028175583931e-05,
      "loss": 1.2634,
      "step": 202400
    },
    {
      "epoch": 0.6309664429948544,
      "grad_norm": 0.551362156867981,
      "learning_rate": 3.9487684544828384e-05,
      "loss": 1.254,
      "step": 202450
    },
    {
      "epoch": 0.63112227565551,
      "grad_norm": 0.6621419787406921,
      "learning_rate": 3.948508733381746e-05,
      "loss": 1.2557,
      "step": 202500
    },
    {
      "epoch": 0.6312781083161658,
      "grad_norm": 0.6607682704925537,
      "learning_rate": 3.948249012280652e-05,
      "loss": 1.2818,
      "step": 202550
    },
    {
      "epoch": 0.6314339409768215,
      "grad_norm": 0.5941862463951111,
      "learning_rate": 3.94798929117956e-05,
      "loss": 1.2417,
      "step": 202600
    },
    {
      "epoch": 0.6315897736374771,
      "grad_norm": 0.6585790514945984,
      "learning_rate": 3.9477295700784674e-05,
      "loss": 1.2544,
      "step": 202650
    },
    {
      "epoch": 0.6317456062981328,
      "grad_norm": 0.554783284664154,
      "learning_rate": 3.947469848977374e-05,
      "loss": 1.2617,
      "step": 202700
    },
    {
      "epoch": 0.6319014389587885,
      "grad_norm": 0.6020936965942383,
      "learning_rate": 3.947210127876281e-05,
      "loss": 1.2521,
      "step": 202750
    },
    {
      "epoch": 0.6320572716194441,
      "grad_norm": 0.6786975264549255,
      "learning_rate": 3.946950406775189e-05,
      "loss": 1.1861,
      "step": 202800
    },
    {
      "epoch": 0.6322131042800998,
      "grad_norm": 0.6468305587768555,
      "learning_rate": 3.946690685674096e-05,
      "loss": 1.2632,
      "step": 202850
    },
    {
      "epoch": 0.6323689369407556,
      "grad_norm": 0.5554978251457214,
      "learning_rate": 3.946430964573003e-05,
      "loss": 1.2505,
      "step": 202900
    },
    {
      "epoch": 0.6325247696014112,
      "grad_norm": 0.5406360626220703,
      "learning_rate": 3.94617124347191e-05,
      "loss": 1.3021,
      "step": 202950
    },
    {
      "epoch": 0.6326806022620669,
      "grad_norm": 0.48198559880256653,
      "learning_rate": 3.9459115223708175e-05,
      "loss": 1.2339,
      "step": 203000
    },
    {
      "epoch": 0.6328364349227226,
      "grad_norm": 0.6152166724205017,
      "learning_rate": 3.945651801269725e-05,
      "loss": 1.2818,
      "step": 203050
    },
    {
      "epoch": 0.6329922675833782,
      "grad_norm": 0.5060771703720093,
      "learning_rate": 3.945392080168632e-05,
      "loss": 1.2423,
      "step": 203100
    },
    {
      "epoch": 0.6331481002440339,
      "grad_norm": 0.5323744416236877,
      "learning_rate": 3.945132359067539e-05,
      "loss": 1.1883,
      "step": 203150
    },
    {
      "epoch": 0.6333039329046897,
      "grad_norm": 0.6167501211166382,
      "learning_rate": 3.9448726379664465e-05,
      "loss": 1.2959,
      "step": 203200
    },
    {
      "epoch": 0.6334597655653453,
      "grad_norm": 0.5765788555145264,
      "learning_rate": 3.944612916865353e-05,
      "loss": 1.2514,
      "step": 203250
    },
    {
      "epoch": 0.633615598226001,
      "grad_norm": 0.4971885085105896,
      "learning_rate": 3.94435319576426e-05,
      "loss": 1.2791,
      "step": 203300
    },
    {
      "epoch": 0.6337714308866567,
      "grad_norm": 0.8058585524559021,
      "learning_rate": 3.944093474663168e-05,
      "loss": 1.2941,
      "step": 203350
    },
    {
      "epoch": 0.6339272635473123,
      "grad_norm": 0.6116172671318054,
      "learning_rate": 3.943833753562075e-05,
      "loss": 1.2808,
      "step": 203400
    },
    {
      "epoch": 0.634083096207968,
      "grad_norm": 0.7258458137512207,
      "learning_rate": 3.943579226883004e-05,
      "loss": 1.2691,
      "step": 203450
    },
    {
      "epoch": 0.6342389288686238,
      "grad_norm": 0.6165646910667419,
      "learning_rate": 3.943319505781911e-05,
      "loss": 1.3024,
      "step": 203500
    },
    {
      "epoch": 0.6343947615292794,
      "grad_norm": 0.6558949947357178,
      "learning_rate": 3.9430597846808185e-05,
      "loss": 1.3323,
      "step": 203550
    },
    {
      "epoch": 0.6345505941899351,
      "grad_norm": 0.5975173711776733,
      "learning_rate": 3.942800063579726e-05,
      "loss": 1.2802,
      "step": 203600
    },
    {
      "epoch": 0.6347064268505908,
      "grad_norm": 0.5510978698730469,
      "learning_rate": 3.942540342478633e-05,
      "loss": 1.2617,
      "step": 203650
    },
    {
      "epoch": 0.6348622595112464,
      "grad_norm": 0.5242619514465332,
      "learning_rate": 3.94228062137754e-05,
      "loss": 1.1923,
      "step": 203700
    },
    {
      "epoch": 0.6350180921719021,
      "grad_norm": 0.5215773582458496,
      "learning_rate": 3.9420209002764475e-05,
      "loss": 1.3244,
      "step": 203750
    },
    {
      "epoch": 0.6351739248325579,
      "grad_norm": 0.5374786257743835,
      "learning_rate": 3.941761179175355e-05,
      "loss": 1.3303,
      "step": 203800
    },
    {
      "epoch": 0.6353297574932135,
      "grad_norm": 0.7214325070381165,
      "learning_rate": 3.9415014580742614e-05,
      "loss": 1.2929,
      "step": 203850
    },
    {
      "epoch": 0.6354855901538692,
      "grad_norm": 0.6232470870018005,
      "learning_rate": 3.941241736973169e-05,
      "loss": 1.2111,
      "step": 203900
    },
    {
      "epoch": 0.6356414228145248,
      "grad_norm": 0.5870875716209412,
      "learning_rate": 3.940982015872076e-05,
      "loss": 1.2973,
      "step": 203950
    },
    {
      "epoch": 0.6357972554751805,
      "grad_norm": 0.6056302785873413,
      "learning_rate": 3.940722294770983e-05,
      "loss": 1.2892,
      "step": 204000
    },
    {
      "epoch": 0.6359530881358362,
      "grad_norm": 0.6498371362686157,
      "learning_rate": 3.9404625736698904e-05,
      "loss": 1.3068,
      "step": 204050
    },
    {
      "epoch": 0.636108920796492,
      "grad_norm": 0.5802509784698486,
      "learning_rate": 3.9402028525687976e-05,
      "loss": 1.2726,
      "step": 204100
    },
    {
      "epoch": 0.6362647534571476,
      "grad_norm": 0.6193143725395203,
      "learning_rate": 3.939943131467705e-05,
      "loss": 1.2561,
      "step": 204150
    },
    {
      "epoch": 0.6364205861178033,
      "grad_norm": 0.5536234974861145,
      "learning_rate": 3.939683410366612e-05,
      "loss": 1.2738,
      "step": 204200
    },
    {
      "epoch": 0.6365764187784589,
      "grad_norm": 0.5579662919044495,
      "learning_rate": 3.9394236892655194e-05,
      "loss": 1.283,
      "step": 204250
    },
    {
      "epoch": 0.6367322514391146,
      "grad_norm": 0.5094005465507507,
      "learning_rate": 3.9391639681644266e-05,
      "loss": 1.2843,
      "step": 204300
    },
    {
      "epoch": 0.6368880840997703,
      "grad_norm": 0.5723372101783752,
      "learning_rate": 3.938904247063334e-05,
      "loss": 1.2673,
      "step": 204350
    },
    {
      "epoch": 0.6370439167604259,
      "grad_norm": 0.46786513924598694,
      "learning_rate": 3.9386445259622404e-05,
      "loss": 1.2711,
      "step": 204400
    },
    {
      "epoch": 0.6371997494210817,
      "grad_norm": 0.603874146938324,
      "learning_rate": 3.9383848048611484e-05,
      "loss": 1.3014,
      "step": 204450
    },
    {
      "epoch": 0.6373555820817374,
      "grad_norm": 0.6491502523422241,
      "learning_rate": 3.9381250837600556e-05,
      "loss": 1.2823,
      "step": 204500
    },
    {
      "epoch": 0.637511414742393,
      "grad_norm": 0.5439231991767883,
      "learning_rate": 3.937865362658962e-05,
      "loss": 1.2693,
      "step": 204550
    },
    {
      "epoch": 0.6376672474030487,
      "grad_norm": 0.4670325219631195,
      "learning_rate": 3.93760564155787e-05,
      "loss": 1.2575,
      "step": 204600
    },
    {
      "epoch": 0.6378230800637044,
      "grad_norm": 0.6245667934417725,
      "learning_rate": 3.937345920456777e-05,
      "loss": 1.2949,
      "step": 204650
    },
    {
      "epoch": 0.63797891272436,
      "grad_norm": 0.6413573622703552,
      "learning_rate": 3.937086199355684e-05,
      "loss": 1.3063,
      "step": 204700
    },
    {
      "epoch": 0.6381347453850158,
      "grad_norm": 0.6173974275588989,
      "learning_rate": 3.936826478254591e-05,
      "loss": 1.2843,
      "step": 204750
    },
    {
      "epoch": 0.6382905780456715,
      "grad_norm": 0.6899098753929138,
      "learning_rate": 3.93657195157552e-05,
      "loss": 1.2913,
      "step": 204800
    },
    {
      "epoch": 0.6384464107063271,
      "grad_norm": 0.5147418975830078,
      "learning_rate": 3.9363122304744277e-05,
      "loss": 1.2405,
      "step": 204850
    },
    {
      "epoch": 0.6386022433669828,
      "grad_norm": 0.509881317615509,
      "learning_rate": 3.936052509373335e-05,
      "loss": 1.2523,
      "step": 204900
    },
    {
      "epoch": 0.6387580760276385,
      "grad_norm": 0.6211353540420532,
      "learning_rate": 3.9357927882722415e-05,
      "loss": 1.2397,
      "step": 204950
    },
    {
      "epoch": 0.6389139086882941,
      "grad_norm": 0.6721922159194946,
      "learning_rate": 3.9355330671711494e-05,
      "loss": 1.3422,
      "step": 205000
    },
    {
      "epoch": 0.6390697413489499,
      "grad_norm": 0.6057949662208557,
      "learning_rate": 3.9352733460700567e-05,
      "loss": 1.2481,
      "step": 205050
    },
    {
      "epoch": 0.6392255740096056,
      "grad_norm": 0.6378331184387207,
      "learning_rate": 3.935013624968963e-05,
      "loss": 1.2714,
      "step": 205100
    },
    {
      "epoch": 0.6393814066702612,
      "grad_norm": 0.5963140726089478,
      "learning_rate": 3.9347539038678705e-05,
      "loss": 1.2366,
      "step": 205150
    },
    {
      "epoch": 0.6395372393309169,
      "grad_norm": 0.6618852019309998,
      "learning_rate": 3.934494182766778e-05,
      "loss": 1.2793,
      "step": 205200
    },
    {
      "epoch": 0.6396930719915725,
      "grad_norm": 0.6779752373695374,
      "learning_rate": 3.934234461665685e-05,
      "loss": 1.2592,
      "step": 205250
    },
    {
      "epoch": 0.6398489046522282,
      "grad_norm": 0.5970356464385986,
      "learning_rate": 3.933974740564592e-05,
      "loss": 1.2371,
      "step": 205300
    },
    {
      "epoch": 0.640004737312884,
      "grad_norm": 0.6301303505897522,
      "learning_rate": 3.9337150194634995e-05,
      "loss": 1.3088,
      "step": 205350
    },
    {
      "epoch": 0.6401605699735397,
      "grad_norm": 0.46891602873802185,
      "learning_rate": 3.933455298362407e-05,
      "loss": 1.2922,
      "step": 205400
    },
    {
      "epoch": 0.6403164026341953,
      "grad_norm": 0.612839937210083,
      "learning_rate": 3.933195577261314e-05,
      "loss": 1.2767,
      "step": 205450
    },
    {
      "epoch": 0.640472235294851,
      "grad_norm": 0.6466915011405945,
      "learning_rate": 3.9329358561602206e-05,
      "loss": 1.2835,
      "step": 205500
    },
    {
      "epoch": 0.6406280679555066,
      "grad_norm": 0.6279035806655884,
      "learning_rate": 3.9326761350591285e-05,
      "loss": 1.2868,
      "step": 205550
    },
    {
      "epoch": 0.6407839006161623,
      "grad_norm": 0.7907896637916565,
      "learning_rate": 3.932416413958036e-05,
      "loss": 1.2606,
      "step": 205600
    },
    {
      "epoch": 0.640939733276818,
      "grad_norm": 0.6087379455566406,
      "learning_rate": 3.932156692856942e-05,
      "loss": 1.2571,
      "step": 205650
    },
    {
      "epoch": 0.6410955659374737,
      "grad_norm": 0.6112149357795715,
      "learning_rate": 3.93189697175585e-05,
      "loss": 1.2776,
      "step": 205700
    },
    {
      "epoch": 0.6412513985981294,
      "grad_norm": 0.599153995513916,
      "learning_rate": 3.9316372506547575e-05,
      "loss": 1.262,
      "step": 205750
    },
    {
      "epoch": 0.6414072312587851,
      "grad_norm": 0.4207738935947418,
      "learning_rate": 3.931377529553664e-05,
      "loss": 1.2368,
      "step": 205800
    },
    {
      "epoch": 0.6415630639194407,
      "grad_norm": 0.5393846035003662,
      "learning_rate": 3.931117808452571e-05,
      "loss": 1.1622,
      "step": 205850
    },
    {
      "epoch": 0.6417188965800964,
      "grad_norm": 0.7007930874824524,
      "learning_rate": 3.9308580873514786e-05,
      "loss": 1.2889,
      "step": 205900
    },
    {
      "epoch": 0.6418747292407521,
      "grad_norm": 0.5132570862770081,
      "learning_rate": 3.930598366250386e-05,
      "loss": 1.2502,
      "step": 205950
    },
    {
      "epoch": 0.6420305619014078,
      "grad_norm": 0.6645498275756836,
      "learning_rate": 3.930338645149293e-05,
      "loss": 1.2787,
      "step": 206000
    },
    {
      "epoch": 0.6421863945620635,
      "grad_norm": 0.5593359470367432,
      "learning_rate": 3.9300789240482e-05,
      "loss": 1.1996,
      "step": 206050
    },
    {
      "epoch": 0.6423422272227192,
      "grad_norm": 0.45760175585746765,
      "learning_rate": 3.9298192029471076e-05,
      "loss": 1.2865,
      "step": 206100
    },
    {
      "epoch": 0.6424980598833748,
      "grad_norm": 0.5597137212753296,
      "learning_rate": 3.929559481846015e-05,
      "loss": 1.2407,
      "step": 206150
    },
    {
      "epoch": 0.6426538925440305,
      "grad_norm": 0.6437963247299194,
      "learning_rate": 3.9292997607449214e-05,
      "loss": 1.3024,
      "step": 206200
    },
    {
      "epoch": 0.6428097252046862,
      "grad_norm": 0.5463171601295471,
      "learning_rate": 3.929040039643829e-05,
      "loss": 1.325,
      "step": 206250
    },
    {
      "epoch": 0.6429655578653419,
      "grad_norm": 0.5051403045654297,
      "learning_rate": 3.9287803185427366e-05,
      "loss": 1.2544,
      "step": 206300
    },
    {
      "epoch": 0.6431213905259976,
      "grad_norm": 0.47887519001960754,
      "learning_rate": 3.928520597441643e-05,
      "loss": 1.2836,
      "step": 206350
    },
    {
      "epoch": 0.6432772231866533,
      "grad_norm": 0.7331908941268921,
      "learning_rate": 3.9282608763405504e-05,
      "loss": 1.2641,
      "step": 206400
    },
    {
      "epoch": 0.6434330558473089,
      "grad_norm": 0.4622064232826233,
      "learning_rate": 3.928001155239458e-05,
      "loss": 1.2723,
      "step": 206450
    },
    {
      "epoch": 0.6435888885079646,
      "grad_norm": 0.5678207874298096,
      "learning_rate": 3.927741434138365e-05,
      "loss": 1.2623,
      "step": 206500
    },
    {
      "epoch": 0.6437447211686202,
      "grad_norm": 0.7286996841430664,
      "learning_rate": 3.927481713037272e-05,
      "loss": 1.2837,
      "step": 206550
    },
    {
      "epoch": 0.643900553829276,
      "grad_norm": 0.6768404245376587,
      "learning_rate": 3.9272219919361794e-05,
      "loss": 1.271,
      "step": 206600
    },
    {
      "epoch": 0.6440563864899317,
      "grad_norm": 0.5279819369316101,
      "learning_rate": 3.9269622708350866e-05,
      "loss": 1.2724,
      "step": 206650
    },
    {
      "epoch": 0.6442122191505874,
      "grad_norm": 0.6049864292144775,
      "learning_rate": 3.926702549733994e-05,
      "loss": 1.3234,
      "step": 206700
    },
    {
      "epoch": 0.644368051811243,
      "grad_norm": 0.570832371711731,
      "learning_rate": 3.926442828632901e-05,
      "loss": 1.2425,
      "step": 206750
    },
    {
      "epoch": 0.6445238844718987,
      "grad_norm": 0.5884249210357666,
      "learning_rate": 3.9261831075318084e-05,
      "loss": 1.2047,
      "step": 206800
    },
    {
      "epoch": 0.6446797171325543,
      "grad_norm": 0.6150093078613281,
      "learning_rate": 3.9259233864307156e-05,
      "loss": 1.2516,
      "step": 206850
    },
    {
      "epoch": 0.64483554979321,
      "grad_norm": 0.5608228445053101,
      "learning_rate": 3.925663665329622e-05,
      "loss": 1.3277,
      "step": 206900
    },
    {
      "epoch": 0.6449913824538658,
      "grad_norm": 0.6622130274772644,
      "learning_rate": 3.92540394422853e-05,
      "loss": 1.262,
      "step": 206950
    },
    {
      "epoch": 0.6451472151145214,
      "grad_norm": 0.565123438835144,
      "learning_rate": 3.9251494175494593e-05,
      "loss": 1.2961,
      "step": 207000
    },
    {
      "epoch": 0.6453030477751771,
      "grad_norm": 0.7196008563041687,
      "learning_rate": 3.924889696448366e-05,
      "loss": 1.222,
      "step": 207050
    },
    {
      "epoch": 0.6454588804358328,
      "grad_norm": 0.6471740007400513,
      "learning_rate": 3.924629975347273e-05,
      "loss": 1.2571,
      "step": 207100
    },
    {
      "epoch": 0.6456147130964884,
      "grad_norm": 0.8304535746574402,
      "learning_rate": 3.9243702542461804e-05,
      "loss": 1.2413,
      "step": 207150
    },
    {
      "epoch": 0.6457705457571441,
      "grad_norm": 0.569484293460846,
      "learning_rate": 3.924110533145088e-05,
      "loss": 1.2206,
      "step": 207200
    },
    {
      "epoch": 0.6459263784177999,
      "grad_norm": 0.7108138799667358,
      "learning_rate": 3.923850812043995e-05,
      "loss": 1.2376,
      "step": 207250
    },
    {
      "epoch": 0.6460822110784555,
      "grad_norm": 0.6270695328712463,
      "learning_rate": 3.923591090942902e-05,
      "loss": 1.2419,
      "step": 207300
    },
    {
      "epoch": 0.6462380437391112,
      "grad_norm": 0.5430042743682861,
      "learning_rate": 3.9233313698418094e-05,
      "loss": 1.3249,
      "step": 207350
    },
    {
      "epoch": 0.6463938763997669,
      "grad_norm": 0.48256292939186096,
      "learning_rate": 3.923071648740717e-05,
      "loss": 1.218,
      "step": 207400
    },
    {
      "epoch": 0.6465497090604225,
      "grad_norm": 0.629317581653595,
      "learning_rate": 3.922811927639623e-05,
      "loss": 1.2603,
      "step": 207450
    },
    {
      "epoch": 0.6467055417210782,
      "grad_norm": 0.5231419205665588,
      "learning_rate": 3.9225522065385305e-05,
      "loss": 1.2618,
      "step": 207500
    },
    {
      "epoch": 0.646861374381734,
      "grad_norm": 0.5909111499786377,
      "learning_rate": 3.9222924854374384e-05,
      "loss": 1.2942,
      "step": 207550
    },
    {
      "epoch": 0.6470172070423896,
      "grad_norm": 0.6928557753562927,
      "learning_rate": 3.922032764336345e-05,
      "loss": 1.2684,
      "step": 207600
    },
    {
      "epoch": 0.6471730397030453,
      "grad_norm": 0.60357666015625,
      "learning_rate": 3.921773043235252e-05,
      "loss": 1.2144,
      "step": 207650
    },
    {
      "epoch": 0.647328872363701,
      "grad_norm": 0.4877378046512604,
      "learning_rate": 3.92151332213416e-05,
      "loss": 1.2585,
      "step": 207700
    },
    {
      "epoch": 0.6474847050243566,
      "grad_norm": 0.719833493232727,
      "learning_rate": 3.921253601033067e-05,
      "loss": 1.2727,
      "step": 207750
    },
    {
      "epoch": 0.6476405376850123,
      "grad_norm": 0.588668942451477,
      "learning_rate": 3.920993879931974e-05,
      "loss": 1.3458,
      "step": 207800
    },
    {
      "epoch": 0.6477963703456681,
      "grad_norm": 0.5920127630233765,
      "learning_rate": 3.920734158830881e-05,
      "loss": 1.2611,
      "step": 207850
    },
    {
      "epoch": 0.6479522030063237,
      "grad_norm": 0.6665476560592651,
      "learning_rate": 3.9204744377297885e-05,
      "loss": 1.2837,
      "step": 207900
    },
    {
      "epoch": 0.6481080356669794,
      "grad_norm": 0.5080063343048096,
      "learning_rate": 3.920214716628696e-05,
      "loss": 1.2529,
      "step": 207950
    },
    {
      "epoch": 0.648263868327635,
      "grad_norm": 0.5553396344184875,
      "learning_rate": 3.919954995527603e-05,
      "loss": 1.2455,
      "step": 208000
    },
    {
      "epoch": 0.6484197009882907,
      "grad_norm": 0.5671030879020691,
      "learning_rate": 3.91969527442651e-05,
      "loss": 1.2596,
      "step": 208050
    },
    {
      "epoch": 0.6485755336489464,
      "grad_norm": 0.6899861097335815,
      "learning_rate": 3.9194355533254175e-05,
      "loss": 1.2941,
      "step": 208100
    },
    {
      "epoch": 0.648731366309602,
      "grad_norm": 0.6475275754928589,
      "learning_rate": 3.919175832224324e-05,
      "loss": 1.2465,
      "step": 208150
    },
    {
      "epoch": 0.6488871989702578,
      "grad_norm": 0.565976619720459,
      "learning_rate": 3.918916111123231e-05,
      "loss": 1.2231,
      "step": 208200
    },
    {
      "epoch": 0.6490430316309135,
      "grad_norm": 0.6049062013626099,
      "learning_rate": 3.918656390022139e-05,
      "loss": 1.2109,
      "step": 208250
    },
    {
      "epoch": 0.6491988642915691,
      "grad_norm": 0.7050091624259949,
      "learning_rate": 3.918396668921046e-05,
      "loss": 1.2655,
      "step": 208300
    },
    {
      "epoch": 0.6493546969522248,
      "grad_norm": 0.6892682909965515,
      "learning_rate": 3.918136947819953e-05,
      "loss": 1.3206,
      "step": 208350
    },
    {
      "epoch": 0.6495105296128805,
      "grad_norm": 0.5270351767539978,
      "learning_rate": 3.91787722671886e-05,
      "loss": 1.279,
      "step": 208400
    },
    {
      "epoch": 0.6496663622735361,
      "grad_norm": 0.6254038214683533,
      "learning_rate": 3.9176175056177676e-05,
      "loss": 1.2827,
      "step": 208450
    },
    {
      "epoch": 0.6498221949341919,
      "grad_norm": 0.5650413036346436,
      "learning_rate": 3.917357784516675e-05,
      "loss": 1.2572,
      "step": 208500
    },
    {
      "epoch": 0.6499780275948476,
      "grad_norm": 0.6316813230514526,
      "learning_rate": 3.917098063415582e-05,
      "loss": 1.2095,
      "step": 208550
    },
    {
      "epoch": 0.6501338602555032,
      "grad_norm": 0.6245519518852234,
      "learning_rate": 3.916838342314489e-05,
      "loss": 1.2921,
      "step": 208600
    },
    {
      "epoch": 0.6502896929161589,
      "grad_norm": 0.6391617655754089,
      "learning_rate": 3.9165786212133966e-05,
      "loss": 1.3033,
      "step": 208650
    },
    {
      "epoch": 0.6504455255768146,
      "grad_norm": 0.5971614122390747,
      "learning_rate": 3.916318900112304e-05,
      "loss": 1.2911,
      "step": 208700
    },
    {
      "epoch": 0.6506013582374702,
      "grad_norm": 0.6017028093338013,
      "learning_rate": 3.9160591790112104e-05,
      "loss": 1.2662,
      "step": 208750
    },
    {
      "epoch": 0.650757190898126,
      "grad_norm": 0.47773778438568115,
      "learning_rate": 3.915799457910118e-05,
      "loss": 1.2668,
      "step": 208800
    },
    {
      "epoch": 0.6509130235587817,
      "grad_norm": 0.701608419418335,
      "learning_rate": 3.915539736809025e-05,
      "loss": 1.3054,
      "step": 208850
    },
    {
      "epoch": 0.6510688562194373,
      "grad_norm": 0.5444220304489136,
      "learning_rate": 3.915280015707932e-05,
      "loss": 1.3179,
      "step": 208900
    },
    {
      "epoch": 0.651224688880093,
      "grad_norm": 0.7250059247016907,
      "learning_rate": 3.91502029460684e-05,
      "loss": 1.2617,
      "step": 208950
    },
    {
      "epoch": 0.6513805215407487,
      "grad_norm": 0.6177025437355042,
      "learning_rate": 3.9147605735057466e-05,
      "loss": 1.2583,
      "step": 209000
    },
    {
      "epoch": 0.6515363542014043,
      "grad_norm": 0.39568352699279785,
      "learning_rate": 3.914500852404654e-05,
      "loss": 1.2893,
      "step": 209050
    },
    {
      "epoch": 0.6516921868620601,
      "grad_norm": 0.570708155632019,
      "learning_rate": 3.914241131303561e-05,
      "loss": 1.2667,
      "step": 209100
    },
    {
      "epoch": 0.6518480195227158,
      "grad_norm": 0.5825814008712769,
      "learning_rate": 3.9139814102024684e-05,
      "loss": 1.2643,
      "step": 209150
    },
    {
      "epoch": 0.6520038521833714,
      "grad_norm": 0.6144130825996399,
      "learning_rate": 3.9137216891013756e-05,
      "loss": 1.2475,
      "step": 209200
    },
    {
      "epoch": 0.6521596848440271,
      "grad_norm": 0.6692184805870056,
      "learning_rate": 3.913467162422305e-05,
      "loss": 1.2334,
      "step": 209250
    },
    {
      "epoch": 0.6523155175046828,
      "grad_norm": 0.48808056116104126,
      "learning_rate": 3.9132074413212114e-05,
      "loss": 1.2096,
      "step": 209300
    },
    {
      "epoch": 0.6524713501653384,
      "grad_norm": 0.5305736660957336,
      "learning_rate": 3.9129477202201194e-05,
      "loss": 1.2555,
      "step": 209350
    },
    {
      "epoch": 0.6526271828259942,
      "grad_norm": 0.8041870594024658,
      "learning_rate": 3.9126879991190266e-05,
      "loss": 1.2727,
      "step": 209400
    },
    {
      "epoch": 0.6527830154866499,
      "grad_norm": 0.6233325004577637,
      "learning_rate": 3.912428278017933e-05,
      "loss": 1.2377,
      "step": 209450
    },
    {
      "epoch": 0.6529388481473055,
      "grad_norm": 0.5725005269050598,
      "learning_rate": 3.9121685569168404e-05,
      "loss": 1.2587,
      "step": 209500
    },
    {
      "epoch": 0.6530946808079612,
      "grad_norm": 0.6243135929107666,
      "learning_rate": 3.911908835815748e-05,
      "loss": 1.2722,
      "step": 209550
    },
    {
      "epoch": 0.6532505134686168,
      "grad_norm": 0.7000234723091125,
      "learning_rate": 3.911649114714655e-05,
      "loss": 1.2746,
      "step": 209600
    },
    {
      "epoch": 0.6534063461292725,
      "grad_norm": 0.6340739130973816,
      "learning_rate": 3.911389393613562e-05,
      "loss": 1.2478,
      "step": 209650
    },
    {
      "epoch": 0.6535621787899282,
      "grad_norm": 0.5805383920669556,
      "learning_rate": 3.9111296725124694e-05,
      "loss": 1.2634,
      "step": 209700
    },
    {
      "epoch": 0.653718011450584,
      "grad_norm": 0.422583669424057,
      "learning_rate": 3.910869951411377e-05,
      "loss": 1.2597,
      "step": 209750
    },
    {
      "epoch": 0.6538738441112396,
      "grad_norm": 0.5076251029968262,
      "learning_rate": 3.910610230310284e-05,
      "loss": 1.2629,
      "step": 209800
    },
    {
      "epoch": 0.6540296767718953,
      "grad_norm": 0.6151871681213379,
      "learning_rate": 3.9103505092091905e-05,
      "loss": 1.2388,
      "step": 209850
    },
    {
      "epoch": 0.6541855094325509,
      "grad_norm": 0.5849591493606567,
      "learning_rate": 3.9100907881080984e-05,
      "loss": 1.2513,
      "step": 209900
    },
    {
      "epoch": 0.6543413420932066,
      "grad_norm": 0.6253712177276611,
      "learning_rate": 3.909831067007006e-05,
      "loss": 1.2028,
      "step": 209950
    },
    {
      "epoch": 0.6544971747538623,
      "grad_norm": 0.5654793977737427,
      "learning_rate": 3.909571345905912e-05,
      "loss": 1.2803,
      "step": 210000
    },
    {
      "epoch": 0.654653007414518,
      "grad_norm": 0.5719831585884094,
      "learning_rate": 3.90931162480482e-05,
      "loss": 1.3027,
      "step": 210050
    },
    {
      "epoch": 0.6548088400751737,
      "grad_norm": 0.5803524255752563,
      "learning_rate": 3.909051903703727e-05,
      "loss": 1.2903,
      "step": 210100
    },
    {
      "epoch": 0.6549646727358294,
      "grad_norm": 0.4431924819946289,
      "learning_rate": 3.908792182602634e-05,
      "loss": 1.2964,
      "step": 210150
    },
    {
      "epoch": 0.655120505396485,
      "grad_norm": 0.6009880900382996,
      "learning_rate": 3.908532461501541e-05,
      "loss": 1.3127,
      "step": 210200
    },
    {
      "epoch": 0.6552763380571407,
      "grad_norm": 0.8074178695678711,
      "learning_rate": 3.9082727404004485e-05,
      "loss": 1.2525,
      "step": 210250
    },
    {
      "epoch": 0.6554321707177964,
      "grad_norm": 0.7129983305931091,
      "learning_rate": 3.908013019299356e-05,
      "loss": 1.2794,
      "step": 210300
    },
    {
      "epoch": 0.6555880033784521,
      "grad_norm": 0.5910574197769165,
      "learning_rate": 3.907753298198263e-05,
      "loss": 1.2636,
      "step": 210350
    },
    {
      "epoch": 0.6557438360391078,
      "grad_norm": 0.487183153629303,
      "learning_rate": 3.9074935770971696e-05,
      "loss": 1.2477,
      "step": 210400
    },
    {
      "epoch": 0.6558996686997635,
      "grad_norm": 0.5436232686042786,
      "learning_rate": 3.9072338559960775e-05,
      "loss": 1.2905,
      "step": 210450
    },
    {
      "epoch": 0.6560555013604191,
      "grad_norm": 0.5602020621299744,
      "learning_rate": 3.906974134894985e-05,
      "loss": 1.2946,
      "step": 210500
    },
    {
      "epoch": 0.6562113340210748,
      "grad_norm": 0.6611653566360474,
      "learning_rate": 3.906714413793891e-05,
      "loss": 1.3215,
      "step": 210550
    },
    {
      "epoch": 0.6563671666817305,
      "grad_norm": 0.5952283143997192,
      "learning_rate": 3.906454692692799e-05,
      "loss": 1.3308,
      "step": 210600
    },
    {
      "epoch": 0.6565229993423862,
      "grad_norm": 0.5993558764457703,
      "learning_rate": 3.9061949715917065e-05,
      "loss": 1.3214,
      "step": 210650
    },
    {
      "epoch": 0.6566788320030419,
      "grad_norm": 0.7498317956924438,
      "learning_rate": 3.905935250490613e-05,
      "loss": 1.2693,
      "step": 210700
    },
    {
      "epoch": 0.6568346646636976,
      "grad_norm": 0.5606663227081299,
      "learning_rate": 3.90567552938952e-05,
      "loss": 1.2388,
      "step": 210750
    },
    {
      "epoch": 0.6569904973243532,
      "grad_norm": 0.6791251301765442,
      "learning_rate": 3.9054158082884276e-05,
      "loss": 1.3376,
      "step": 210800
    },
    {
      "epoch": 0.6571463299850089,
      "grad_norm": 0.5995296835899353,
      "learning_rate": 3.905156087187335e-05,
      "loss": 1.2886,
      "step": 210850
    },
    {
      "epoch": 0.6573021626456645,
      "grad_norm": 0.6022339463233948,
      "learning_rate": 3.904896366086242e-05,
      "loss": 1.2192,
      "step": 210900
    },
    {
      "epoch": 0.6574579953063202,
      "grad_norm": 0.5467187762260437,
      "learning_rate": 3.9046366449851493e-05,
      "loss": 1.2632,
      "step": 210950
    },
    {
      "epoch": 0.657613827966976,
      "grad_norm": 0.7695060968399048,
      "learning_rate": 3.9043769238840566e-05,
      "loss": 1.2768,
      "step": 211000
    },
    {
      "epoch": 0.6577696606276316,
      "grad_norm": 0.4998600482940674,
      "learning_rate": 3.904117202782964e-05,
      "loss": 1.2431,
      "step": 211050
    },
    {
      "epoch": 0.6579254932882873,
      "grad_norm": 0.644580602645874,
      "learning_rate": 3.9038574816818704e-05,
      "loss": 1.2717,
      "step": 211100
    },
    {
      "epoch": 0.658081325948943,
      "grad_norm": 0.42434370517730713,
      "learning_rate": 3.9035977605807783e-05,
      "loss": 1.3021,
      "step": 211150
    },
    {
      "epoch": 0.6582371586095986,
      "grad_norm": 0.6631210446357727,
      "learning_rate": 3.9033380394796856e-05,
      "loss": 1.2993,
      "step": 211200
    },
    {
      "epoch": 0.6583929912702543,
      "grad_norm": 0.6309583187103271,
      "learning_rate": 3.903078318378592e-05,
      "loss": 1.256,
      "step": 211250
    },
    {
      "epoch": 0.6585488239309101,
      "grad_norm": 0.553085446357727,
      "learning_rate": 3.9028185972775e-05,
      "loss": 1.2493,
      "step": 211300
    },
    {
      "epoch": 0.6587046565915657,
      "grad_norm": 0.4869319498538971,
      "learning_rate": 3.9025588761764073e-05,
      "loss": 1.2881,
      "step": 211350
    },
    {
      "epoch": 0.6588604892522214,
      "grad_norm": 0.7643495798110962,
      "learning_rate": 3.902299155075314e-05,
      "loss": 1.2718,
      "step": 211400
    },
    {
      "epoch": 0.6590163219128771,
      "grad_norm": 0.5467463731765747,
      "learning_rate": 3.902044628396243e-05,
      "loss": 1.3123,
      "step": 211450
    },
    {
      "epoch": 0.6591721545735327,
      "grad_norm": 0.6566811203956604,
      "learning_rate": 3.9017849072951504e-05,
      "loss": 1.2737,
      "step": 211500
    },
    {
      "epoch": 0.6593279872341884,
      "grad_norm": 0.5393149256706238,
      "learning_rate": 3.9015251861940576e-05,
      "loss": 1.2962,
      "step": 211550
    },
    {
      "epoch": 0.6594838198948442,
      "grad_norm": 0.5531846284866333,
      "learning_rate": 3.901265465092965e-05,
      "loss": 1.2916,
      "step": 211600
    },
    {
      "epoch": 0.6596396525554998,
      "grad_norm": 0.561568558216095,
      "learning_rate": 3.901005743991872e-05,
      "loss": 1.2871,
      "step": 211650
    },
    {
      "epoch": 0.6597954852161555,
      "grad_norm": 0.5912753939628601,
      "learning_rate": 3.9007460228907794e-05,
      "loss": 1.2477,
      "step": 211700
    },
    {
      "epoch": 0.6599513178768112,
      "grad_norm": 0.4647214114665985,
      "learning_rate": 3.9004863017896866e-05,
      "loss": 1.2465,
      "step": 211750
    },
    {
      "epoch": 0.6601071505374668,
      "grad_norm": 0.6386982202529907,
      "learning_rate": 3.900226580688593e-05,
      "loss": 1.2888,
      "step": 211800
    },
    {
      "epoch": 0.6602629831981225,
      "grad_norm": 0.5088251829147339,
      "learning_rate": 3.8999668595875005e-05,
      "loss": 1.2673,
      "step": 211850
    },
    {
      "epoch": 0.6604188158587783,
      "grad_norm": 0.6624652147293091,
      "learning_rate": 3.8997071384864084e-05,
      "loss": 1.306,
      "step": 211900
    },
    {
      "epoch": 0.6605746485194339,
      "grad_norm": 0.677498996257782,
      "learning_rate": 3.899447417385315e-05,
      "loss": 1.2433,
      "step": 211950
    },
    {
      "epoch": 0.6607304811800896,
      "grad_norm": 0.5630437731742859,
      "learning_rate": 3.899187696284222e-05,
      "loss": 1.3207,
      "step": 212000
    },
    {
      "epoch": 0.6608863138407453,
      "grad_norm": 0.6254017353057861,
      "learning_rate": 3.89892797518313e-05,
      "loss": 1.2384,
      "step": 212050
    },
    {
      "epoch": 0.6610421465014009,
      "grad_norm": 0.6275790929794312,
      "learning_rate": 3.898668254082037e-05,
      "loss": 1.2285,
      "step": 212100
    },
    {
      "epoch": 0.6611979791620566,
      "grad_norm": 0.7020673155784607,
      "learning_rate": 3.898408532980944e-05,
      "loss": 1.2082,
      "step": 212150
    },
    {
      "epoch": 0.6613538118227122,
      "grad_norm": 0.6525088548660278,
      "learning_rate": 3.898148811879851e-05,
      "loss": 1.2849,
      "step": 212200
    },
    {
      "epoch": 0.661509644483368,
      "grad_norm": 0.5702049136161804,
      "learning_rate": 3.8978890907787585e-05,
      "loss": 1.3124,
      "step": 212250
    },
    {
      "epoch": 0.6616654771440237,
      "grad_norm": 0.5864314436912537,
      "learning_rate": 3.897629369677666e-05,
      "loss": 1.309,
      "step": 212300
    },
    {
      "epoch": 0.6618213098046793,
      "grad_norm": 0.5350309014320374,
      "learning_rate": 3.897369648576572e-05,
      "loss": 1.2756,
      "step": 212350
    },
    {
      "epoch": 0.661977142465335,
      "grad_norm": 0.7118269205093384,
      "learning_rate": 3.8971099274754795e-05,
      "loss": 1.308,
      "step": 212400
    },
    {
      "epoch": 0.6621329751259907,
      "grad_norm": 0.5786530375480652,
      "learning_rate": 3.8968502063743875e-05,
      "loss": 1.2557,
      "step": 212450
    },
    {
      "epoch": 0.6622888077866463,
      "grad_norm": 0.758378803730011,
      "learning_rate": 3.896590485273294e-05,
      "loss": 1.2621,
      "step": 212500
    },
    {
      "epoch": 0.6624446404473021,
      "grad_norm": 0.43557947874069214,
      "learning_rate": 3.896330764172201e-05,
      "loss": 1.2549,
      "step": 212550
    },
    {
      "epoch": 0.6626004731079578,
      "grad_norm": 0.5975394248962402,
      "learning_rate": 3.896071043071109e-05,
      "loss": 1.2781,
      "step": 212600
    },
    {
      "epoch": 0.6627563057686134,
      "grad_norm": 0.5159555077552795,
      "learning_rate": 3.895811321970016e-05,
      "loss": 1.2154,
      "step": 212650
    },
    {
      "epoch": 0.6629121384292691,
      "grad_norm": 0.5474268198013306,
      "learning_rate": 3.895551600868923e-05,
      "loss": 1.2856,
      "step": 212700
    },
    {
      "epoch": 0.6630679710899248,
      "grad_norm": 0.5420993566513062,
      "learning_rate": 3.89529187976783e-05,
      "loss": 1.2837,
      "step": 212750
    },
    {
      "epoch": 0.6632238037505804,
      "grad_norm": 0.7661621570587158,
      "learning_rate": 3.8950321586667375e-05,
      "loss": 1.2509,
      "step": 212800
    },
    {
      "epoch": 0.6633796364112362,
      "grad_norm": 0.6193599104881287,
      "learning_rate": 3.894772437565645e-05,
      "loss": 1.2211,
      "step": 212850
    },
    {
      "epoch": 0.6635354690718919,
      "grad_norm": 0.6310214996337891,
      "learning_rate": 3.894512716464552e-05,
      "loss": 1.3027,
      "step": 212900
    },
    {
      "epoch": 0.6636913017325475,
      "grad_norm": 0.6170612573623657,
      "learning_rate": 3.894252995363459e-05,
      "loss": 1.2962,
      "step": 212950
    },
    {
      "epoch": 0.6638471343932032,
      "grad_norm": 0.6974238157272339,
      "learning_rate": 3.8939932742623665e-05,
      "loss": 1.2811,
      "step": 213000
    },
    {
      "epoch": 0.6640029670538589,
      "grad_norm": 0.5729466676712036,
      "learning_rate": 3.893733553161273e-05,
      "loss": 1.2661,
      "step": 213050
    },
    {
      "epoch": 0.6641587997145145,
      "grad_norm": 0.6020253300666809,
      "learning_rate": 3.8934738320601804e-05,
      "loss": 1.2295,
      "step": 213100
    },
    {
      "epoch": 0.6643146323751703,
      "grad_norm": 0.49640750885009766,
      "learning_rate": 3.893214110959088e-05,
      "loss": 1.2744,
      "step": 213150
    },
    {
      "epoch": 0.664470465035826,
      "grad_norm": 0.5673109292984009,
      "learning_rate": 3.892954389857995e-05,
      "loss": 1.3061,
      "step": 213200
    },
    {
      "epoch": 0.6646262976964816,
      "grad_norm": 0.649787425994873,
      "learning_rate": 3.892694668756902e-05,
      "loss": 1.2663,
      "step": 213250
    },
    {
      "epoch": 0.6647821303571373,
      "grad_norm": 0.7478031516075134,
      "learning_rate": 3.89243494765581e-05,
      "loss": 1.2537,
      "step": 213300
    },
    {
      "epoch": 0.664937963017793,
      "grad_norm": 0.700840950012207,
      "learning_rate": 3.8921752265547166e-05,
      "loss": 1.2361,
      "step": 213350
    },
    {
      "epoch": 0.6650937956784486,
      "grad_norm": 0.6217307448387146,
      "learning_rate": 3.891915505453624e-05,
      "loss": 1.2741,
      "step": 213400
    },
    {
      "epoch": 0.6652496283391043,
      "grad_norm": 0.6990775465965271,
      "learning_rate": 3.891655784352531e-05,
      "loss": 1.2549,
      "step": 213450
    },
    {
      "epoch": 0.6654054609997601,
      "grad_norm": 0.5480942726135254,
      "learning_rate": 3.8913960632514384e-05,
      "loss": 1.2706,
      "step": 213500
    },
    {
      "epoch": 0.6655612936604157,
      "grad_norm": 0.6143566370010376,
      "learning_rate": 3.8911363421503456e-05,
      "loss": 1.2786,
      "step": 213550
    },
    {
      "epoch": 0.6657171263210714,
      "grad_norm": 0.5220068693161011,
      "learning_rate": 3.890876621049253e-05,
      "loss": 1.2677,
      "step": 213600
    },
    {
      "epoch": 0.665872958981727,
      "grad_norm": 0.5569909811019897,
      "learning_rate": 3.8906220943701814e-05,
      "loss": 1.2797,
      "step": 213650
    },
    {
      "epoch": 0.6660287916423827,
      "grad_norm": 0.4941173791885376,
      "learning_rate": 3.890362373269089e-05,
      "loss": 1.2842,
      "step": 213700
    },
    {
      "epoch": 0.6661846243030384,
      "grad_norm": 0.5038810968399048,
      "learning_rate": 3.890102652167996e-05,
      "loss": 1.2464,
      "step": 213750
    },
    {
      "epoch": 0.6663404569636941,
      "grad_norm": 0.5193643569946289,
      "learning_rate": 3.889842931066903e-05,
      "loss": 1.2781,
      "step": 213800
    },
    {
      "epoch": 0.6664962896243498,
      "grad_norm": 0.5438086986541748,
      "learning_rate": 3.8895832099658104e-05,
      "loss": 1.3046,
      "step": 213850
    },
    {
      "epoch": 0.6666521222850055,
      "grad_norm": 0.6593633890151978,
      "learning_rate": 3.8893234888647176e-05,
      "loss": 1.3257,
      "step": 213900
    },
    {
      "epoch": 0.6668079549456611,
      "grad_norm": 0.861831784248352,
      "learning_rate": 3.889063767763625e-05,
      "loss": 1.2851,
      "step": 213950
    },
    {
      "epoch": 0.6669637876063168,
      "grad_norm": 0.5527218580245972,
      "learning_rate": 3.888804046662532e-05,
      "loss": 1.2867,
      "step": 214000
    },
    {
      "epoch": 0.6671196202669725,
      "grad_norm": 0.5409154891967773,
      "learning_rate": 3.8885443255614394e-05,
      "loss": 1.2443,
      "step": 214050
    },
    {
      "epoch": 0.6672754529276282,
      "grad_norm": 0.657130241394043,
      "learning_rate": 3.8882846044603466e-05,
      "loss": 1.2526,
      "step": 214100
    },
    {
      "epoch": 0.6674312855882839,
      "grad_norm": 0.6858641505241394,
      "learning_rate": 3.888024883359254e-05,
      "loss": 1.29,
      "step": 214150
    },
    {
      "epoch": 0.6675871182489396,
      "grad_norm": 0.680986762046814,
      "learning_rate": 3.8877651622581605e-05,
      "loss": 1.2049,
      "step": 214200
    },
    {
      "epoch": 0.6677429509095952,
      "grad_norm": 0.5677402019500732,
      "learning_rate": 3.8875054411570684e-05,
      "loss": 1.3222,
      "step": 214250
    },
    {
      "epoch": 0.6678987835702509,
      "grad_norm": 0.6014267206192017,
      "learning_rate": 3.8872457200559756e-05,
      "loss": 1.2829,
      "step": 214300
    },
    {
      "epoch": 0.6680546162309066,
      "grad_norm": 0.5864558219909668,
      "learning_rate": 3.886985998954882e-05,
      "loss": 1.2736,
      "step": 214350
    },
    {
      "epoch": 0.6682104488915623,
      "grad_norm": 0.6493791341781616,
      "learning_rate": 3.8867262778537895e-05,
      "loss": 1.2828,
      "step": 214400
    },
    {
      "epoch": 0.668366281552218,
      "grad_norm": 0.5639650821685791,
      "learning_rate": 3.886466556752697e-05,
      "loss": 1.2786,
      "step": 214450
    },
    {
      "epoch": 0.6685221142128737,
      "grad_norm": 0.565219521522522,
      "learning_rate": 3.886206835651604e-05,
      "loss": 1.2627,
      "step": 214500
    },
    {
      "epoch": 0.6686779468735293,
      "grad_norm": 0.6319504380226135,
      "learning_rate": 3.885947114550511e-05,
      "loss": 1.3381,
      "step": 214550
    },
    {
      "epoch": 0.668833779534185,
      "grad_norm": 0.5931169986724854,
      "learning_rate": 3.8856873934494185e-05,
      "loss": 1.2439,
      "step": 214600
    },
    {
      "epoch": 0.6689896121948407,
      "grad_norm": 0.3659757971763611,
      "learning_rate": 3.885427672348326e-05,
      "loss": 1.3344,
      "step": 214650
    },
    {
      "epoch": 0.6691454448554963,
      "grad_norm": 0.5374014377593994,
      "learning_rate": 3.885167951247233e-05,
      "loss": 1.2634,
      "step": 214700
    },
    {
      "epoch": 0.6693012775161521,
      "grad_norm": 0.6945520639419556,
      "learning_rate": 3.8849082301461395e-05,
      "loss": 1.2414,
      "step": 214750
    },
    {
      "epoch": 0.6694571101768078,
      "grad_norm": 0.664117157459259,
      "learning_rate": 3.8846485090450475e-05,
      "loss": 1.3072,
      "step": 214800
    },
    {
      "epoch": 0.6696129428374634,
      "grad_norm": 0.683945894241333,
      "learning_rate": 3.884388787943955e-05,
      "loss": 1.2979,
      "step": 214850
    },
    {
      "epoch": 0.6697687754981191,
      "grad_norm": 0.6304740905761719,
      "learning_rate": 3.884129066842861e-05,
      "loss": 1.239,
      "step": 214900
    },
    {
      "epoch": 0.6699246081587747,
      "grad_norm": 0.5806061625480652,
      "learning_rate": 3.883869345741769e-05,
      "loss": 1.2609,
      "step": 214950
    },
    {
      "epoch": 0.6700804408194304,
      "grad_norm": 0.5097149014472961,
      "learning_rate": 3.883609624640676e-05,
      "loss": 1.2998,
      "step": 215000
    },
    {
      "epoch": 0.6702362734800862,
      "grad_norm": 0.6059367656707764,
      "learning_rate": 3.883349903539583e-05,
      "loss": 1.2662,
      "step": 215050
    },
    {
      "epoch": 0.6703921061407419,
      "grad_norm": 0.6283444166183472,
      "learning_rate": 3.88309018243849e-05,
      "loss": 1.2657,
      "step": 215100
    },
    {
      "epoch": 0.6705479388013975,
      "grad_norm": 0.5728991627693176,
      "learning_rate": 3.8828304613373975e-05,
      "loss": 1.2605,
      "step": 215150
    },
    {
      "epoch": 0.6707037714620532,
      "grad_norm": 0.49237266182899475,
      "learning_rate": 3.882570740236305e-05,
      "loss": 1.3111,
      "step": 215200
    },
    {
      "epoch": 0.6708596041227088,
      "grad_norm": 0.46971267461776733,
      "learning_rate": 3.882311019135212e-05,
      "loss": 1.2914,
      "step": 215250
    },
    {
      "epoch": 0.6710154367833645,
      "grad_norm": 0.6420092582702637,
      "learning_rate": 3.882051298034119e-05,
      "loss": 1.287,
      "step": 215300
    },
    {
      "epoch": 0.6711712694440203,
      "grad_norm": 0.5970301032066345,
      "learning_rate": 3.8817915769330265e-05,
      "loss": 1.2934,
      "step": 215350
    },
    {
      "epoch": 0.6713271021046759,
      "grad_norm": 0.6381885409355164,
      "learning_rate": 3.881531855831934e-05,
      "loss": 1.2422,
      "step": 215400
    },
    {
      "epoch": 0.6714829347653316,
      "grad_norm": 0.49748435616493225,
      "learning_rate": 3.8812721347308404e-05,
      "loss": 1.2999,
      "step": 215450
    },
    {
      "epoch": 0.6716387674259873,
      "grad_norm": 0.558681309223175,
      "learning_rate": 3.881012413629748e-05,
      "loss": 1.1817,
      "step": 215500
    },
    {
      "epoch": 0.6717946000866429,
      "grad_norm": 0.6196579337120056,
      "learning_rate": 3.8807526925286556e-05,
      "loss": 1.2236,
      "step": 215550
    },
    {
      "epoch": 0.6719504327472986,
      "grad_norm": 0.6664630770683289,
      "learning_rate": 3.880492971427562e-05,
      "loss": 1.3297,
      "step": 215600
    },
    {
      "epoch": 0.6721062654079544,
      "grad_norm": 0.6053444743156433,
      "learning_rate": 3.8802332503264694e-05,
      "loss": 1.231,
      "step": 215650
    },
    {
      "epoch": 0.67226209806861,
      "grad_norm": 0.5984180569648743,
      "learning_rate": 3.8799735292253766e-05,
      "loss": 1.2582,
      "step": 215700
    },
    {
      "epoch": 0.6724179307292657,
      "grad_norm": 0.5079615116119385,
      "learning_rate": 3.879713808124284e-05,
      "loss": 1.2353,
      "step": 215750
    },
    {
      "epoch": 0.6725737633899214,
      "grad_norm": 0.6678498983383179,
      "learning_rate": 3.879454087023191e-05,
      "loss": 1.2541,
      "step": 215800
    },
    {
      "epoch": 0.672729596050577,
      "grad_norm": 0.5396826267242432,
      "learning_rate": 3.8791943659220984e-05,
      "loss": 1.2497,
      "step": 215850
    },
    {
      "epoch": 0.6728854287112327,
      "grad_norm": 0.5678374767303467,
      "learning_rate": 3.8789346448210056e-05,
      "loss": 1.2936,
      "step": 215900
    },
    {
      "epoch": 0.6730412613718885,
      "grad_norm": 0.5450857281684875,
      "learning_rate": 3.878674923719913e-05,
      "loss": 1.2193,
      "step": 215950
    },
    {
      "epoch": 0.6731970940325441,
      "grad_norm": 0.5452477335929871,
      "learning_rate": 3.8784152026188194e-05,
      "loss": 1.3363,
      "step": 216000
    },
    {
      "epoch": 0.6733529266931998,
      "grad_norm": 0.5161108374595642,
      "learning_rate": 3.8781554815177274e-05,
      "loss": 1.2823,
      "step": 216050
    },
    {
      "epoch": 0.6735087593538555,
      "grad_norm": 0.6787498593330383,
      "learning_rate": 3.8778957604166346e-05,
      "loss": 1.2861,
      "step": 216100
    },
    {
      "epoch": 0.6736645920145111,
      "grad_norm": 0.5849643349647522,
      "learning_rate": 3.877641233737563e-05,
      "loss": 1.2984,
      "step": 216150
    },
    {
      "epoch": 0.6738204246751668,
      "grad_norm": 0.6229057312011719,
      "learning_rate": 3.8773815126364704e-05,
      "loss": 1.2335,
      "step": 216200
    },
    {
      "epoch": 0.6739762573358224,
      "grad_norm": 0.5608031153678894,
      "learning_rate": 3.8771217915353783e-05,
      "loss": 1.2822,
      "step": 216250
    },
    {
      "epoch": 0.6741320899964782,
      "grad_norm": 0.5560820698738098,
      "learning_rate": 3.876862070434285e-05,
      "loss": 1.287,
      "step": 216300
    },
    {
      "epoch": 0.6742879226571339,
      "grad_norm": 0.5300893187522888,
      "learning_rate": 3.876602349333192e-05,
      "loss": 1.2959,
      "step": 216350
    },
    {
      "epoch": 0.6744437553177896,
      "grad_norm": 0.660762369632721,
      "learning_rate": 3.8763426282320994e-05,
      "loss": 1.2729,
      "step": 216400
    },
    {
      "epoch": 0.6745995879784452,
      "grad_norm": 0.5922929644584656,
      "learning_rate": 3.8760881015530286e-05,
      "loss": 1.2625,
      "step": 216450
    },
    {
      "epoch": 0.6747554206391009,
      "grad_norm": 0.5195740461349487,
      "learning_rate": 3.875828380451936e-05,
      "loss": 1.2698,
      "step": 216500
    },
    {
      "epoch": 0.6749112532997565,
      "grad_norm": 0.6477301120758057,
      "learning_rate": 3.875568659350843e-05,
      "loss": 1.229,
      "step": 216550
    },
    {
      "epoch": 0.6750670859604123,
      "grad_norm": 0.5738145112991333,
      "learning_rate": 3.87530893824975e-05,
      "loss": 1.2295,
      "step": 216600
    },
    {
      "epoch": 0.675222918621068,
      "grad_norm": 0.6310129761695862,
      "learning_rate": 3.8750492171486576e-05,
      "loss": 1.2873,
      "step": 216650
    },
    {
      "epoch": 0.6753787512817236,
      "grad_norm": 0.6184458136558533,
      "learning_rate": 3.874789496047564e-05,
      "loss": 1.3369,
      "step": 216700
    },
    {
      "epoch": 0.6755345839423793,
      "grad_norm": 0.49848270416259766,
      "learning_rate": 3.8745297749464715e-05,
      "loss": 1.3049,
      "step": 216750
    },
    {
      "epoch": 0.675690416603035,
      "grad_norm": 0.6047067642211914,
      "learning_rate": 3.8742700538453794e-05,
      "loss": 1.2324,
      "step": 216800
    },
    {
      "epoch": 0.6758462492636906,
      "grad_norm": 0.6030552387237549,
      "learning_rate": 3.874010332744286e-05,
      "loss": 1.28,
      "step": 216850
    },
    {
      "epoch": 0.6760020819243464,
      "grad_norm": 0.5832059979438782,
      "learning_rate": 3.873750611643193e-05,
      "loss": 1.2274,
      "step": 216900
    },
    {
      "epoch": 0.6761579145850021,
      "grad_norm": 0.7073049545288086,
      "learning_rate": 3.8734908905421005e-05,
      "loss": 1.2522,
      "step": 216950
    },
    {
      "epoch": 0.6763137472456577,
      "grad_norm": 0.5671322345733643,
      "learning_rate": 3.873231169441008e-05,
      "loss": 1.2919,
      "step": 217000
    },
    {
      "epoch": 0.6764695799063134,
      "grad_norm": 0.6949108242988586,
      "learning_rate": 3.872971448339915e-05,
      "loss": 1.3169,
      "step": 217050
    },
    {
      "epoch": 0.6766254125669691,
      "grad_norm": 0.5049668550491333,
      "learning_rate": 3.872711727238822e-05,
      "loss": 1.232,
      "step": 217100
    },
    {
      "epoch": 0.6767812452276247,
      "grad_norm": 0.6480880379676819,
      "learning_rate": 3.8724520061377295e-05,
      "loss": 1.2692,
      "step": 217150
    },
    {
      "epoch": 0.6769370778882805,
      "grad_norm": 0.638563334941864,
      "learning_rate": 3.872192285036637e-05,
      "loss": 1.2915,
      "step": 217200
    },
    {
      "epoch": 0.6770929105489362,
      "grad_norm": 0.5621353387832642,
      "learning_rate": 3.871932563935543e-05,
      "loss": 1.2722,
      "step": 217250
    },
    {
      "epoch": 0.6772487432095918,
      "grad_norm": 0.6087936162948608,
      "learning_rate": 3.8716728428344505e-05,
      "loss": 1.26,
      "step": 217300
    },
    {
      "epoch": 0.6774045758702475,
      "grad_norm": 0.5805373191833496,
      "learning_rate": 3.8714131217333585e-05,
      "loss": 1.301,
      "step": 217350
    },
    {
      "epoch": 0.6775604085309032,
      "grad_norm": 0.6307465434074402,
      "learning_rate": 3.871153400632265e-05,
      "loss": 1.3052,
      "step": 217400
    },
    {
      "epoch": 0.6777162411915588,
      "grad_norm": 0.6182193160057068,
      "learning_rate": 3.870893679531172e-05,
      "loss": 1.2867,
      "step": 217450
    },
    {
      "epoch": 0.6778720738522145,
      "grad_norm": 0.6774037480354309,
      "learning_rate": 3.8706339584300795e-05,
      "loss": 1.3174,
      "step": 217500
    },
    {
      "epoch": 0.6780279065128703,
      "grad_norm": 0.5712915658950806,
      "learning_rate": 3.870374237328987e-05,
      "loss": 1.2764,
      "step": 217550
    },
    {
      "epoch": 0.6781837391735259,
      "grad_norm": 0.5293627381324768,
      "learning_rate": 3.870114516227894e-05,
      "loss": 1.2134,
      "step": 217600
    },
    {
      "epoch": 0.6783395718341816,
      "grad_norm": 0.7582887411117554,
      "learning_rate": 3.869854795126801e-05,
      "loss": 1.2429,
      "step": 217650
    },
    {
      "epoch": 0.6784954044948373,
      "grad_norm": 0.7269229292869568,
      "learning_rate": 3.8695950740257085e-05,
      "loss": 1.2644,
      "step": 217700
    },
    {
      "epoch": 0.6786512371554929,
      "grad_norm": 0.5189063549041748,
      "learning_rate": 3.869335352924616e-05,
      "loss": 1.2776,
      "step": 217750
    },
    {
      "epoch": 0.6788070698161486,
      "grad_norm": 0.6239196062088013,
      "learning_rate": 3.869075631823523e-05,
      "loss": 1.3359,
      "step": 217800
    },
    {
      "epoch": 0.6789629024768044,
      "grad_norm": 0.590872585773468,
      "learning_rate": 3.8688159107224296e-05,
      "loss": 1.256,
      "step": 217850
    },
    {
      "epoch": 0.67911873513746,
      "grad_norm": 0.5155825614929199,
      "learning_rate": 3.8685561896213375e-05,
      "loss": 1.2605,
      "step": 217900
    },
    {
      "epoch": 0.6792745677981157,
      "grad_norm": 0.5279022455215454,
      "learning_rate": 3.868296468520244e-05,
      "loss": 1.3216,
      "step": 217950
    },
    {
      "epoch": 0.6794304004587713,
      "grad_norm": 0.5762861371040344,
      "learning_rate": 3.8680367474191514e-05,
      "loss": 1.2426,
      "step": 218000
    },
    {
      "epoch": 0.679586233119427,
      "grad_norm": 0.7288283705711365,
      "learning_rate": 3.867777026318059e-05,
      "loss": 1.2398,
      "step": 218050
    },
    {
      "epoch": 0.6797420657800827,
      "grad_norm": 0.6539058685302734,
      "learning_rate": 3.867517305216966e-05,
      "loss": 1.2698,
      "step": 218100
    },
    {
      "epoch": 0.6798978984407384,
      "grad_norm": 0.457358717918396,
      "learning_rate": 3.867257584115873e-05,
      "loss": 1.2159,
      "step": 218150
    },
    {
      "epoch": 0.6800537311013941,
      "grad_norm": 0.5919903516769409,
      "learning_rate": 3.8669978630147804e-05,
      "loss": 1.2348,
      "step": 218200
    },
    {
      "epoch": 0.6802095637620498,
      "grad_norm": 0.599507749080658,
      "learning_rate": 3.8667381419136876e-05,
      "loss": 1.3197,
      "step": 218250
    },
    {
      "epoch": 0.6803653964227054,
      "grad_norm": 0.5266317129135132,
      "learning_rate": 3.866478420812595e-05,
      "loss": 1.2692,
      "step": 218300
    },
    {
      "epoch": 0.6805212290833611,
      "grad_norm": 0.6781002283096313,
      "learning_rate": 3.866218699711502e-05,
      "loss": 1.3041,
      "step": 218350
    },
    {
      "epoch": 0.6806770617440168,
      "grad_norm": 0.5720258355140686,
      "learning_rate": 3.8659589786104094e-05,
      "loss": 1.2599,
      "step": 218400
    },
    {
      "epoch": 0.6808328944046725,
      "grad_norm": 0.6375699639320374,
      "learning_rate": 3.8656992575093166e-05,
      "loss": 1.2294,
      "step": 218450
    },
    {
      "epoch": 0.6809887270653282,
      "grad_norm": 0.6226464509963989,
      "learning_rate": 3.865439536408224e-05,
      "loss": 1.2743,
      "step": 218500
    },
    {
      "epoch": 0.6811445597259839,
      "grad_norm": 0.5182372331619263,
      "learning_rate": 3.8651798153071304e-05,
      "loss": 1.2799,
      "step": 218550
    },
    {
      "epoch": 0.6813003923866395,
      "grad_norm": 0.7253084778785706,
      "learning_rate": 3.8649200942060384e-05,
      "loss": 1.2074,
      "step": 218600
    },
    {
      "epoch": 0.6814562250472952,
      "grad_norm": 0.6197847723960876,
      "learning_rate": 3.864660373104945e-05,
      "loss": 1.2649,
      "step": 218650
    },
    {
      "epoch": 0.6816120577079509,
      "grad_norm": 0.5789700746536255,
      "learning_rate": 3.864400652003852e-05,
      "loss": 1.2551,
      "step": 218700
    },
    {
      "epoch": 0.6817678903686065,
      "grad_norm": 0.5605229735374451,
      "learning_rate": 3.8641409309027594e-05,
      "loss": 1.2248,
      "step": 218750
    },
    {
      "epoch": 0.6819237230292623,
      "grad_norm": 0.6347636580467224,
      "learning_rate": 3.863881209801667e-05,
      "loss": 1.2725,
      "step": 218800
    },
    {
      "epoch": 0.682079555689918,
      "grad_norm": 0.41454869508743286,
      "learning_rate": 3.863621488700574e-05,
      "loss": 1.232,
      "step": 218850
    },
    {
      "epoch": 0.6822353883505736,
      "grad_norm": 0.5902369618415833,
      "learning_rate": 3.863361767599481e-05,
      "loss": 1.3339,
      "step": 218900
    },
    {
      "epoch": 0.6823912210112293,
      "grad_norm": 0.6257354617118835,
      "learning_rate": 3.8631020464983884e-05,
      "loss": 1.2289,
      "step": 218950
    },
    {
      "epoch": 0.682547053671885,
      "grad_norm": 0.5012897253036499,
      "learning_rate": 3.862842325397296e-05,
      "loss": 1.217,
      "step": 219000
    },
    {
      "epoch": 0.6827028863325406,
      "grad_norm": 0.6023706197738647,
      "learning_rate": 3.862582604296203e-05,
      "loss": 1.2898,
      "step": 219050
    },
    {
      "epoch": 0.6828587189931964,
      "grad_norm": 0.6936305165290833,
      "learning_rate": 3.8623228831951095e-05,
      "loss": 1.2751,
      "step": 219100
    },
    {
      "epoch": 0.683014551653852,
      "grad_norm": 0.6410790681838989,
      "learning_rate": 3.8620631620940174e-05,
      "loss": 1.2809,
      "step": 219150
    },
    {
      "epoch": 0.6831703843145077,
      "grad_norm": 0.6765620112419128,
      "learning_rate": 3.861803440992925e-05,
      "loss": 1.2707,
      "step": 219200
    },
    {
      "epoch": 0.6833262169751634,
      "grad_norm": 0.433355450630188,
      "learning_rate": 3.861543719891831e-05,
      "loss": 1.2611,
      "step": 219250
    },
    {
      "epoch": 0.683482049635819,
      "grad_norm": 0.6918677687644958,
      "learning_rate": 3.861283998790739e-05,
      "loss": 1.2503,
      "step": 219300
    },
    {
      "epoch": 0.6836378822964747,
      "grad_norm": 0.6051877737045288,
      "learning_rate": 3.861024277689646e-05,
      "loss": 1.2313,
      "step": 219350
    },
    {
      "epoch": 0.6837937149571305,
      "grad_norm": 0.5124432444572449,
      "learning_rate": 3.860764556588553e-05,
      "loss": 1.2815,
      "step": 219400
    },
    {
      "epoch": 0.6839495476177861,
      "grad_norm": 0.6438618898391724,
      "learning_rate": 3.86050483548746e-05,
      "loss": 1.2212,
      "step": 219450
    },
    {
      "epoch": 0.6841053802784418,
      "grad_norm": 1.1900278329849243,
      "learning_rate": 3.8602451143863675e-05,
      "loss": 1.3192,
      "step": 219500
    },
    {
      "epoch": 0.6842612129390975,
      "grad_norm": 0.5324357151985168,
      "learning_rate": 3.859985393285275e-05,
      "loss": 1.2641,
      "step": 219550
    },
    {
      "epoch": 0.6844170455997531,
      "grad_norm": 0.6595821380615234,
      "learning_rate": 3.859725672184182e-05,
      "loss": 1.2433,
      "step": 219600
    },
    {
      "epoch": 0.6845728782604088,
      "grad_norm": 0.6104232668876648,
      "learning_rate": 3.859465951083089e-05,
      "loss": 1.3339,
      "step": 219650
    },
    {
      "epoch": 0.6847287109210646,
      "grad_norm": 0.5387767553329468,
      "learning_rate": 3.8592062299819965e-05,
      "loss": 1.271,
      "step": 219700
    },
    {
      "epoch": 0.6848845435817202,
      "grad_norm": 0.6215363144874573,
      "learning_rate": 3.858946508880904e-05,
      "loss": 1.2344,
      "step": 219750
    },
    {
      "epoch": 0.6850403762423759,
      "grad_norm": 0.7801370620727539,
      "learning_rate": 3.85868678777981e-05,
      "loss": 1.2676,
      "step": 219800
    },
    {
      "epoch": 0.6851962089030316,
      "grad_norm": 0.5719462633132935,
      "learning_rate": 3.858427066678718e-05,
      "loss": 1.2093,
      "step": 219850
    },
    {
      "epoch": 0.6853520415636872,
      "grad_norm": 0.608959436416626,
      "learning_rate": 3.858167345577625e-05,
      "loss": 1.3158,
      "step": 219900
    },
    {
      "epoch": 0.6855078742243429,
      "grad_norm": 0.6341690421104431,
      "learning_rate": 3.857907624476532e-05,
      "loss": 1.3246,
      "step": 219950
    },
    {
      "epoch": 0.6856637068849986,
      "grad_norm": 0.40665385127067566,
      "learning_rate": 3.857647903375439e-05,
      "loss": 1.2373,
      "step": 220000
    },
    {
      "epoch": 0.6858195395456543,
      "grad_norm": 0.5712733268737793,
      "learning_rate": 3.8573881822743466e-05,
      "loss": 1.2513,
      "step": 220050
    },
    {
      "epoch": 0.68597537220631,
      "grad_norm": 0.572960376739502,
      "learning_rate": 3.857128461173254e-05,
      "loss": 1.2947,
      "step": 220100
    },
    {
      "epoch": 0.6861312048669657,
      "grad_norm": 0.704538106918335,
      "learning_rate": 3.856868740072161e-05,
      "loss": 1.3044,
      "step": 220150
    },
    {
      "epoch": 0.6862870375276213,
      "grad_norm": 0.5121258497238159,
      "learning_rate": 3.8566142133930896e-05,
      "loss": 1.2926,
      "step": 220200
    },
    {
      "epoch": 0.686442870188277,
      "grad_norm": 0.5448752045631409,
      "learning_rate": 3.8563544922919975e-05,
      "loss": 1.2702,
      "step": 220250
    },
    {
      "epoch": 0.6865987028489327,
      "grad_norm": 0.6963172554969788,
      "learning_rate": 3.856094771190905e-05,
      "loss": 1.273,
      "step": 220300
    },
    {
      "epoch": 0.6867545355095884,
      "grad_norm": 0.5953500866889954,
      "learning_rate": 3.8558350500898114e-05,
      "loss": 1.3024,
      "step": 220350
    },
    {
      "epoch": 0.6869103681702441,
      "grad_norm": 0.6819295883178711,
      "learning_rate": 3.855575328988719e-05,
      "loss": 1.3036,
      "step": 220400
    },
    {
      "epoch": 0.6870662008308998,
      "grad_norm": 0.4385388493537903,
      "learning_rate": 3.8553156078876265e-05,
      "loss": 1.2671,
      "step": 220450
    },
    {
      "epoch": 0.6872220334915554,
      "grad_norm": 0.6513774991035461,
      "learning_rate": 3.855055886786533e-05,
      "loss": 1.2917,
      "step": 220500
    },
    {
      "epoch": 0.6873778661522111,
      "grad_norm": 0.5634100437164307,
      "learning_rate": 3.8547961656854404e-05,
      "loss": 1.2907,
      "step": 220550
    },
    {
      "epoch": 0.6875336988128667,
      "grad_norm": 0.6073367595672607,
      "learning_rate": 3.8545364445843476e-05,
      "loss": 1.2867,
      "step": 220600
    },
    {
      "epoch": 0.6876895314735225,
      "grad_norm": 0.6153193712234497,
      "learning_rate": 3.854276723483255e-05,
      "loss": 1.2837,
      "step": 220650
    },
    {
      "epoch": 0.6878453641341782,
      "grad_norm": 0.6010151505470276,
      "learning_rate": 3.854017002382162e-05,
      "loss": 1.3006,
      "step": 220700
    },
    {
      "epoch": 0.6880011967948338,
      "grad_norm": 0.6170875430107117,
      "learning_rate": 3.8537572812810694e-05,
      "loss": 1.2655,
      "step": 220750
    },
    {
      "epoch": 0.6881570294554895,
      "grad_norm": 0.6583067774772644,
      "learning_rate": 3.8534975601799766e-05,
      "loss": 1.2661,
      "step": 220800
    },
    {
      "epoch": 0.6883128621161452,
      "grad_norm": 0.6471160054206848,
      "learning_rate": 3.853237839078884e-05,
      "loss": 1.2581,
      "step": 220850
    },
    {
      "epoch": 0.6884686947768008,
      "grad_norm": 0.5278262495994568,
      "learning_rate": 3.8529781179777904e-05,
      "loss": 1.2747,
      "step": 220900
    },
    {
      "epoch": 0.6886245274374566,
      "grad_norm": 0.4269121587276459,
      "learning_rate": 3.8527183968766984e-05,
      "loss": 1.2759,
      "step": 220950
    },
    {
      "epoch": 0.6887803600981123,
      "grad_norm": 0.5439375042915344,
      "learning_rate": 3.8524586757756056e-05,
      "loss": 1.2687,
      "step": 221000
    },
    {
      "epoch": 0.6889361927587679,
      "grad_norm": 0.7156884074211121,
      "learning_rate": 3.852198954674512e-05,
      "loss": 1.2805,
      "step": 221050
    },
    {
      "epoch": 0.6890920254194236,
      "grad_norm": 0.7553223967552185,
      "learning_rate": 3.8519392335734194e-05,
      "loss": 1.2677,
      "step": 221100
    },
    {
      "epoch": 0.6892478580800793,
      "grad_norm": 0.4622093439102173,
      "learning_rate": 3.8516795124723274e-05,
      "loss": 1.2462,
      "step": 221150
    },
    {
      "epoch": 0.6894036907407349,
      "grad_norm": 0.6428795456886292,
      "learning_rate": 3.851419791371234e-05,
      "loss": 1.2361,
      "step": 221200
    },
    {
      "epoch": 0.6895595234013907,
      "grad_norm": 0.5824047327041626,
      "learning_rate": 3.851160070270141e-05,
      "loss": 1.3653,
      "step": 221250
    },
    {
      "epoch": 0.6897153560620464,
      "grad_norm": 0.5825938582420349,
      "learning_rate": 3.8509003491690484e-05,
      "loss": 1.3154,
      "step": 221300
    },
    {
      "epoch": 0.689871188722702,
      "grad_norm": 0.580592930316925,
      "learning_rate": 3.850640628067956e-05,
      "loss": 1.3002,
      "step": 221350
    },
    {
      "epoch": 0.6900270213833577,
      "grad_norm": 0.5084434747695923,
      "learning_rate": 3.850380906966863e-05,
      "loss": 1.2719,
      "step": 221400
    },
    {
      "epoch": 0.6901828540440134,
      "grad_norm": 0.8738624453544617,
      "learning_rate": 3.85012118586577e-05,
      "loss": 1.291,
      "step": 221450
    },
    {
      "epoch": 0.690338686704669,
      "grad_norm": 0.6071756482124329,
      "learning_rate": 3.8498614647646774e-05,
      "loss": 1.28,
      "step": 221500
    },
    {
      "epoch": 0.6904945193653247,
      "grad_norm": 0.7035241723060608,
      "learning_rate": 3.849601743663585e-05,
      "loss": 1.2723,
      "step": 221550
    },
    {
      "epoch": 0.6906503520259805,
      "grad_norm": 0.7292667031288147,
      "learning_rate": 3.849342022562491e-05,
      "loss": 1.2471,
      "step": 221600
    },
    {
      "epoch": 0.6908061846866361,
      "grad_norm": 0.6675006747245789,
      "learning_rate": 3.849082301461399e-05,
      "loss": 1.2375,
      "step": 221650
    },
    {
      "epoch": 0.6909620173472918,
      "grad_norm": 0.7024587988853455,
      "learning_rate": 3.8488225803603065e-05,
      "loss": 1.2742,
      "step": 221700
    },
    {
      "epoch": 0.6911178500079475,
      "grad_norm": 0.523885190486908,
      "learning_rate": 3.848562859259213e-05,
      "loss": 1.2876,
      "step": 221750
    },
    {
      "epoch": 0.6912736826686031,
      "grad_norm": 0.569093644618988,
      "learning_rate": 3.84830313815812e-05,
      "loss": 1.2928,
      "step": 221800
    },
    {
      "epoch": 0.6914295153292588,
      "grad_norm": 0.5813813805580139,
      "learning_rate": 3.848043417057028e-05,
      "loss": 1.2438,
      "step": 221850
    },
    {
      "epoch": 0.6915853479899146,
      "grad_norm": 0.5783903002738953,
      "learning_rate": 3.847783695955935e-05,
      "loss": 1.2429,
      "step": 221900
    },
    {
      "epoch": 0.6917411806505702,
      "grad_norm": 0.6278117895126343,
      "learning_rate": 3.847523974854842e-05,
      "loss": 1.2915,
      "step": 221950
    },
    {
      "epoch": 0.6918970133112259,
      "grad_norm": 0.6161544322967529,
      "learning_rate": 3.847264253753749e-05,
      "loss": 1.2511,
      "step": 222000
    },
    {
      "epoch": 0.6920528459718815,
      "grad_norm": 0.5270116925239563,
      "learning_rate": 3.8470045326526565e-05,
      "loss": 1.2334,
      "step": 222050
    },
    {
      "epoch": 0.6922086786325372,
      "grad_norm": 0.6311718821525574,
      "learning_rate": 3.846744811551564e-05,
      "loss": 1.3165,
      "step": 222100
    },
    {
      "epoch": 0.6923645112931929,
      "grad_norm": 0.6617861390113831,
      "learning_rate": 3.8464850904504703e-05,
      "loss": 1.3311,
      "step": 222150
    },
    {
      "epoch": 0.6925203439538486,
      "grad_norm": 0.615720808506012,
      "learning_rate": 3.846225369349378e-05,
      "loss": 1.2822,
      "step": 222200
    },
    {
      "epoch": 0.6926761766145043,
      "grad_norm": 0.5483191013336182,
      "learning_rate": 3.8459656482482855e-05,
      "loss": 1.2391,
      "step": 222250
    },
    {
      "epoch": 0.69283200927516,
      "grad_norm": 0.6286429166793823,
      "learning_rate": 3.845705927147192e-05,
      "loss": 1.2629,
      "step": 222300
    },
    {
      "epoch": 0.6929878419358156,
      "grad_norm": 0.5428540110588074,
      "learning_rate": 3.8454462060460993e-05,
      "loss": 1.302,
      "step": 222350
    },
    {
      "epoch": 0.6931436745964713,
      "grad_norm": 0.5504327416419983,
      "learning_rate": 3.845186484945007e-05,
      "loss": 1.3328,
      "step": 222400
    },
    {
      "epoch": 0.693299507257127,
      "grad_norm": 0.6469354629516602,
      "learning_rate": 3.844926763843914e-05,
      "loss": 1.2691,
      "step": 222450
    },
    {
      "epoch": 0.6934553399177827,
      "grad_norm": 0.6416425704956055,
      "learning_rate": 3.844667042742821e-05,
      "loss": 1.2828,
      "step": 222500
    },
    {
      "epoch": 0.6936111725784384,
      "grad_norm": 0.6256413459777832,
      "learning_rate": 3.8444073216417284e-05,
      "loss": 1.2774,
      "step": 222550
    },
    {
      "epoch": 0.6937670052390941,
      "grad_norm": 0.667960524559021,
      "learning_rate": 3.8441476005406356e-05,
      "loss": 1.2658,
      "step": 222600
    },
    {
      "epoch": 0.6939228378997497,
      "grad_norm": 0.659234881401062,
      "learning_rate": 3.843887879439543e-05,
      "loss": 1.2546,
      "step": 222650
    },
    {
      "epoch": 0.6940786705604054,
      "grad_norm": 0.6028130650520325,
      "learning_rate": 3.84362815833845e-05,
      "loss": 1.2656,
      "step": 222700
    },
    {
      "epoch": 0.6942345032210611,
      "grad_norm": 0.6594247221946716,
      "learning_rate": 3.8433684372373574e-05,
      "loss": 1.2549,
      "step": 222750
    },
    {
      "epoch": 0.6943903358817167,
      "grad_norm": 0.5852530002593994,
      "learning_rate": 3.8431087161362646e-05,
      "loss": 1.2669,
      "step": 222800
    },
    {
      "epoch": 0.6945461685423725,
      "grad_norm": 0.6009590029716492,
      "learning_rate": 3.842848995035171e-05,
      "loss": 1.2641,
      "step": 222850
    },
    {
      "epoch": 0.6947020012030282,
      "grad_norm": 0.6564375758171082,
      "learning_rate": 3.842589273934079e-05,
      "loss": 1.2788,
      "step": 222900
    },
    {
      "epoch": 0.6948578338636838,
      "grad_norm": 0.6077991127967834,
      "learning_rate": 3.8423295528329864e-05,
      "loss": 1.2955,
      "step": 222950
    },
    {
      "epoch": 0.6950136665243395,
      "grad_norm": 0.6012656688690186,
      "learning_rate": 3.842069831731893e-05,
      "loss": 1.2873,
      "step": 223000
    },
    {
      "epoch": 0.6951694991849952,
      "grad_norm": 0.4961608052253723,
      "learning_rate": 3.8418101106308e-05,
      "loss": 1.221,
      "step": 223050
    },
    {
      "epoch": 0.6953253318456508,
      "grad_norm": 0.42529773712158203,
      "learning_rate": 3.841550389529708e-05,
      "loss": 1.2658,
      "step": 223100
    },
    {
      "epoch": 0.6954811645063066,
      "grad_norm": 0.6120665073394775,
      "learning_rate": 3.841290668428615e-05,
      "loss": 1.3032,
      "step": 223150
    },
    {
      "epoch": 0.6956369971669623,
      "grad_norm": 0.5443832278251648,
      "learning_rate": 3.841030947327522e-05,
      "loss": 1.2421,
      "step": 223200
    },
    {
      "epoch": 0.6957928298276179,
      "grad_norm": 0.6132510900497437,
      "learning_rate": 3.840771226226429e-05,
      "loss": 1.2303,
      "step": 223250
    },
    {
      "epoch": 0.6959486624882736,
      "grad_norm": 0.6513470411300659,
      "learning_rate": 3.8405115051253364e-05,
      "loss": 1.2848,
      "step": 223300
    },
    {
      "epoch": 0.6961044951489292,
      "grad_norm": 0.6049199104309082,
      "learning_rate": 3.840251784024244e-05,
      "loss": 1.2367,
      "step": 223350
    },
    {
      "epoch": 0.6962603278095849,
      "grad_norm": 0.6091359257698059,
      "learning_rate": 3.839992062923151e-05,
      "loss": 1.2931,
      "step": 223400
    },
    {
      "epoch": 0.6964161604702407,
      "grad_norm": 0.41910719871520996,
      "learning_rate": 3.839732341822058e-05,
      "loss": 1.2443,
      "step": 223450
    },
    {
      "epoch": 0.6965719931308963,
      "grad_norm": 0.6511192321777344,
      "learning_rate": 3.8394726207209654e-05,
      "loss": 1.2776,
      "step": 223500
    },
    {
      "epoch": 0.696727825791552,
      "grad_norm": 0.7182865142822266,
      "learning_rate": 3.839212899619872e-05,
      "loss": 1.217,
      "step": 223550
    },
    {
      "epoch": 0.6968836584522077,
      "grad_norm": 0.6366153955459595,
      "learning_rate": 3.838953178518779e-05,
      "loss": 1.2322,
      "step": 223600
    },
    {
      "epoch": 0.6970394911128633,
      "grad_norm": 0.6008146405220032,
      "learning_rate": 3.838693457417687e-05,
      "loss": 1.2624,
      "step": 223650
    },
    {
      "epoch": 0.697195323773519,
      "grad_norm": 0.7014869451522827,
      "learning_rate": 3.838433736316594e-05,
      "loss": 1.2808,
      "step": 223700
    },
    {
      "epoch": 0.6973511564341748,
      "grad_norm": 0.6423860192298889,
      "learning_rate": 3.838174015215501e-05,
      "loss": 1.2702,
      "step": 223750
    },
    {
      "epoch": 0.6975069890948304,
      "grad_norm": 0.6878579258918762,
      "learning_rate": 3.837914294114409e-05,
      "loss": 1.2482,
      "step": 223800
    },
    {
      "epoch": 0.6976628217554861,
      "grad_norm": 0.7670820951461792,
      "learning_rate": 3.8376545730133155e-05,
      "loss": 1.2742,
      "step": 223850
    },
    {
      "epoch": 0.6978186544161418,
      "grad_norm": 0.5180837512016296,
      "learning_rate": 3.837394851912223e-05,
      "loss": 1.264,
      "step": 223900
    },
    {
      "epoch": 0.6979744870767974,
      "grad_norm": 0.653955340385437,
      "learning_rate": 3.83713513081113e-05,
      "loss": 1.2714,
      "step": 223950
    },
    {
      "epoch": 0.6981303197374531,
      "grad_norm": 0.6489236950874329,
      "learning_rate": 3.836875409710037e-05,
      "loss": 1.3074,
      "step": 224000
    },
    {
      "epoch": 0.6982861523981088,
      "grad_norm": 0.6175026297569275,
      "learning_rate": 3.8366156886089445e-05,
      "loss": 1.2527,
      "step": 224050
    },
    {
      "epoch": 0.6984419850587645,
      "grad_norm": 0.5651133060455322,
      "learning_rate": 3.836355967507852e-05,
      "loss": 1.2442,
      "step": 224100
    },
    {
      "epoch": 0.6985978177194202,
      "grad_norm": 0.5451850295066833,
      "learning_rate": 3.836096246406759e-05,
      "loss": 1.254,
      "step": 224150
    },
    {
      "epoch": 0.6987536503800759,
      "grad_norm": 0.6190124750137329,
      "learning_rate": 3.835836525305666e-05,
      "loss": 1.2416,
      "step": 224200
    },
    {
      "epoch": 0.6989094830407315,
      "grad_norm": 0.5320219993591309,
      "learning_rate": 3.835576804204573e-05,
      "loss": 1.3043,
      "step": 224250
    },
    {
      "epoch": 0.6990653157013872,
      "grad_norm": 0.6411357522010803,
      "learning_rate": 3.83531708310348e-05,
      "loss": 1.2923,
      "step": 224300
    },
    {
      "epoch": 0.6992211483620429,
      "grad_norm": 0.6765016913414001,
      "learning_rate": 3.835057362002388e-05,
      "loss": 1.2504,
      "step": 224350
    },
    {
      "epoch": 0.6993769810226986,
      "grad_norm": 0.532819926738739,
      "learning_rate": 3.8347976409012946e-05,
      "loss": 1.2844,
      "step": 224400
    },
    {
      "epoch": 0.6995328136833543,
      "grad_norm": 0.5990460515022278,
      "learning_rate": 3.834537919800202e-05,
      "loss": 1.2847,
      "step": 224450
    },
    {
      "epoch": 0.69968864634401,
      "grad_norm": 0.5987202525138855,
      "learning_rate": 3.834278198699109e-05,
      "loss": 1.2566,
      "step": 224500
    },
    {
      "epoch": 0.6998444790046656,
      "grad_norm": 0.5193474888801575,
      "learning_rate": 3.834018477598016e-05,
      "loss": 1.3642,
      "step": 224550
    },
    {
      "epoch": 0.7000003116653213,
      "grad_norm": 0.6035270690917969,
      "learning_rate": 3.8337587564969236e-05,
      "loss": 1.2572,
      "step": 224600
    },
    {
      "epoch": 0.700156144325977,
      "grad_norm": 0.5057016015052795,
      "learning_rate": 3.833499035395831e-05,
      "loss": 1.3057,
      "step": 224650
    },
    {
      "epoch": 0.7003119769866327,
      "grad_norm": 0.6251031756401062,
      "learning_rate": 3.833239314294738e-05,
      "loss": 1.2978,
      "step": 224700
    },
    {
      "epoch": 0.7004678096472884,
      "grad_norm": 0.6075912117958069,
      "learning_rate": 3.832979593193645e-05,
      "loss": 1.3083,
      "step": 224750
    },
    {
      "epoch": 0.700623642307944,
      "grad_norm": 0.5882459282875061,
      "learning_rate": 3.832719872092552e-05,
      "loss": 1.3328,
      "step": 224800
    },
    {
      "epoch": 0.7007794749685997,
      "grad_norm": 0.6395958065986633,
      "learning_rate": 3.832460150991459e-05,
      "loss": 1.2859,
      "step": 224850
    },
    {
      "epoch": 0.7009353076292554,
      "grad_norm": 0.6376960277557373,
      "learning_rate": 3.832200429890367e-05,
      "loss": 1.2849,
      "step": 224900
    },
    {
      "epoch": 0.701091140289911,
      "grad_norm": 0.560437023639679,
      "learning_rate": 3.8319407087892737e-05,
      "loss": 1.2756,
      "step": 224950
    },
    {
      "epoch": 0.7012469729505668,
      "grad_norm": 0.46070119738578796,
      "learning_rate": 3.831680987688181e-05,
      "loss": 1.301,
      "step": 225000
    },
    {
      "epoch": 0.7014028056112225,
      "grad_norm": 0.5219460725784302,
      "learning_rate": 3.831421266587089e-05,
      "loss": 1.2837,
      "step": 225050
    },
    {
      "epoch": 0.7015586382718781,
      "grad_norm": 0.6471385955810547,
      "learning_rate": 3.8311615454859954e-05,
      "loss": 1.329,
      "step": 225100
    },
    {
      "epoch": 0.7017144709325338,
      "grad_norm": 0.602907657623291,
      "learning_rate": 3.8309018243849027e-05,
      "loss": 1.2864,
      "step": 225150
    },
    {
      "epoch": 0.7018703035931895,
      "grad_norm": 0.5850275158882141,
      "learning_rate": 3.83064210328381e-05,
      "loss": 1.2871,
      "step": 225200
    },
    {
      "epoch": 0.7020261362538451,
      "grad_norm": 0.6235746741294861,
      "learning_rate": 3.830382382182717e-05,
      "loss": 1.2923,
      "step": 225250
    },
    {
      "epoch": 0.7021819689145008,
      "grad_norm": 0.5277842879295349,
      "learning_rate": 3.8301226610816244e-05,
      "loss": 1.2701,
      "step": 225300
    },
    {
      "epoch": 0.7023378015751566,
      "grad_norm": 0.6235537528991699,
      "learning_rate": 3.8298629399805317e-05,
      "loss": 1.2875,
      "step": 225350
    },
    {
      "epoch": 0.7024936342358122,
      "grad_norm": 0.6166501045227051,
      "learning_rate": 3.829603218879439e-05,
      "loss": 1.2456,
      "step": 225400
    },
    {
      "epoch": 0.7026494668964679,
      "grad_norm": 0.725633978843689,
      "learning_rate": 3.829343497778346e-05,
      "loss": 1.279,
      "step": 225450
    },
    {
      "epoch": 0.7028052995571236,
      "grad_norm": 0.5163593292236328,
      "learning_rate": 3.829083776677253e-05,
      "loss": 1.2653,
      "step": 225500
    },
    {
      "epoch": 0.7029611322177792,
      "grad_norm": 0.5590040683746338,
      "learning_rate": 3.82882405557616e-05,
      "loss": 1.2858,
      "step": 225550
    },
    {
      "epoch": 0.7031169648784349,
      "grad_norm": 0.7326391935348511,
      "learning_rate": 3.828564334475068e-05,
      "loss": 1.3056,
      "step": 225600
    },
    {
      "epoch": 0.7032727975390907,
      "grad_norm": 0.5964632034301758,
      "learning_rate": 3.8283046133739745e-05,
      "loss": 1.2613,
      "step": 225650
    },
    {
      "epoch": 0.7034286301997463,
      "grad_norm": 0.6270112991333008,
      "learning_rate": 3.828044892272882e-05,
      "loss": 1.2937,
      "step": 225700
    },
    {
      "epoch": 0.703584462860402,
      "grad_norm": 0.5926751494407654,
      "learning_rate": 3.827785171171789e-05,
      "loss": 1.2119,
      "step": 225750
    },
    {
      "epoch": 0.7037402955210577,
      "grad_norm": 0.6010657548904419,
      "learning_rate": 3.827525450070696e-05,
      "loss": 1.2812,
      "step": 225800
    },
    {
      "epoch": 0.7038961281817133,
      "grad_norm": 0.5202394127845764,
      "learning_rate": 3.8272657289696035e-05,
      "loss": 1.2732,
      "step": 225850
    },
    {
      "epoch": 0.704051960842369,
      "grad_norm": 0.5914682745933533,
      "learning_rate": 3.827006007868511e-05,
      "loss": 1.145,
      "step": 225900
    },
    {
      "epoch": 0.7042077935030248,
      "grad_norm": 0.6307606101036072,
      "learning_rate": 3.826746286767418e-05,
      "loss": 1.3625,
      "step": 225950
    },
    {
      "epoch": 0.7043636261636804,
      "grad_norm": 0.5807955265045166,
      "learning_rate": 3.826486565666325e-05,
      "loss": 1.2744,
      "step": 226000
    },
    {
      "epoch": 0.7045194588243361,
      "grad_norm": 0.5069102048873901,
      "learning_rate": 3.8262320389872544e-05,
      "loss": 1.2759,
      "step": 226050
    },
    {
      "epoch": 0.7046752914849918,
      "grad_norm": 0.738965630531311,
      "learning_rate": 3.825972317886161e-05,
      "loss": 1.3023,
      "step": 226100
    },
    {
      "epoch": 0.7048311241456474,
      "grad_norm": 0.6013668179512024,
      "learning_rate": 3.825712596785069e-05,
      "loss": 1.2888,
      "step": 226150
    },
    {
      "epoch": 0.7049869568063031,
      "grad_norm": 0.6123942732810974,
      "learning_rate": 3.8254528756839755e-05,
      "loss": 1.2957,
      "step": 226200
    },
    {
      "epoch": 0.7051427894669589,
      "grad_norm": 0.6795884370803833,
      "learning_rate": 3.825193154582883e-05,
      "loss": 1.2173,
      "step": 226250
    },
    {
      "epoch": 0.7052986221276145,
      "grad_norm": 0.6390951871871948,
      "learning_rate": 3.82493343348179e-05,
      "loss": 1.261,
      "step": 226300
    },
    {
      "epoch": 0.7054544547882702,
      "grad_norm": 0.6322436928749084,
      "learning_rate": 3.824673712380697e-05,
      "loss": 1.2506,
      "step": 226350
    },
    {
      "epoch": 0.7056102874489258,
      "grad_norm": 0.6854349970817566,
      "learning_rate": 3.8244139912796045e-05,
      "loss": 1.2903,
      "step": 226400
    },
    {
      "epoch": 0.7057661201095815,
      "grad_norm": 0.6224084496498108,
      "learning_rate": 3.824154270178512e-05,
      "loss": 1.2815,
      "step": 226450
    },
    {
      "epoch": 0.7059219527702372,
      "grad_norm": 0.702999472618103,
      "learning_rate": 3.8238945490774183e-05,
      "loss": 1.298,
      "step": 226500
    },
    {
      "epoch": 0.7060777854308928,
      "grad_norm": 0.6487095952033997,
      "learning_rate": 3.823634827976326e-05,
      "loss": 1.2231,
      "step": 226550
    },
    {
      "epoch": 0.7062336180915486,
      "grad_norm": 0.6374621391296387,
      "learning_rate": 3.8233751068752335e-05,
      "loss": 1.2847,
      "step": 226600
    },
    {
      "epoch": 0.7063894507522043,
      "grad_norm": 0.5197203755378723,
      "learning_rate": 3.82311538577414e-05,
      "loss": 1.2866,
      "step": 226650
    },
    {
      "epoch": 0.7065452834128599,
      "grad_norm": 0.601650059223175,
      "learning_rate": 3.822855664673048e-05,
      "loss": 1.3066,
      "step": 226700
    },
    {
      "epoch": 0.7067011160735156,
      "grad_norm": 0.7000349164009094,
      "learning_rate": 3.822595943571955e-05,
      "loss": 1.2593,
      "step": 226750
    },
    {
      "epoch": 0.7068569487341713,
      "grad_norm": 0.6223819851875305,
      "learning_rate": 3.822336222470862e-05,
      "loss": 1.2997,
      "step": 226800
    },
    {
      "epoch": 0.7070127813948269,
      "grad_norm": 0.5522267818450928,
      "learning_rate": 3.822076501369769e-05,
      "loss": 1.2457,
      "step": 226850
    },
    {
      "epoch": 0.7071686140554827,
      "grad_norm": 0.5817459225654602,
      "learning_rate": 3.8218167802686763e-05,
      "loss": 1.2937,
      "step": 226900
    },
    {
      "epoch": 0.7073244467161384,
      "grad_norm": 0.6363166570663452,
      "learning_rate": 3.8215570591675836e-05,
      "loss": 1.2487,
      "step": 226950
    },
    {
      "epoch": 0.707480279376794,
      "grad_norm": 0.6149252653121948,
      "learning_rate": 3.821297338066491e-05,
      "loss": 1.2771,
      "step": 227000
    },
    {
      "epoch": 0.7076361120374497,
      "grad_norm": 0.6100028157234192,
      "learning_rate": 3.821037616965398e-05,
      "loss": 1.2923,
      "step": 227050
    },
    {
      "epoch": 0.7077919446981054,
      "grad_norm": 0.5575994253158569,
      "learning_rate": 3.8207778958643053e-05,
      "loss": 1.2913,
      "step": 227100
    },
    {
      "epoch": 0.707947777358761,
      "grad_norm": 0.6423393487930298,
      "learning_rate": 3.8205181747632126e-05,
      "loss": 1.2699,
      "step": 227150
    },
    {
      "epoch": 0.7081036100194168,
      "grad_norm": 0.537501335144043,
      "learning_rate": 3.820258453662119e-05,
      "loss": 1.2721,
      "step": 227200
    },
    {
      "epoch": 0.7082594426800725,
      "grad_norm": 0.6297368407249451,
      "learning_rate": 3.819998732561027e-05,
      "loss": 1.2849,
      "step": 227250
    },
    {
      "epoch": 0.7084152753407281,
      "grad_norm": 0.6871578097343445,
      "learning_rate": 3.8197390114599343e-05,
      "loss": 1.3028,
      "step": 227300
    },
    {
      "epoch": 0.7085711080013838,
      "grad_norm": 0.7617630958557129,
      "learning_rate": 3.819479290358841e-05,
      "loss": 1.2825,
      "step": 227350
    },
    {
      "epoch": 0.7087269406620395,
      "grad_norm": 0.45117273926734924,
      "learning_rate": 3.819219569257749e-05,
      "loss": 1.2934,
      "step": 227400
    },
    {
      "epoch": 0.7088827733226951,
      "grad_norm": 0.4761255979537964,
      "learning_rate": 3.8189598481566554e-05,
      "loss": 1.285,
      "step": 227450
    },
    {
      "epoch": 0.7090386059833509,
      "grad_norm": 0.571709394454956,
      "learning_rate": 3.818700127055563e-05,
      "loss": 1.3016,
      "step": 227500
    },
    {
      "epoch": 0.7091944386440066,
      "grad_norm": 0.6368414759635925,
      "learning_rate": 3.81844040595447e-05,
      "loss": 1.2625,
      "step": 227550
    },
    {
      "epoch": 0.7093502713046622,
      "grad_norm": 0.5719836354255676,
      "learning_rate": 3.818180684853377e-05,
      "loss": 1.2992,
      "step": 227600
    },
    {
      "epoch": 0.7095061039653179,
      "grad_norm": 0.614700436592102,
      "learning_rate": 3.8179209637522844e-05,
      "loss": 1.2464,
      "step": 227650
    },
    {
      "epoch": 0.7096619366259735,
      "grad_norm": 0.6490445137023926,
      "learning_rate": 3.8176664370732136e-05,
      "loss": 1.2678,
      "step": 227700
    },
    {
      "epoch": 0.7098177692866292,
      "grad_norm": 0.6247842907905579,
      "learning_rate": 3.81740671597212e-05,
      "loss": 1.2913,
      "step": 227750
    },
    {
      "epoch": 0.709973601947285,
      "grad_norm": 0.566092848777771,
      "learning_rate": 3.817146994871028e-05,
      "loss": 1.2732,
      "step": 227800
    },
    {
      "epoch": 0.7101294346079406,
      "grad_norm": 0.8039466142654419,
      "learning_rate": 3.8168872737699354e-05,
      "loss": 1.2905,
      "step": 227850
    },
    {
      "epoch": 0.7102852672685963,
      "grad_norm": 0.6702989339828491,
      "learning_rate": 3.816627552668842e-05,
      "loss": 1.3079,
      "step": 227900
    },
    {
      "epoch": 0.710441099929252,
      "grad_norm": 0.5750254392623901,
      "learning_rate": 3.816367831567749e-05,
      "loss": 1.2821,
      "step": 227950
    },
    {
      "epoch": 0.7105969325899076,
      "grad_norm": 0.6540876030921936,
      "learning_rate": 3.816108110466657e-05,
      "loss": 1.291,
      "step": 228000
    },
    {
      "epoch": 0.7107527652505633,
      "grad_norm": 0.6611250042915344,
      "learning_rate": 3.815848389365564e-05,
      "loss": 1.2542,
      "step": 228050
    },
    {
      "epoch": 0.710908597911219,
      "grad_norm": 0.6614908576011658,
      "learning_rate": 3.815588668264471e-05,
      "loss": 1.3009,
      "step": 228100
    },
    {
      "epoch": 0.7110644305718747,
      "grad_norm": 0.5952023267745972,
      "learning_rate": 3.815328947163378e-05,
      "loss": 1.2377,
      "step": 228150
    },
    {
      "epoch": 0.7112202632325304,
      "grad_norm": 0.6825924515724182,
      "learning_rate": 3.8150692260622855e-05,
      "loss": 1.2692,
      "step": 228200
    },
    {
      "epoch": 0.7113760958931861,
      "grad_norm": 0.45141562819480896,
      "learning_rate": 3.814809504961193e-05,
      "loss": 1.2643,
      "step": 228250
    },
    {
      "epoch": 0.7115319285538417,
      "grad_norm": 0.46163153648376465,
      "learning_rate": 3.8145497838601e-05,
      "loss": 1.2628,
      "step": 228300
    },
    {
      "epoch": 0.7116877612144974,
      "grad_norm": 0.5964059233665466,
      "learning_rate": 3.814290062759007e-05,
      "loss": 1.2573,
      "step": 228350
    },
    {
      "epoch": 0.7118435938751531,
      "grad_norm": 0.48968350887298584,
      "learning_rate": 3.8140303416579145e-05,
      "loss": 1.2796,
      "step": 228400
    },
    {
      "epoch": 0.7119994265358088,
      "grad_norm": 0.5624823570251465,
      "learning_rate": 3.813770620556821e-05,
      "loss": 1.3419,
      "step": 228450
    },
    {
      "epoch": 0.7121552591964645,
      "grad_norm": 0.6525262594223022,
      "learning_rate": 3.813510899455728e-05,
      "loss": 1.2749,
      "step": 228500
    },
    {
      "epoch": 0.7123110918571202,
      "grad_norm": 0.561313807964325,
      "learning_rate": 3.813251178354636e-05,
      "loss": 1.2619,
      "step": 228550
    },
    {
      "epoch": 0.7124669245177758,
      "grad_norm": 0.6477123498916626,
      "learning_rate": 3.812991457253543e-05,
      "loss": 1.325,
      "step": 228600
    },
    {
      "epoch": 0.7126227571784315,
      "grad_norm": 0.608691930770874,
      "learning_rate": 3.81273173615245e-05,
      "loss": 1.256,
      "step": 228650
    },
    {
      "epoch": 0.7127785898390872,
      "grad_norm": 0.6788438558578491,
      "learning_rate": 3.812472015051358e-05,
      "loss": 1.2703,
      "step": 228700
    },
    {
      "epoch": 0.7129344224997429,
      "grad_norm": 0.6678411960601807,
      "learning_rate": 3.8122122939502645e-05,
      "loss": 1.3172,
      "step": 228750
    },
    {
      "epoch": 0.7130902551603986,
      "grad_norm": 0.8827277421951294,
      "learning_rate": 3.811952572849172e-05,
      "loss": 1.2636,
      "step": 228800
    },
    {
      "epoch": 0.7132460878210543,
      "grad_norm": 0.5275673866271973,
      "learning_rate": 3.811692851748079e-05,
      "loss": 1.2838,
      "step": 228850
    },
    {
      "epoch": 0.7134019204817099,
      "grad_norm": 0.5283324122428894,
      "learning_rate": 3.811433130646986e-05,
      "loss": 1.2206,
      "step": 228900
    },
    {
      "epoch": 0.7135577531423656,
      "grad_norm": 0.7082450985908508,
      "learning_rate": 3.8111734095458935e-05,
      "loss": 1.2798,
      "step": 228950
    },
    {
      "epoch": 0.7137135858030212,
      "grad_norm": 0.703050971031189,
      "learning_rate": 3.810913688444801e-05,
      "loss": 1.2954,
      "step": 229000
    },
    {
      "epoch": 0.713869418463677,
      "grad_norm": 0.597017228603363,
      "learning_rate": 3.810653967343708e-05,
      "loss": 1.301,
      "step": 229050
    },
    {
      "epoch": 0.7140252511243327,
      "grad_norm": 0.41890621185302734,
      "learning_rate": 3.810394246242615e-05,
      "loss": 1.2543,
      "step": 229100
    },
    {
      "epoch": 0.7141810837849883,
      "grad_norm": 0.625853955745697,
      "learning_rate": 3.810134525141522e-05,
      "loss": 1.2286,
      "step": 229150
    },
    {
      "epoch": 0.714336916445644,
      "grad_norm": 0.6552789807319641,
      "learning_rate": 3.809874804040429e-05,
      "loss": 1.3235,
      "step": 229200
    },
    {
      "epoch": 0.7144927491062997,
      "grad_norm": 0.593352198600769,
      "learning_rate": 3.809615082939337e-05,
      "loss": 1.274,
      "step": 229250
    },
    {
      "epoch": 0.7146485817669553,
      "grad_norm": 0.5969127416610718,
      "learning_rate": 3.8093553618382436e-05,
      "loss": 1.3098,
      "step": 229300
    },
    {
      "epoch": 0.714804414427611,
      "grad_norm": 0.8107829689979553,
      "learning_rate": 3.809095640737151e-05,
      "loss": 1.3247,
      "step": 229350
    },
    {
      "epoch": 0.7149602470882668,
      "grad_norm": 0.6142402291297913,
      "learning_rate": 3.808835919636059e-05,
      "loss": 1.2555,
      "step": 229400
    },
    {
      "epoch": 0.7151160797489224,
      "grad_norm": 0.7837262749671936,
      "learning_rate": 3.8085761985349654e-05,
      "loss": 1.2374,
      "step": 229450
    },
    {
      "epoch": 0.7152719124095781,
      "grad_norm": 0.5677619576454163,
      "learning_rate": 3.8083164774338726e-05,
      "loss": 1.2498,
      "step": 229500
    },
    {
      "epoch": 0.7154277450702338,
      "grad_norm": 0.5427860617637634,
      "learning_rate": 3.80805675633278e-05,
      "loss": 1.2467,
      "step": 229550
    },
    {
      "epoch": 0.7155835777308894,
      "grad_norm": 0.4852469861507416,
      "learning_rate": 3.807797035231687e-05,
      "loss": 1.277,
      "step": 229600
    },
    {
      "epoch": 0.7157394103915451,
      "grad_norm": 0.7147624492645264,
      "learning_rate": 3.8075373141305944e-05,
      "loss": 1.2875,
      "step": 229650
    },
    {
      "epoch": 0.7158952430522009,
      "grad_norm": 0.5148875713348389,
      "learning_rate": 3.807277593029501e-05,
      "loss": 1.3167,
      "step": 229700
    },
    {
      "epoch": 0.7160510757128565,
      "grad_norm": 0.5253544449806213,
      "learning_rate": 3.807017871928408e-05,
      "loss": 1.313,
      "step": 229750
    },
    {
      "epoch": 0.7162069083735122,
      "grad_norm": 0.5777243971824646,
      "learning_rate": 3.806758150827316e-05,
      "loss": 1.2737,
      "step": 229800
    },
    {
      "epoch": 0.7163627410341679,
      "grad_norm": 0.667147696018219,
      "learning_rate": 3.806498429726223e-05,
      "loss": 1.2707,
      "step": 229850
    },
    {
      "epoch": 0.7165185736948235,
      "grad_norm": 0.8227442502975464,
      "learning_rate": 3.80623870862513e-05,
      "loss": 1.2792,
      "step": 229900
    },
    {
      "epoch": 0.7166744063554792,
      "grad_norm": 0.5788028836250305,
      "learning_rate": 3.805978987524038e-05,
      "loss": 1.3309,
      "step": 229950
    },
    {
      "epoch": 0.716830239016135,
      "grad_norm": 0.4782269299030304,
      "learning_rate": 3.8057192664229444e-05,
      "loss": 1.3225,
      "step": 230000
    },
    {
      "epoch": 0.7169860716767906,
      "grad_norm": 0.644482433795929,
      "learning_rate": 3.805459545321852e-05,
      "loss": 1.2577,
      "step": 230050
    },
    {
      "epoch": 0.7171419043374463,
      "grad_norm": 0.7161645293235779,
      "learning_rate": 3.805199824220759e-05,
      "loss": 1.2947,
      "step": 230100
    },
    {
      "epoch": 0.717297736998102,
      "grad_norm": 0.5881797075271606,
      "learning_rate": 3.804940103119666e-05,
      "loss": 1.2037,
      "step": 230150
    },
    {
      "epoch": 0.7174535696587576,
      "grad_norm": 0.6862604022026062,
      "learning_rate": 3.8046803820185734e-05,
      "loss": 1.2922,
      "step": 230200
    },
    {
      "epoch": 0.7176094023194133,
      "grad_norm": 0.6903862357139587,
      "learning_rate": 3.804420660917481e-05,
      "loss": 1.3188,
      "step": 230250
    },
    {
      "epoch": 0.7177652349800691,
      "grad_norm": 0.6148134469985962,
      "learning_rate": 3.804160939816388e-05,
      "loss": 1.2388,
      "step": 230300
    },
    {
      "epoch": 0.7179210676407247,
      "grad_norm": 0.5309343934059143,
      "learning_rate": 3.803901218715295e-05,
      "loss": 1.2732,
      "step": 230350
    },
    {
      "epoch": 0.7180769003013804,
      "grad_norm": 0.5083359479904175,
      "learning_rate": 3.803641497614202e-05,
      "loss": 1.2945,
      "step": 230400
    },
    {
      "epoch": 0.718232732962036,
      "grad_norm": 0.7229200601577759,
      "learning_rate": 3.803381776513109e-05,
      "loss": 1.319,
      "step": 230450
    },
    {
      "epoch": 0.7183885656226917,
      "grad_norm": 0.5950010418891907,
      "learning_rate": 3.803122055412017e-05,
      "loss": 1.283,
      "step": 230500
    },
    {
      "epoch": 0.7185443982833474,
      "grad_norm": 0.6700016856193542,
      "learning_rate": 3.8028623343109235e-05,
      "loss": 1.252,
      "step": 230550
    },
    {
      "epoch": 0.718700230944003,
      "grad_norm": 0.45499005913734436,
      "learning_rate": 3.802602613209831e-05,
      "loss": 1.2965,
      "step": 230600
    },
    {
      "epoch": 0.7188560636046588,
      "grad_norm": 0.4355093538761139,
      "learning_rate": 3.802342892108739e-05,
      "loss": 1.2471,
      "step": 230650
    },
    {
      "epoch": 0.7190118962653145,
      "grad_norm": 0.6919988989830017,
      "learning_rate": 3.802083171007645e-05,
      "loss": 1.2817,
      "step": 230700
    },
    {
      "epoch": 0.7191677289259701,
      "grad_norm": 0.6361396312713623,
      "learning_rate": 3.8018234499065525e-05,
      "loss": 1.2607,
      "step": 230750
    },
    {
      "epoch": 0.7193235615866258,
      "grad_norm": 0.6837528347969055,
      "learning_rate": 3.80156372880546e-05,
      "loss": 1.257,
      "step": 230800
    },
    {
      "epoch": 0.7194793942472815,
      "grad_norm": 0.6663384437561035,
      "learning_rate": 3.801304007704367e-05,
      "loss": 1.2838,
      "step": 230850
    },
    {
      "epoch": 0.7196352269079371,
      "grad_norm": 0.5854607224464417,
      "learning_rate": 3.801044286603274e-05,
      "loss": 1.24,
      "step": 230900
    },
    {
      "epoch": 0.7197910595685929,
      "grad_norm": 0.5468781590461731,
      "learning_rate": 3.8007845655021815e-05,
      "loss": 1.2983,
      "step": 230950
    },
    {
      "epoch": 0.7199468922292486,
      "grad_norm": 0.6555346846580505,
      "learning_rate": 3.800524844401088e-05,
      "loss": 1.2433,
      "step": 231000
    },
    {
      "epoch": 0.7201027248899042,
      "grad_norm": 0.6332968473434448,
      "learning_rate": 3.800265123299996e-05,
      "loss": 1.3329,
      "step": 231050
    },
    {
      "epoch": 0.7202585575505599,
      "grad_norm": 0.4929444491863251,
      "learning_rate": 3.8000054021989026e-05,
      "loss": 1.3121,
      "step": 231100
    },
    {
      "epoch": 0.7204143902112156,
      "grad_norm": 0.6857550144195557,
      "learning_rate": 3.79974568109781e-05,
      "loss": 1.2068,
      "step": 231150
    },
    {
      "epoch": 0.7205702228718712,
      "grad_norm": 0.5389072299003601,
      "learning_rate": 3.799485959996718e-05,
      "loss": 1.3154,
      "step": 231200
    },
    {
      "epoch": 0.720726055532527,
      "grad_norm": 0.46658870577812195,
      "learning_rate": 3.7992262388956243e-05,
      "loss": 1.2512,
      "step": 231250
    },
    {
      "epoch": 0.7208818881931827,
      "grad_norm": 0.7349151968955994,
      "learning_rate": 3.7989665177945316e-05,
      "loss": 1.2454,
      "step": 231300
    },
    {
      "epoch": 0.7210377208538383,
      "grad_norm": 0.6219954490661621,
      "learning_rate": 3.798706796693439e-05,
      "loss": 1.3162,
      "step": 231350
    },
    {
      "epoch": 0.721193553514494,
      "grad_norm": 0.7162017226219177,
      "learning_rate": 3.798447075592346e-05,
      "loss": 1.2769,
      "step": 231400
    },
    {
      "epoch": 0.7213493861751497,
      "grad_norm": 0.6069883108139038,
      "learning_rate": 3.7981873544912533e-05,
      "loss": 1.2446,
      "step": 231450
    },
    {
      "epoch": 0.7215052188358053,
      "grad_norm": 0.5355592966079712,
      "learning_rate": 3.7979276333901606e-05,
      "loss": 1.3408,
      "step": 231500
    },
    {
      "epoch": 0.7216610514964611,
      "grad_norm": 0.5878487229347229,
      "learning_rate": 3.797667912289068e-05,
      "loss": 1.2431,
      "step": 231550
    },
    {
      "epoch": 0.7218168841571168,
      "grad_norm": 0.5449097156524658,
      "learning_rate": 3.797408191187975e-05,
      "loss": 1.2654,
      "step": 231600
    },
    {
      "epoch": 0.7219727168177724,
      "grad_norm": 0.7773796319961548,
      "learning_rate": 3.7971484700868823e-05,
      "loss": 1.2685,
      "step": 231650
    },
    {
      "epoch": 0.7221285494784281,
      "grad_norm": 0.5281091928482056,
      "learning_rate": 3.796888748985789e-05,
      "loss": 1.321,
      "step": 231700
    },
    {
      "epoch": 0.7222843821390837,
      "grad_norm": 0.6310574412345886,
      "learning_rate": 3.796629027884697e-05,
      "loss": 1.2519,
      "step": 231750
    },
    {
      "epoch": 0.7224402147997394,
      "grad_norm": 0.669415295124054,
      "learning_rate": 3.7963693067836034e-05,
      "loss": 1.2727,
      "step": 231800
    },
    {
      "epoch": 0.7225960474603951,
      "grad_norm": 0.6428513526916504,
      "learning_rate": 3.796109585682511e-05,
      "loss": 1.2708,
      "step": 231850
    },
    {
      "epoch": 0.7227518801210508,
      "grad_norm": 0.7619204521179199,
      "learning_rate": 3.7958498645814186e-05,
      "loss": 1.2642,
      "step": 231900
    },
    {
      "epoch": 0.7229077127817065,
      "grad_norm": 0.6070218682289124,
      "learning_rate": 3.795590143480325e-05,
      "loss": 1.271,
      "step": 231950
    },
    {
      "epoch": 0.7230635454423622,
      "grad_norm": 0.7615216374397278,
      "learning_rate": 3.7953304223792324e-05,
      "loss": 1.2241,
      "step": 232000
    },
    {
      "epoch": 0.7232193781030178,
      "grad_norm": 0.6819813847541809,
      "learning_rate": 3.79507070127814e-05,
      "loss": 1.2807,
      "step": 232050
    },
    {
      "epoch": 0.7233752107636735,
      "grad_norm": 0.6213021874427795,
      "learning_rate": 3.794810980177047e-05,
      "loss": 1.2487,
      "step": 232100
    },
    {
      "epoch": 0.7235310434243292,
      "grad_norm": 0.6504143476486206,
      "learning_rate": 3.794551259075954e-05,
      "loss": 1.2745,
      "step": 232150
    },
    {
      "epoch": 0.7236868760849849,
      "grad_norm": 0.5752540230751038,
      "learning_rate": 3.7942967323968834e-05,
      "loss": 1.3181,
      "step": 232200
    },
    {
      "epoch": 0.7238427087456406,
      "grad_norm": 0.6899639964103699,
      "learning_rate": 3.79403701129579e-05,
      "loss": 1.3225,
      "step": 232250
    },
    {
      "epoch": 0.7239985414062963,
      "grad_norm": 0.7159543037414551,
      "learning_rate": 3.793777290194698e-05,
      "loss": 1.2787,
      "step": 232300
    },
    {
      "epoch": 0.7241543740669519,
      "grad_norm": 0.6237257719039917,
      "learning_rate": 3.7935175690936045e-05,
      "loss": 1.247,
      "step": 232350
    },
    {
      "epoch": 0.7243102067276076,
      "grad_norm": 0.4832223951816559,
      "learning_rate": 3.793257847992512e-05,
      "loss": 1.2649,
      "step": 232400
    },
    {
      "epoch": 0.7244660393882633,
      "grad_norm": 0.61541748046875,
      "learning_rate": 3.792998126891419e-05,
      "loss": 1.2765,
      "step": 232450
    },
    {
      "epoch": 0.724621872048919,
      "grad_norm": 0.7196128964424133,
      "learning_rate": 3.792738405790326e-05,
      "loss": 1.2626,
      "step": 232500
    },
    {
      "epoch": 0.7247777047095747,
      "grad_norm": 0.6942976117134094,
      "learning_rate": 3.7924786846892335e-05,
      "loss": 1.2617,
      "step": 232550
    },
    {
      "epoch": 0.7249335373702304,
      "grad_norm": 0.5318410396575928,
      "learning_rate": 3.792218963588141e-05,
      "loss": 1.3065,
      "step": 232600
    },
    {
      "epoch": 0.725089370030886,
      "grad_norm": 0.5727712512016296,
      "learning_rate": 3.791959242487048e-05,
      "loss": 1.2538,
      "step": 232650
    },
    {
      "epoch": 0.7252452026915417,
      "grad_norm": 0.6953859329223633,
      "learning_rate": 3.791699521385955e-05,
      "loss": 1.317,
      "step": 232700
    },
    {
      "epoch": 0.7254010353521974,
      "grad_norm": 0.4183025658130646,
      "learning_rate": 3.7914398002848625e-05,
      "loss": 1.2812,
      "step": 232750
    },
    {
      "epoch": 0.7255568680128531,
      "grad_norm": 0.41451674699783325,
      "learning_rate": 3.791180079183769e-05,
      "loss": 1.3016,
      "step": 232800
    },
    {
      "epoch": 0.7257127006735088,
      "grad_norm": 0.5146942734718323,
      "learning_rate": 3.790920358082677e-05,
      "loss": 1.2753,
      "step": 232850
    },
    {
      "epoch": 0.7258685333341645,
      "grad_norm": 0.6299505829811096,
      "learning_rate": 3.790660636981584e-05,
      "loss": 1.316,
      "step": 232900
    },
    {
      "epoch": 0.7260243659948201,
      "grad_norm": 0.4996073544025421,
      "learning_rate": 3.790400915880491e-05,
      "loss": 1.2194,
      "step": 232950
    },
    {
      "epoch": 0.7261801986554758,
      "grad_norm": 0.6979779005050659,
      "learning_rate": 3.790141194779398e-05,
      "loss": 1.2657,
      "step": 233000
    },
    {
      "epoch": 0.7263360313161314,
      "grad_norm": 0.641635000705719,
      "learning_rate": 3.789881473678305e-05,
      "loss": 1.2454,
      "step": 233050
    },
    {
      "epoch": 0.7264918639767872,
      "grad_norm": 0.6246194243431091,
      "learning_rate": 3.7896217525772125e-05,
      "loss": 1.2517,
      "step": 233100
    },
    {
      "epoch": 0.7266476966374429,
      "grad_norm": 0.5134180188179016,
      "learning_rate": 3.78936203147612e-05,
      "loss": 1.275,
      "step": 233150
    },
    {
      "epoch": 0.7268035292980985,
      "grad_norm": 0.5737140774726868,
      "learning_rate": 3.789102310375027e-05,
      "loss": 1.3136,
      "step": 233200
    },
    {
      "epoch": 0.7269593619587542,
      "grad_norm": 0.6212708950042725,
      "learning_rate": 3.788842589273934e-05,
      "loss": 1.2651,
      "step": 233250
    },
    {
      "epoch": 0.7271151946194099,
      "grad_norm": 0.572079598903656,
      "learning_rate": 3.7885828681728415e-05,
      "loss": 1.2432,
      "step": 233300
    },
    {
      "epoch": 0.7272710272800655,
      "grad_norm": 0.595110297203064,
      "learning_rate": 3.788323147071748e-05,
      "loss": 1.2618,
      "step": 233350
    },
    {
      "epoch": 0.7274268599407212,
      "grad_norm": 0.5770220160484314,
      "learning_rate": 3.788063425970656e-05,
      "loss": 1.2821,
      "step": 233400
    },
    {
      "epoch": 0.727582692601377,
      "grad_norm": 0.46990329027175903,
      "learning_rate": 3.787803704869563e-05,
      "loss": 1.3134,
      "step": 233450
    },
    {
      "epoch": 0.7277385252620326,
      "grad_norm": 0.6917449831962585,
      "learning_rate": 3.78754398376847e-05,
      "loss": 1.2285,
      "step": 233500
    },
    {
      "epoch": 0.7278943579226883,
      "grad_norm": 0.5546277761459351,
      "learning_rate": 3.787284262667378e-05,
      "loss": 1.2967,
      "step": 233550
    },
    {
      "epoch": 0.728050190583344,
      "grad_norm": 0.5727751851081848,
      "learning_rate": 3.787024541566285e-05,
      "loss": 1.2579,
      "step": 233600
    },
    {
      "epoch": 0.7282060232439996,
      "grad_norm": 0.6034719944000244,
      "learning_rate": 3.7867648204651916e-05,
      "loss": 1.2815,
      "step": 233650
    },
    {
      "epoch": 0.7283618559046553,
      "grad_norm": 0.7316389083862305,
      "learning_rate": 3.786505099364099e-05,
      "loss": 1.2534,
      "step": 233700
    },
    {
      "epoch": 0.7285176885653111,
      "grad_norm": 0.49418914318084717,
      "learning_rate": 3.786245378263006e-05,
      "loss": 1.2535,
      "step": 233750
    },
    {
      "epoch": 0.7286735212259667,
      "grad_norm": 0.7165614366531372,
      "learning_rate": 3.7859856571619134e-05,
      "loss": 1.2728,
      "step": 233800
    },
    {
      "epoch": 0.7288293538866224,
      "grad_norm": 0.6082864999771118,
      "learning_rate": 3.7857259360608206e-05,
      "loss": 1.2832,
      "step": 233850
    },
    {
      "epoch": 0.7289851865472781,
      "grad_norm": 0.8042028546333313,
      "learning_rate": 3.785466214959728e-05,
      "loss": 1.2709,
      "step": 233900
    },
    {
      "epoch": 0.7291410192079337,
      "grad_norm": 0.5841940641403198,
      "learning_rate": 3.785206493858635e-05,
      "loss": 1.282,
      "step": 233950
    },
    {
      "epoch": 0.7292968518685894,
      "grad_norm": 0.5727594494819641,
      "learning_rate": 3.7849467727575424e-05,
      "loss": 1.32,
      "step": 234000
    },
    {
      "epoch": 0.7294526845292452,
      "grad_norm": 0.5788766145706177,
      "learning_rate": 3.784687051656449e-05,
      "loss": 1.2349,
      "step": 234050
    },
    {
      "epoch": 0.7296085171899008,
      "grad_norm": 0.6673904657363892,
      "learning_rate": 3.784427330555357e-05,
      "loss": 1.2797,
      "step": 234100
    },
    {
      "epoch": 0.7297643498505565,
      "grad_norm": 0.5604996085166931,
      "learning_rate": 3.784167609454264e-05,
      "loss": 1.2798,
      "step": 234150
    },
    {
      "epoch": 0.7299201825112122,
      "grad_norm": 0.5106792449951172,
      "learning_rate": 3.783907888353171e-05,
      "loss": 1.2291,
      "step": 234200
    },
    {
      "epoch": 0.7300760151718678,
      "grad_norm": 0.5026671290397644,
      "learning_rate": 3.783648167252078e-05,
      "loss": 1.2626,
      "step": 234250
    },
    {
      "epoch": 0.7302318478325235,
      "grad_norm": 0.6027716398239136,
      "learning_rate": 3.783393640573008e-05,
      "loss": 1.3306,
      "step": 234300
    },
    {
      "epoch": 0.7303876804931793,
      "grad_norm": 0.5953543186187744,
      "learning_rate": 3.7831339194719144e-05,
      "loss": 1.2485,
      "step": 234350
    },
    {
      "epoch": 0.7305435131538349,
      "grad_norm": 0.6393712162971497,
      "learning_rate": 3.7828741983708216e-05,
      "loss": 1.2368,
      "step": 234400
    },
    {
      "epoch": 0.7306993458144906,
      "grad_norm": 0.7200289964675903,
      "learning_rate": 3.782614477269729e-05,
      "loss": 1.2096,
      "step": 234450
    },
    {
      "epoch": 0.7308551784751462,
      "grad_norm": 0.5568264722824097,
      "learning_rate": 3.782354756168636e-05,
      "loss": 1.2611,
      "step": 234500
    },
    {
      "epoch": 0.7310110111358019,
      "grad_norm": 0.5995111465454102,
      "learning_rate": 3.7820950350675434e-05,
      "loss": 1.2605,
      "step": 234550
    },
    {
      "epoch": 0.7311668437964576,
      "grad_norm": 0.48950767517089844,
      "learning_rate": 3.78183531396645e-05,
      "loss": 1.2972,
      "step": 234600
    },
    {
      "epoch": 0.7313226764571132,
      "grad_norm": 0.4749108850955963,
      "learning_rate": 3.781575592865358e-05,
      "loss": 1.2543,
      "step": 234650
    },
    {
      "epoch": 0.731478509117769,
      "grad_norm": 0.5801718831062317,
      "learning_rate": 3.781315871764265e-05,
      "loss": 1.2648,
      "step": 234700
    },
    {
      "epoch": 0.7316343417784247,
      "grad_norm": 0.5596044063568115,
      "learning_rate": 3.781056150663172e-05,
      "loss": 1.2889,
      "step": 234750
    },
    {
      "epoch": 0.7317901744390803,
      "grad_norm": 0.6975328326225281,
      "learning_rate": 3.780796429562079e-05,
      "loss": 1.2738,
      "step": 234800
    },
    {
      "epoch": 0.731946007099736,
      "grad_norm": 0.5818811655044556,
      "learning_rate": 3.780536708460987e-05,
      "loss": 1.2461,
      "step": 234850
    },
    {
      "epoch": 0.7321018397603917,
      "grad_norm": 0.6513124704360962,
      "learning_rate": 3.7802769873598935e-05,
      "loss": 1.2534,
      "step": 234900
    },
    {
      "epoch": 0.7322576724210473,
      "grad_norm": 0.6070747971534729,
      "learning_rate": 3.780017266258801e-05,
      "loss": 1.3131,
      "step": 234950
    },
    {
      "epoch": 0.7324135050817031,
      "grad_norm": 0.6326707601547241,
      "learning_rate": 3.779757545157708e-05,
      "loss": 1.2363,
      "step": 235000
    },
    {
      "epoch": 0.7325693377423588,
      "grad_norm": 0.6077414751052856,
      "learning_rate": 3.779497824056615e-05,
      "loss": 1.3699,
      "step": 235050
    },
    {
      "epoch": 0.7327251704030144,
      "grad_norm": 0.6069465279579163,
      "learning_rate": 3.7792381029555225e-05,
      "loss": 1.2678,
      "step": 235100
    },
    {
      "epoch": 0.7328810030636701,
      "grad_norm": 0.4538349509239197,
      "learning_rate": 3.77897838185443e-05,
      "loss": 1.2761,
      "step": 235150
    },
    {
      "epoch": 0.7330368357243258,
      "grad_norm": 0.5898944139480591,
      "learning_rate": 3.778718660753337e-05,
      "loss": 1.2011,
      "step": 235200
    },
    {
      "epoch": 0.7331926683849814,
      "grad_norm": 0.5217171311378479,
      "learning_rate": 3.778458939652244e-05,
      "loss": 1.2584,
      "step": 235250
    },
    {
      "epoch": 0.7333485010456372,
      "grad_norm": 0.5947607159614563,
      "learning_rate": 3.778199218551151e-05,
      "loss": 1.2867,
      "step": 235300
    },
    {
      "epoch": 0.7335043337062929,
      "grad_norm": 0.5734007358551025,
      "learning_rate": 3.777939497450058e-05,
      "loss": 1.2121,
      "step": 235350
    },
    {
      "epoch": 0.7336601663669485,
      "grad_norm": 0.562778651714325,
      "learning_rate": 3.777679776348966e-05,
      "loss": 1.2762,
      "step": 235400
    },
    {
      "epoch": 0.7338159990276042,
      "grad_norm": 0.6115782260894775,
      "learning_rate": 3.7774200552478725e-05,
      "loss": 1.3093,
      "step": 235450
    },
    {
      "epoch": 0.7339718316882599,
      "grad_norm": 0.5463190674781799,
      "learning_rate": 3.77716033414678e-05,
      "loss": 1.2699,
      "step": 235500
    },
    {
      "epoch": 0.7341276643489155,
      "grad_norm": 0.6627485156059265,
      "learning_rate": 3.776900613045688e-05,
      "loss": 1.2748,
      "step": 235550
    },
    {
      "epoch": 0.7342834970095713,
      "grad_norm": 0.6870367527008057,
      "learning_rate": 3.776640891944594e-05,
      "loss": 1.2974,
      "step": 235600
    },
    {
      "epoch": 0.734439329670227,
      "grad_norm": 0.5784271359443665,
      "learning_rate": 3.7763811708435016e-05,
      "loss": 1.3049,
      "step": 235650
    },
    {
      "epoch": 0.7345951623308826,
      "grad_norm": 0.6893800497055054,
      "learning_rate": 3.776121449742409e-05,
      "loss": 1.2804,
      "step": 235700
    },
    {
      "epoch": 0.7347509949915383,
      "grad_norm": 0.5445193648338318,
      "learning_rate": 3.775861728641316e-05,
      "loss": 1.3106,
      "step": 235750
    },
    {
      "epoch": 0.734906827652194,
      "grad_norm": 0.589982807636261,
      "learning_rate": 3.775602007540223e-05,
      "loss": 1.2632,
      "step": 235800
    },
    {
      "epoch": 0.7350626603128496,
      "grad_norm": 0.6039903163909912,
      "learning_rate": 3.7753422864391306e-05,
      "loss": 1.284,
      "step": 235850
    },
    {
      "epoch": 0.7352184929735053,
      "grad_norm": 0.5653588771820068,
      "learning_rate": 3.775082565338038e-05,
      "loss": 1.2473,
      "step": 235900
    },
    {
      "epoch": 0.735374325634161,
      "grad_norm": 0.60331130027771,
      "learning_rate": 3.774822844236945e-05,
      "loss": 1.229,
      "step": 235950
    },
    {
      "epoch": 0.7355301582948167,
      "grad_norm": 0.4340108036994934,
      "learning_rate": 3.7745631231358516e-05,
      "loss": 1.2866,
      "step": 236000
    },
    {
      "epoch": 0.7356859909554724,
      "grad_norm": 0.5813091993331909,
      "learning_rate": 3.774303402034759e-05,
      "loss": 1.2295,
      "step": 236050
    },
    {
      "epoch": 0.735841823616128,
      "grad_norm": 0.6590694785118103,
      "learning_rate": 3.774043680933667e-05,
      "loss": 1.3142,
      "step": 236100
    },
    {
      "epoch": 0.7359976562767837,
      "grad_norm": 0.5876161456108093,
      "learning_rate": 3.7737839598325734e-05,
      "loss": 1.311,
      "step": 236150
    },
    {
      "epoch": 0.7361534889374394,
      "grad_norm": 0.6618829965591431,
      "learning_rate": 3.7735242387314806e-05,
      "loss": 1.2608,
      "step": 236200
    },
    {
      "epoch": 0.7363093215980951,
      "grad_norm": 0.6345254182815552,
      "learning_rate": 3.7732645176303886e-05,
      "loss": 1.2794,
      "step": 236250
    },
    {
      "epoch": 0.7364651542587508,
      "grad_norm": 0.6420865654945374,
      "learning_rate": 3.773004796529295e-05,
      "loss": 1.2655,
      "step": 236300
    },
    {
      "epoch": 0.7366209869194065,
      "grad_norm": 0.6401189565658569,
      "learning_rate": 3.7727450754282024e-05,
      "loss": 1.2671,
      "step": 236350
    },
    {
      "epoch": 0.7367768195800621,
      "grad_norm": 0.6373091340065002,
      "learning_rate": 3.7724853543271096e-05,
      "loss": 1.2276,
      "step": 236400
    },
    {
      "epoch": 0.7369326522407178,
      "grad_norm": 0.6716729998588562,
      "learning_rate": 3.772225633226017e-05,
      "loss": 1.3192,
      "step": 236450
    },
    {
      "epoch": 0.7370884849013735,
      "grad_norm": 0.5961744785308838,
      "learning_rate": 3.771965912124924e-05,
      "loss": 1.2427,
      "step": 236500
    },
    {
      "epoch": 0.7372443175620292,
      "grad_norm": 0.6622959971427917,
      "learning_rate": 3.7717061910238314e-05,
      "loss": 1.2741,
      "step": 236550
    },
    {
      "epoch": 0.7374001502226849,
      "grad_norm": 0.5791953206062317,
      "learning_rate": 3.771446469922738e-05,
      "loss": 1.2965,
      "step": 236600
    },
    {
      "epoch": 0.7375559828833406,
      "grad_norm": 0.6220414042472839,
      "learning_rate": 3.771186748821646e-05,
      "loss": 1.2905,
      "step": 236650
    },
    {
      "epoch": 0.7377118155439962,
      "grad_norm": 0.6618169546127319,
      "learning_rate": 3.7709270277205525e-05,
      "loss": 1.2314,
      "step": 236700
    },
    {
      "epoch": 0.7378676482046519,
      "grad_norm": 0.8551799058914185,
      "learning_rate": 3.77066730661946e-05,
      "loss": 1.298,
      "step": 236750
    },
    {
      "epoch": 0.7380234808653076,
      "grad_norm": 0.533203125,
      "learning_rate": 3.7704075855183676e-05,
      "loss": 1.308,
      "step": 236800
    },
    {
      "epoch": 0.7381793135259633,
      "grad_norm": 0.6152728199958801,
      "learning_rate": 3.770147864417274e-05,
      "loss": 1.2341,
      "step": 236850
    },
    {
      "epoch": 0.738335146186619,
      "grad_norm": 0.49246737360954285,
      "learning_rate": 3.7698933377382034e-05,
      "loss": 1.2201,
      "step": 236900
    },
    {
      "epoch": 0.7384909788472747,
      "grad_norm": 0.5680564641952515,
      "learning_rate": 3.769633616637111e-05,
      "loss": 1.3087,
      "step": 236950
    },
    {
      "epoch": 0.7386468115079303,
      "grad_norm": 0.4851732552051544,
      "learning_rate": 3.769373895536018e-05,
      "loss": 1.3019,
      "step": 237000
    },
    {
      "epoch": 0.738802644168586,
      "grad_norm": 0.757364809513092,
      "learning_rate": 3.769114174434925e-05,
      "loss": 1.2736,
      "step": 237050
    },
    {
      "epoch": 0.7389584768292417,
      "grad_norm": 0.7035018801689148,
      "learning_rate": 3.7688544533338324e-05,
      "loss": 1.2975,
      "step": 237100
    },
    {
      "epoch": 0.7391143094898973,
      "grad_norm": 0.5210107564926147,
      "learning_rate": 3.768594732232739e-05,
      "loss": 1.2724,
      "step": 237150
    },
    {
      "epoch": 0.7392701421505531,
      "grad_norm": 0.5484238862991333,
      "learning_rate": 3.768335011131647e-05,
      "loss": 1.3031,
      "step": 237200
    },
    {
      "epoch": 0.7394259748112088,
      "grad_norm": 0.6362617611885071,
      "learning_rate": 3.7680752900305535e-05,
      "loss": 1.2193,
      "step": 237250
    },
    {
      "epoch": 0.7395818074718644,
      "grad_norm": 0.5788515210151672,
      "learning_rate": 3.767815568929461e-05,
      "loss": 1.2531,
      "step": 237300
    },
    {
      "epoch": 0.7397376401325201,
      "grad_norm": 0.6512762904167175,
      "learning_rate": 3.767555847828368e-05,
      "loss": 1.2369,
      "step": 237350
    },
    {
      "epoch": 0.7398934727931757,
      "grad_norm": 0.5917673110961914,
      "learning_rate": 3.767296126727275e-05,
      "loss": 1.2629,
      "step": 237400
    },
    {
      "epoch": 0.7400493054538314,
      "grad_norm": 0.6145670413970947,
      "learning_rate": 3.7670364056261825e-05,
      "loss": 1.2375,
      "step": 237450
    },
    {
      "epoch": 0.7402051381144872,
      "grad_norm": 0.6140380501747131,
      "learning_rate": 3.76677668452509e-05,
      "loss": 1.2505,
      "step": 237500
    },
    {
      "epoch": 0.7403609707751428,
      "grad_norm": 0.5712834596633911,
      "learning_rate": 3.766516963423997e-05,
      "loss": 1.2452,
      "step": 237550
    },
    {
      "epoch": 0.7405168034357985,
      "grad_norm": 0.6734721064567566,
      "learning_rate": 3.766257242322904e-05,
      "loss": 1.318,
      "step": 237600
    },
    {
      "epoch": 0.7406726360964542,
      "grad_norm": 0.4404653310775757,
      "learning_rate": 3.7659975212218115e-05,
      "loss": 1.2805,
      "step": 237650
    },
    {
      "epoch": 0.7408284687571098,
      "grad_norm": 0.7222359776496887,
      "learning_rate": 3.765737800120718e-05,
      "loss": 1.257,
      "step": 237700
    },
    {
      "epoch": 0.7409843014177655,
      "grad_norm": 0.5817176103591919,
      "learning_rate": 3.765478079019626e-05,
      "loss": 1.3032,
      "step": 237750
    },
    {
      "epoch": 0.7411401340784213,
      "grad_norm": 0.5464566349983215,
      "learning_rate": 3.765218357918533e-05,
      "loss": 1.2703,
      "step": 237800
    },
    {
      "epoch": 0.7412959667390769,
      "grad_norm": 0.6060156226158142,
      "learning_rate": 3.76495863681744e-05,
      "loss": 1.2627,
      "step": 237850
    },
    {
      "epoch": 0.7414517993997326,
      "grad_norm": 0.56698077917099,
      "learning_rate": 3.764698915716348e-05,
      "loss": 1.2679,
      "step": 237900
    },
    {
      "epoch": 0.7416076320603883,
      "grad_norm": 0.6334854364395142,
      "learning_rate": 3.764439194615254e-05,
      "loss": 1.3196,
      "step": 237950
    },
    {
      "epoch": 0.7417634647210439,
      "grad_norm": 0.6508264541625977,
      "learning_rate": 3.7641794735141616e-05,
      "loss": 1.1841,
      "step": 238000
    },
    {
      "epoch": 0.7419192973816996,
      "grad_norm": 0.5579233765602112,
      "learning_rate": 3.763919752413069e-05,
      "loss": 1.3329,
      "step": 238050
    },
    {
      "epoch": 0.7420751300423554,
      "grad_norm": 0.5983953475952148,
      "learning_rate": 3.763660031311976e-05,
      "loss": 1.2415,
      "step": 238100
    },
    {
      "epoch": 0.742230962703011,
      "grad_norm": 0.583034873008728,
      "learning_rate": 3.763400310210883e-05,
      "loss": 1.3124,
      "step": 238150
    },
    {
      "epoch": 0.7423867953636667,
      "grad_norm": 0.6554883718490601,
      "learning_rate": 3.7631405891097906e-05,
      "loss": 1.3109,
      "step": 238200
    },
    {
      "epoch": 0.7425426280243224,
      "grad_norm": 0.8056042194366455,
      "learning_rate": 3.762880868008698e-05,
      "loss": 1.2625,
      "step": 238250
    },
    {
      "epoch": 0.742698460684978,
      "grad_norm": 0.6345216035842896,
      "learning_rate": 3.762621146907605e-05,
      "loss": 1.3184,
      "step": 238300
    },
    {
      "epoch": 0.7428542933456337,
      "grad_norm": 0.5975669026374817,
      "learning_rate": 3.762361425806512e-05,
      "loss": 1.2827,
      "step": 238350
    },
    {
      "epoch": 0.7430101260062894,
      "grad_norm": 0.4959540367126465,
      "learning_rate": 3.762101704705419e-05,
      "loss": 1.3276,
      "step": 238400
    },
    {
      "epoch": 0.7431659586669451,
      "grad_norm": 0.709790050983429,
      "learning_rate": 3.761841983604327e-05,
      "loss": 1.2429,
      "step": 238450
    },
    {
      "epoch": 0.7433217913276008,
      "grad_norm": 0.6355859637260437,
      "learning_rate": 3.761582262503234e-05,
      "loss": 1.3313,
      "step": 238500
    },
    {
      "epoch": 0.7434776239882565,
      "grad_norm": 0.6216545104980469,
      "learning_rate": 3.7613225414021406e-05,
      "loss": 1.2682,
      "step": 238550
    },
    {
      "epoch": 0.7436334566489121,
      "grad_norm": 0.6055887937545776,
      "learning_rate": 3.761062820301048e-05,
      "loss": 1.2696,
      "step": 238600
    },
    {
      "epoch": 0.7437892893095678,
      "grad_norm": 0.6596593856811523,
      "learning_rate": 3.760803099199955e-05,
      "loss": 1.309,
      "step": 238650
    },
    {
      "epoch": 0.7439451219702234,
      "grad_norm": 0.44596484303474426,
      "learning_rate": 3.7605433780988624e-05,
      "loss": 1.3589,
      "step": 238700
    },
    {
      "epoch": 0.7441009546308792,
      "grad_norm": 0.6108133792877197,
      "learning_rate": 3.7602836569977696e-05,
      "loss": 1.318,
      "step": 238750
    },
    {
      "epoch": 0.7442567872915349,
      "grad_norm": 0.551838755607605,
      "learning_rate": 3.760023935896677e-05,
      "loss": 1.3153,
      "step": 238800
    },
    {
      "epoch": 0.7444126199521905,
      "grad_norm": 0.604456901550293,
      "learning_rate": 3.759764214795584e-05,
      "loss": 1.3201,
      "step": 238850
    },
    {
      "epoch": 0.7445684526128462,
      "grad_norm": 0.7448132634162903,
      "learning_rate": 3.7595044936944914e-05,
      "loss": 1.2429,
      "step": 238900
    },
    {
      "epoch": 0.7447242852735019,
      "grad_norm": 0.5435047149658203,
      "learning_rate": 3.759244772593398e-05,
      "loss": 1.2897,
      "step": 238950
    },
    {
      "epoch": 0.7448801179341575,
      "grad_norm": 0.5713126063346863,
      "learning_rate": 3.758985051492306e-05,
      "loss": 1.3048,
      "step": 239000
    },
    {
      "epoch": 0.7450359505948133,
      "grad_norm": 0.5812811851501465,
      "learning_rate": 3.758725330391213e-05,
      "loss": 1.2324,
      "step": 239050
    },
    {
      "epoch": 0.745191783255469,
      "grad_norm": 0.7081101536750793,
      "learning_rate": 3.75846560929012e-05,
      "loss": 1.3339,
      "step": 239100
    },
    {
      "epoch": 0.7453476159161246,
      "grad_norm": 0.6279228329658508,
      "learning_rate": 3.7582058881890276e-05,
      "loss": 1.3053,
      "step": 239150
    },
    {
      "epoch": 0.7455034485767803,
      "grad_norm": 0.5748538970947266,
      "learning_rate": 3.757946167087935e-05,
      "loss": 1.293,
      "step": 239200
    },
    {
      "epoch": 0.745659281237436,
      "grad_norm": 0.618865966796875,
      "learning_rate": 3.7576864459868415e-05,
      "loss": 1.2443,
      "step": 239250
    },
    {
      "epoch": 0.7458151138980916,
      "grad_norm": 0.7652727365493774,
      "learning_rate": 3.757426724885749e-05,
      "loss": 1.2914,
      "step": 239300
    },
    {
      "epoch": 0.7459709465587474,
      "grad_norm": 0.7561241984367371,
      "learning_rate": 3.757167003784656e-05,
      "loss": 1.317,
      "step": 239350
    },
    {
      "epoch": 0.7461267792194031,
      "grad_norm": 0.5331397652626038,
      "learning_rate": 3.756907282683563e-05,
      "loss": 1.2252,
      "step": 239400
    },
    {
      "epoch": 0.7462826118800587,
      "grad_norm": 0.5281304717063904,
      "learning_rate": 3.7566475615824705e-05,
      "loss": 1.2828,
      "step": 239450
    },
    {
      "epoch": 0.7464384445407144,
      "grad_norm": 0.46528419852256775,
      "learning_rate": 3.756387840481378e-05,
      "loss": 1.2817,
      "step": 239500
    },
    {
      "epoch": 0.7465942772013701,
      "grad_norm": 0.6317300200462341,
      "learning_rate": 3.756128119380285e-05,
      "loss": 1.2258,
      "step": 239550
    },
    {
      "epoch": 0.7467501098620257,
      "grad_norm": 0.4186657667160034,
      "learning_rate": 3.755868398279192e-05,
      "loss": 1.2024,
      "step": 239600
    },
    {
      "epoch": 0.7469059425226815,
      "grad_norm": 0.5784814357757568,
      "learning_rate": 3.755608677178099e-05,
      "loss": 1.2266,
      "step": 239650
    },
    {
      "epoch": 0.7470617751833372,
      "grad_norm": 0.5691726207733154,
      "learning_rate": 3.755348956077007e-05,
      "loss": 1.2638,
      "step": 239700
    },
    {
      "epoch": 0.7472176078439928,
      "grad_norm": 0.8086286783218384,
      "learning_rate": 3.755089234975914e-05,
      "loss": 1.2677,
      "step": 239750
    },
    {
      "epoch": 0.7473734405046485,
      "grad_norm": 0.6298354268074036,
      "learning_rate": 3.7548295138748205e-05,
      "loss": 1.3275,
      "step": 239800
    },
    {
      "epoch": 0.7475292731653042,
      "grad_norm": 0.5311959981918335,
      "learning_rate": 3.754569792773728e-05,
      "loss": 1.2105,
      "step": 239850
    },
    {
      "epoch": 0.7476851058259598,
      "grad_norm": 0.6260274052619934,
      "learning_rate": 3.754310071672635e-05,
      "loss": 1.24,
      "step": 239900
    },
    {
      "epoch": 0.7478409384866155,
      "grad_norm": 0.5867217183113098,
      "learning_rate": 3.754050350571542e-05,
      "loss": 1.3205,
      "step": 239950
    },
    {
      "epoch": 0.7479967711472713,
      "grad_norm": 0.5467709302902222,
      "learning_rate": 3.7537906294704495e-05,
      "loss": 1.2976,
      "step": 240000
    },
    {
      "epoch": 0.7481526038079269,
      "grad_norm": 0.5968489050865173,
      "learning_rate": 3.753530908369357e-05,
      "loss": 1.2668,
      "step": 240050
    },
    {
      "epoch": 0.7483084364685826,
      "grad_norm": 0.712109386920929,
      "learning_rate": 3.753271187268264e-05,
      "loss": 1.3075,
      "step": 240100
    },
    {
      "epoch": 0.7484642691292382,
      "grad_norm": 0.5039492249488831,
      "learning_rate": 3.753011466167171e-05,
      "loss": 1.2645,
      "step": 240150
    },
    {
      "epoch": 0.7486201017898939,
      "grad_norm": 0.5821762681007385,
      "learning_rate": 3.752751745066078e-05,
      "loss": 1.2535,
      "step": 240200
    },
    {
      "epoch": 0.7487759344505496,
      "grad_norm": 0.6162744164466858,
      "learning_rate": 3.752492023964986e-05,
      "loss": 1.2355,
      "step": 240250
    },
    {
      "epoch": 0.7489317671112053,
      "grad_norm": 0.6128153204917908,
      "learning_rate": 3.752232302863893e-05,
      "loss": 1.2573,
      "step": 240300
    },
    {
      "epoch": 0.749087599771861,
      "grad_norm": 0.675758957862854,
      "learning_rate": 3.7519725817627996e-05,
      "loss": 1.2895,
      "step": 240350
    },
    {
      "epoch": 0.7492434324325167,
      "grad_norm": 0.5502106547355652,
      "learning_rate": 3.7517128606617075e-05,
      "loss": 1.2378,
      "step": 240400
    },
    {
      "epoch": 0.7493992650931723,
      "grad_norm": 0.5542824864387512,
      "learning_rate": 3.751453139560615e-05,
      "loss": 1.2528,
      "step": 240450
    },
    {
      "epoch": 0.749555097753828,
      "grad_norm": 0.6975026726722717,
      "learning_rate": 3.7511934184595214e-05,
      "loss": 1.2882,
      "step": 240500
    }
  ],
  "logging_steps": 50,
  "max_steps": 962571,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.85446188613632e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
