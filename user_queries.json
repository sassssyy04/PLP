{"timestamp": "2025-04-22T19:17:49.536188", "query": "from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel, PeftConfig\nfrom bertopic import BERTopic\nimport torch\nimport json\nimport os\nfrom datetime import datetime\nimport uuid\n\n# === Model Setup ===\npeft_model_path = \"./phi2-finetuned\"\npeft_config = PeftConfig.from_pretrained(peft_model_path)\nbase_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, trust_remote_code=True)\nmodel = PeftModel.from_pretrained(base_model, peft_model_path)\n\ntokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n\n# === Utility Functions ===\nSESSION_DIR = \"sessions\"\nos.makedirs(SESSION_DIR, exist_ok=True)\n\ndef save_user_query(session_id, query):\n    path = os.path.join(SESSION_DIR, f\"{session_id}.json\")\n    entry = {\"timestamp\": datetime.now().isoformat(), \"query\": query}\n    with open(path, \"a\") as f:\n        json.dump(entry, f)\n        f.write(\"\\n\")\n\ndef load_session_queries(session_id):\n    path = os.path.join(SESSION_DIR, f\"{session_id}.json\")\n    queries = []\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            for line in f:\n                queries.append(json.loads(line)[\"query\"])\n    return queries\n\ndef generate_text(prompt, max_tokens=250):\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)[len(prompt):].strip()\n\n# === Topic Modeling + Module Generation ===\ndef generate_module_for_session(session_id):\n    queries = load_session_queries(session_id)\n    if not queries:\n        return \"No queries found for this session.\"\n\n    # Topic extraction using BERTopic\n    topic_model = BERTopic()\n    topics, _ = topic_model.fit_transform(queries)\n    topic_info = topic_model.get_topic_info()\n    top_topic = topic_info.iloc[1][\"Name\"]  # Skip -1 outlier\n    topic_summary = top_topic.split(\":\")[-1].strip()\n\n    # Generate module content\n    prompts = {\n        \"explanation\": f\"### Prompt:\\nWrite a brief explanation about this topic: {topic_summary}\\n\\n### Response:\\n\",\n        \"question\": f\"### Prompt:\\nCreate a thought-provoking question on: {topic_summary}\\n\\n### Response:\\n\",\n        \"mcq\": f\"### Prompt:\\nGenerate one multiple choice question (MCQ) with 4 options and one correct answer on: {topic_summary}\\n\\n### Response:\\n\"\n    }\n\n    module = {\n        \"topic\": topic_summary,\n        \"explanation\": generate_text(prompts[\"explanation\"]),\n        \"question\": generate_text(prompts[\"question\"]),\n        \"mcq\": generate_text(prompts[\"mcq\"])\n    }\n\n    return module\n\n# === Example: Create Session and Generate Module ===\nif __name__ == \"__main__\":\n    # Example usage\n    session_id = str(uuid.uuid4())  # Simulate a user session\n\n    # Simulate interaction\n    queries = [\n        \"What is transfer learning?\",\n        \"How do neural networks retain knowledge?\",\n        \"Can pre-trained models be reused?\"\n    ]\n\n    for q in queries:\n        save_user_query(session_id, q)\n\n    # Generate module\n    module = generate_module_for_session(session_id)\n\n    print(f\"\\n=== Learning Module for Session: {session_id} ===\")\n    print(f\"\ud83d\udcda Topic: {module['topic']}\\n\")\n    print(f\"\ud83e\udde0 Explanation:\\n{module['explanation']}\\n\")\n    print(f\"\u2753 Open-ended Question:\\n{module['question']}\\n\")\n    print(f\"\ud83d\udcdd MCQ:\\n{module['mcq']}\\n\")\n"}
